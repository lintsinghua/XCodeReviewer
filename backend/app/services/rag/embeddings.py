"""
åµŒå…¥æ¨¡å‹æœåŠ¡
æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹æä¾›å•†: OpenAI, Azure, Ollama, Cohere, HuggingFace, Jina
"""

import asyncio
import hashlib
import logging
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod
from dataclasses import dataclass

import httpx

from app.core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class EmbeddingResult:
    """åµŒå…¥ç»“æœ"""
    embedding: List[float]
    tokens_used: int
    model: str


class EmbeddingProvider(ABC):
    """åµŒå…¥æä¾›å•†åŸºç±»"""
    
    @abstractmethod
    async def embed_text(self, text: str) -> EmbeddingResult:
        """åµŒå…¥å•ä¸ªæ–‡æœ¬"""
        pass
    
    @abstractmethod
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        """æ‰¹é‡åµŒå…¥æ–‡æœ¬"""
        pass
    
    @property
    @abstractmethod
    def dimension(self) -> int:
        """åµŒå…¥å‘é‡ç»´åº¦"""
        pass


class OpenAIEmbedding(EmbeddingProvider):
    """OpenAI åµŒå…¥æœåŠ¡"""
    
    MODELS = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
    }
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "text-embedding-3-small",
    ):
        self.api_key = api_key or settings.LLM_API_KEY
        self.base_url = base_url or "https://api.openai.com/v1"
        self.model = model
        self._dimension = self.MODELS.get(model, 1536)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        max_length = 8191
        truncated_texts = [text[:max_length] for text in texts]
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": self.model,
            "input": truncated_texts,
        }
        
        url = f"{self.base_url.rstrip('/')}/embeddings"
        
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for item in data.get("data", []):
                results.append(EmbeddingResult(
                    embedding=item["embedding"],
                    tokens_used=data.get("usage", {}).get("total_tokens", 0) // len(texts),
                    model=self.model,
                ))
            
            return results


class AzureOpenAIEmbedding(EmbeddingProvider):
    """
    Azure OpenAI åµŒå…¥æœåŠ¡
    
    ä½¿ç”¨æœ€æ–° API ç‰ˆæœ¬ 2024-10-21 (GA)
    ç«¯ç‚¹æ ¼å¼: https://<resource>.openai.azure.com/openai/deployments/<deployment>/embeddings?api-version=2024-10-21
    """
    
    MODELS = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536,
    }
    
    # æœ€æ–°çš„ GA API ç‰ˆæœ¬
    API_VERSION = "2024-10-21"
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "text-embedding-3-small",
    ):
        self.api_key = api_key
        self.base_url = base_url or "https://your-resource.openai.azure.com"
        self.model = model
        self._dimension = self.MODELS.get(model, 1536)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        max_length = 8191
        truncated_texts = [text[:max_length] for text in texts]
        
        headers = {
            "api-key": self.api_key,
            "Content-Type": "application/json",
        }
        
        payload = {
            "input": truncated_texts,
        }
        
        # Azure URL æ ¼å¼ - ä½¿ç”¨æœ€æ–° API ç‰ˆæœ¬
        url = f"{self.base_url.rstrip('/')}/openai/deployments/{self.model}/embeddings?api-version={self.API_VERSION}"
        
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for item in data.get("data", []):
                results.append(EmbeddingResult(
                    embedding=item["embedding"],
                    tokens_used=data.get("usage", {}).get("total_tokens", 0) // len(texts),
                    model=self.model,
                ))
            
            return results


class OllamaEmbedding(EmbeddingProvider):
    """
    Ollama æœ¬åœ°åµŒå…¥æœåŠ¡
    
    ä½¿ç”¨æ–°çš„ /api/embed ç«¯ç‚¹ (2024å¹´èµ·):
    - æ”¯æŒæ‰¹é‡åµŒå…¥
    - ä½¿ç”¨ 'input' å‚æ•°ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²æ•°ç»„ï¼‰
    """
    
    MODELS = {
        "nomic-embed-text": 768,
        "mxbai-embed-large": 1024,
        "all-minilm": 384,
        "snowflake-arctic-embed": 1024,
        "bge-m3": 1024,
        "qwen3-embedding": 1024,
    }
    
    def __init__(
        self,
        base_url: Optional[str] = None,
        model: str = "nomic-embed-text",
    ):
        self.base_url = base_url or "http://localhost:11434"
        self.model = model
        self._dimension = self.MODELS.get(model, 768)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        # æ–°çš„ Ollama /api/embed ç«¯ç‚¹
        url = f"{self.base_url.rstrip('/')}/api/embed"
        
        payload = {
            "model": self.model,
            "input": texts,  # æ–° API ä½¿ç”¨ 'input' å‚æ•°ï¼Œæ”¯æŒæ‰¹é‡
        }
        
        async with httpx.AsyncClient(timeout=120) as client:
            response = await client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()
            
            # æ–° API è¿”å›æ ¼å¼: {"embeddings": [[...], [...], ...]}
            embeddings = data.get("embeddings", [])
            
            results = []
            for i, embedding in enumerate(embeddings):
                results.append(EmbeddingResult(
                    embedding=embedding,
                    tokens_used=len(texts[i]) // 4,
                    model=self.model,
                ))
            
            return results


class CohereEmbedding(EmbeddingProvider):
    """
    Cohere åµŒå…¥æœåŠ¡
    
    ä½¿ç”¨æ–°çš„ v2 API (2024å¹´èµ·):
    - ç«¯ç‚¹: https://api.cohere.com/v2/embed
    - ä½¿ç”¨ 'inputs' å‚æ•°æ›¿ä»£ 'texts'
    - éœ€è¦æŒ‡å®š 'embedding_types'
    """
    
    MODELS = {
        "embed-english-v3.0": 1024,
        "embed-multilingual-v3.0": 1024,
        "embed-english-light-v3.0": 384,
        "embed-multilingual-light-v3.0": 384,
        "embed-v4.0": 1024,  # æœ€æ–°æ¨¡å‹
    }
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "embed-multilingual-v3.0",
    ):
        self.api_key = api_key
        # æ–°çš„ v2 API ç«¯ç‚¹
        self.base_url = base_url or "https://api.cohere.com/v2"
        self.model = model
        self._dimension = self.MODELS.get(model, 1024)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        # v2 API å‚æ•°æ ¼å¼
        payload = {
            "model": self.model,
            "inputs": texts,  # v2 ä½¿ç”¨ 'inputs' è€Œé 'texts'
            "input_type": "search_document",
            "embedding_types": ["float"],  # v2 éœ€è¦æŒ‡å®šåµŒå…¥ç±»å‹
        }
        
        url = f"{self.base_url.rstrip('/')}/embed"
        
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            results = []
            # v2 API è¿”å›æ ¼å¼: {"embeddings": {"float": [[...], [...]]}, ...}
            embeddings_data = data.get("embeddings", {})
            embeddings = embeddings_data.get("float", []) if isinstance(embeddings_data, dict) else embeddings_data
            
            for embedding in embeddings:
                results.append(EmbeddingResult(
                    embedding=embedding,
                    tokens_used=data.get("meta", {}).get("billed_units", {}).get("input_tokens", 0) // max(len(texts), 1),
                    model=self.model,
                ))
            
            return results


class HuggingFaceEmbedding(EmbeddingProvider):
    """
    HuggingFace Inference Providers åµŒå…¥æœåŠ¡
    
    ä½¿ç”¨æ–°çš„ Router ç«¯ç‚¹ (2025å¹´èµ·):
    https://router.huggingface.co/hf-inference/models/{model}/pipeline/feature-extraction
    """
    
    MODELS = {
        "sentence-transformers/all-MiniLM-L6-v2": 384,
        "sentence-transformers/all-mpnet-base-v2": 768,
        "BAAI/bge-large-zh-v1.5": 1024,
        "BAAI/bge-m3": 1024,
        "BAAI/bge-small-en-v1.5": 384,
        "BAAI/bge-base-en-v1.5": 768,
    }
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "BAAI/bge-m3",
    ):
        self.api_key = api_key
        # æ–°çš„ Router ç«¯ç‚¹
        self.base_url = base_url or "https://router.huggingface.co"
        self.model = model
        self._dimension = self.MODELS.get(model, 1024)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        # æ–°çš„ HuggingFace Router URL æ ¼å¼
        # https://router.huggingface.co/hf-inference/models/{model}/pipeline/feature-extraction
        url = f"{self.base_url.rstrip('/')}/hf-inference/models/{self.model}/pipeline/feature-extraction"
        
        payload = {
            "inputs": texts,
            "options": {
                "wait_for_model": True,
            }
        }
        
        async with httpx.AsyncClient(timeout=120) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            results = []
            # HuggingFace è¿”å›æ ¼å¼: [[embedding1], [embedding2], ...]
            for embedding in data:
                # æœ‰æ—¶å€™è¿”å›çš„æ˜¯åµŒå¥—çš„åˆ—è¡¨
                if isinstance(embedding, list) and len(embedding) > 0:
                    if isinstance(embedding[0], list):
                        # å–å¹³å‡æˆ–ç¬¬ä¸€ä¸ª
                        embedding = embedding[0]
                
                results.append(EmbeddingResult(
                    embedding=embedding,
                    tokens_used=len(texts[len(results)]) // 4,
                    model=self.model,
                ))
            
            return results


class JinaEmbedding(EmbeddingProvider):
    """Jina AI åµŒå…¥æœåŠ¡"""
    
    MODELS = {
        "jina-embeddings-v2-base-code": 768,
        "jina-embeddings-v2-base-en": 768,
        "jina-embeddings-v2-base-zh": 768,
        "jina-embeddings-v2-small-en": 512,
    }
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "jina-embeddings-v2-base-code",
    ):
        self.api_key = api_key
        self.base_url = base_url or "https://api.jina.ai/v1"
        self.model = model
        self._dimension = self.MODELS.get(model, 768)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": self.model,
            "input": texts,
        }
        
        url = f"{self.base_url.rstrip('/')}/embeddings"
        
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            results = []
            for item in data.get("data", []):
                results.append(EmbeddingResult(
                    embedding=item["embedding"],
                    tokens_used=data.get("usage", {}).get("total_tokens", 0) // len(texts),
                    model=self.model,
                ))
            
            return results


class QwenEmbedding(EmbeddingProvider):
    """Qwen åµŒå…¥æœåŠ¡ï¼ˆåŸºäºé˜¿é‡Œäº‘ DashScope embeddings APIï¼‰"""
    
    MODELS = {
        # DashScope Qwen åµŒå…¥æ¨¡å‹åŠå…¶é»˜è®¤ç»´åº¦
        "text-embedding-v4": 1024,  # æ”¯æŒç»´åº¦: 2048, 1536, 1024(é»˜è®¤), 768, 512, 256, 128, 64
        "text-embedding-v3": 1024,  # æ”¯æŒç»´åº¦: 1024(é»˜è®¤), 768, 512, 256, 128, 64
        "text-embedding-v2": 1536,  # æ”¯æŒç»´åº¦: 1536
    }
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: str = "text-embedding-v4",
    ):
        # ä¼˜å…ˆä½¿ç”¨æ˜¾å¼ä¼ å…¥çš„ api_keyï¼Œå…¶æ¬¡ä½¿ç”¨ EMBEDDING_API_KEY/QWEN_API_KEY/LLM_API_KEY
        self.api_key = (
            api_key
            or getattr(settings, "EMBEDDING_API_KEY", None)
            or getattr(settings, "QWEN_API_KEY", None)
            or settings.LLM_API_KEY
        )
        # DashScope å…¼å®¹ OpenAI çš„ embeddings ç«¯ç‚¹
        self.base_url = base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.model = model
        self._dimension = self.MODELS.get(model, 1024)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    async def embed_text(self, text: str) -> EmbeddingResult:
        results = await self.embed_texts([text])
        return results[0]
    
    async def embed_texts(self, texts: List[str]) -> List[EmbeddingResult]:
        if not texts:
            return []
        
        # ä¸ OpenAI æ¥å£ä¿æŒä¸€è‡´çš„æˆªæ–­ç­–ç•¥
        max_length = 8191
        truncated_texts = [text[:max_length] for text in texts]
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": self.model,
            "input": truncated_texts,
            "encoding_format": "float",
        }
        
        url = f"{self.base_url.rstrip('/')}/embeddings"
        
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            data = response.json()
            
            usage = data.get("usage", {}) or {}
            total_tokens = usage.get("total_tokens") or usage.get("prompt_tokens") or 0
            
            results: List[EmbeddingResult] = []
            for item in data.get("data", []):
                results.append(EmbeddingResult(
                    embedding=item["embedding"],
                    tokens_used=total_tokens // max(len(texts), 1),
                    model=self.model,
                ))
            
            return results


class EmbeddingService:
    """
    åµŒå…¥æœåŠ¡
    ç»Ÿä¸€ç®¡ç†åµŒå…¥æ¨¡å‹å’Œç¼“å­˜
    
    æ”¯æŒçš„æä¾›å•†:
    - openai: OpenAI å®˜æ–¹
    - azure: Azure OpenAI
    - ollama: Ollama æœ¬åœ°
    - cohere: Cohere
    - huggingface: HuggingFace Inference API
    - jina: Jina AI
    """
    
    def __init__(
        self,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        cache_enabled: bool = True,
    ):
        """
        åˆå§‹åŒ–åµŒå…¥æœåŠ¡

        Args:
            provider: æä¾›å•† (openai, azure, ollama, cohere, huggingface, jina)
            model: æ¨¡å‹åç§°
            api_key: API Key
            base_url: API Base URL
            cache_enabled: æ˜¯å¦å¯ç”¨ç¼“å­˜
        """
        self.cache_enabled = cache_enabled
        self._cache: Dict[str, List[float]] = {}

        # ç¡®å®šæä¾›å•†ï¼ˆä¿å­˜åŸå§‹å€¼ç”¨äºå±æ€§è®¿é—®ï¼‰
        self.provider = provider or getattr(settings, 'EMBEDDING_PROVIDER', 'openai')
        self.model = model or getattr(settings, 'EMBEDDING_MODEL', 'text-embedding-3-small')
        self.api_key = api_key
        self.base_url = base_url

        # åˆ›å»ºæä¾›å•†å®ä¾‹
        self._provider = self._create_provider(
            provider=self.provider,
            model=self.model,
            api_key=api_key,
            base_url=base_url,
        )

        logger.info(f"Embedding service initialized with {self.provider}/{self.model}")
    
    def _create_provider(
        self,
        provider: str,
        model: str,
        api_key: Optional[str],
        base_url: Optional[str],
    ) -> EmbeddingProvider:
        """åˆ›å»ºåµŒå…¥æä¾›å•†å®ä¾‹"""
        provider = provider.lower()
        
        if provider == "ollama":
            return OllamaEmbedding(base_url=base_url, model=model)
        
        elif provider == "azure":
            return AzureOpenAIEmbedding(api_key=api_key, base_url=base_url, model=model)
        
        elif provider == "cohere":
            return CohereEmbedding(api_key=api_key, base_url=base_url, model=model)
        
        elif provider == "huggingface":
            return HuggingFaceEmbedding(api_key=api_key, base_url=base_url, model=model)
        
        elif provider == "jina":
            return JinaEmbedding(api_key=api_key, base_url=base_url, model=model)
        
        elif provider == "qwen":
            return QwenEmbedding(api_key=api_key, base_url=base_url, model=model)
        
        else:
            # é»˜è®¤ä½¿ç”¨ OpenAI
            return OpenAIEmbedding(api_key=api_key, base_url=base_url, model=model)
    
    @property
    def dimension(self) -> int:
        """åµŒå…¥å‘é‡ç»´åº¦"""
        return self._provider.dimension
    
    def _cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.sha256(text.encode()).hexdigest()[:32]
    
    async def embed(self, text: str) -> List[float]:
        """
        åµŒå…¥å•ä¸ªæ–‡æœ¬
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            åµŒå…¥å‘é‡
        """
        if not text or not text.strip():
            return [0.0] * self.dimension
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_enabled:
            cache_key = self._cache_key(text)
            if cache_key in self._cache:
                return self._cache[cache_key]
        
        result = await self._provider.embed_text(text)
        
        # å­˜å…¥ç¼“å­˜
        if self.cache_enabled:
            self._cache[cache_key] = result.embedding
        
        return result.embedding
    
    async def embed_batch(
        self,
        texts: List[str],
        batch_size: int = 100,
        show_progress: bool = False,
        progress_callback: Optional[callable] = None,
        cancel_check: Optional[callable] = None,
    ) -> List[List[float]]:
        """
        æ‰¹é‡åµŒå…¥æ–‡æœ¬

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (processed, total) å‚æ•°
            cancel_check: å–æ¶ˆæ£€æŸ¥å‡½æ•°ï¼Œè¿”å› True è¡¨ç¤ºåº”è¯¥å–æ¶ˆ

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨

        Raises:
            asyncio.CancelledError: å½“ cancel_check è¿”å› True æ—¶
        """
        if not texts:
            return []

        embeddings = []
        uncached_indices = []
        uncached_texts = []

        # æ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            if not text or not text.strip():
                embeddings.append([0.0] * self.dimension)
                continue

            if self.cache_enabled:
                cache_key = self._cache_key(text)
                if cache_key in self._cache:
                    embeddings.append(self._cache[cache_key])
                    continue

            embeddings.append(None)  # å ä½
            uncached_indices.append(i)
            uncached_texts.append(text)

        # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
        if uncached_texts:
            total_batches = (len(uncached_texts) + batch_size - 1) // batch_size
            processed_batches = 0

            for i in range(0, len(uncached_texts), batch_size):
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦åº”è¯¥å–æ¶ˆ
                if cancel_check and cancel_check():
                    logger.info(f"[Embedding] Cancelled at batch {processed_batches + 1}/{total_batches}")
                    raise asyncio.CancelledError("åµŒå…¥æ“ä½œå·²å–æ¶ˆ")

                batch = uncached_texts[i:i + batch_size]
                batch_indices = uncached_indices[i:i + batch_size]

                try:
                    results = await self._provider.embed_texts(batch)

                    for idx, result in zip(batch_indices, results):
                        embeddings[idx] = result.embedding

                        # å­˜å…¥ç¼“å­˜
                        if self.cache_enabled:
                            cache_key = self._cache_key(texts[idx])
                            self._cache[cache_key] = result.embedding

                except asyncio.CancelledError:
                    # ğŸ”¥ é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸
                    raise
                except Exception as e:
                    logger.error(f"Batch embedding error: {e}")
                    # å¯¹å¤±è´¥çš„ä½¿ç”¨é›¶å‘é‡
                    for idx in batch_indices:
                        if embeddings[idx] is None:
                            embeddings[idx] = [0.0] * self.dimension

                processed_batches += 1

                # ğŸ”¥ è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    processed_count = min(i + batch_size, len(uncached_texts))
                    try:
                        progress_callback(processed_count, len(uncached_texts))
                    except Exception as e:
                        logger.warning(f"Progress callback error: {e}")

                # æ·»åŠ å°å»¶è¿Ÿé¿å…é™æµ
                await asyncio.sleep(0.1)

        # ç¡®ä¿æ²¡æœ‰ None
        return [e if e is not None else [0.0] * self.dimension for e in embeddings]
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
    
    @property
    def cache_size(self) -> int:
        """ç¼“å­˜å¤§å°"""
        return len(self._cache)

