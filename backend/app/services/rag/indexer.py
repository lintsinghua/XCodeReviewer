"""
ä»£ç ç´¢å¼•å™¨
å°†ä»£ç åˆ†å—å¹¶ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
"""

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from pathlib import Path
from dataclasses import dataclass
import json

from .splitter import CodeSplitter, CodeChunk
from .embeddings import EmbeddingService

logger = logging.getLogger(__name__)


# æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶æ‰©å±•å
TEXT_EXTENSIONS = {
    ".py", ".js", ".ts", ".tsx", ".jsx", ".java", ".go", ".rs",
    ".cpp", ".c", ".h", ".cc", ".hh", ".cs", ".php", ".rb",
    ".kt", ".swift", ".sql", ".sh", ".json", ".yml", ".yaml",
    ".xml", ".html", ".css", ".vue", ".svelte", ".md",
}

# æ’é™¤çš„ç›®å½•
EXCLUDE_DIRS = {
    "node_modules", "vendor", "dist", "build", ".git",
    "__pycache__", ".pytest_cache", "coverage", ".nyc_output",
    ".vscode", ".idea", ".vs", "target", "out", "bin", "obj",
    "__MACOSX", ".next", ".nuxt", "venv", "env", ".env",
}

# æ’é™¤çš„æ–‡ä»¶
EXCLUDE_FILES = {
    ".DS_Store", "package-lock.json", "yarn.lock", "pnpm-lock.yaml",
    "Cargo.lock", "poetry.lock", "composer.lock", "Gemfile.lock",
}


@dataclass
class IndexingProgress:
    """ç´¢å¼•è¿›åº¦"""
    total_files: int = 0
    processed_files: int = 0
    total_chunks: int = 0
    indexed_chunks: int = 0
    current_file: str = ""
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
    
    @property
    def progress_percentage(self) -> float:
        if self.total_files == 0:
            return 0.0
        return (self.processed_files / self.total_files) * 100


@dataclass
class IndexingResult:
    """ç´¢å¼•ç»“æœ"""
    success: bool
    total_files: int
    indexed_files: int
    total_chunks: int
    errors: List[str]
    collection_name: str


class VectorStore:
    """å‘é‡å­˜å‚¨æŠ½è±¡åŸºç±»"""
    
    async def initialize(self):
        """åˆå§‹åŒ–å­˜å‚¨"""
        pass
    
    async def add_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ·»åŠ æ–‡æ¡£"""
        raise NotImplementedError
    
    async def query(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢"""
        raise NotImplementedError
    
    async def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        raise NotImplementedError
    
    async def get_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        raise NotImplementedError


class ChromaVectorStore(VectorStore):
    """Chroma å‘é‡å­˜å‚¨"""

    def __init__(
        self,
        collection_name: str,
        persist_directory: Optional[str] = None,
        embedding_config: Optional[Dict[str, Any]] = None,  # ğŸ”¥ æ–°å¢ï¼šembedding é…ç½®
    ):
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_config = embedding_config or {}  # ğŸ”¥ å­˜å‚¨ embedding é…ç½®
        self._client = None
        self._collection = None

    async def initialize(self):
        """åˆå§‹åŒ– Chroma"""
        try:
            import chromadb
            from chromadb.config import Settings

            if self.persist_directory:
                self._client = chromadb.PersistentClient(
                    path=self.persist_directory,
                    settings=Settings(anonymized_telemetry=False),
                )
            else:
                self._client = chromadb.Client(
                    settings=Settings(anonymized_telemetry=False),
                )

            # ğŸ”¥ æ„å»º collection å…ƒæ•°æ®ï¼ŒåŒ…å« embedding é…ç½®
            collection_metadata = {"hnsw:space": "cosine"}
            if self.embedding_config:
                # åœ¨å…ƒæ•°æ®ä¸­è®°å½• embedding é…ç½®
                collection_metadata["embedding_provider"] = self.embedding_config.get("provider", "openai")
                collection_metadata["embedding_model"] = self.embedding_config.get("model", "text-embedding-3-small")
                collection_metadata["embedding_dimension"] = self.embedding_config.get("dimension", 1536)
                if self.embedding_config.get("base_url"):
                    collection_metadata["embedding_base_url"] = self.embedding_config.get("base_url")

            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                metadata=collection_metadata,
            )

            logger.info(f"Chroma collection '{self.collection_name}' initialized")

        except ImportError:
            raise ImportError("chromadb is required. Install with: pip install chromadb")

    def get_embedding_config(self) -> Dict[str, Any]:
        """
        ğŸ”¥ è·å– collection çš„ embedding é…ç½®

        Returns:
            åŒ…å« provider, model, dimension, base_url çš„å­—å…¸
        """
        if not self._collection:
            return {}

        metadata = self._collection.metadata or {}
        return {
            "provider": metadata.get("embedding_provider"),
            "model": metadata.get("embedding_model"),
            "dimension": metadata.get("embedding_dimension"),
            "base_url": metadata.get("embedding_base_url"),
        }
    
    async def add_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ·»åŠ æ–‡æ¡£åˆ° Chroma"""
        if not ids:
            return
        
        # Chroma å¯¹å…ƒæ•°æ®æœ‰é™åˆ¶ï¼Œéœ€è¦æ¸…ç†
        cleaned_metadatas = []
        for meta in metadatas:
            cleaned = {}
            for k, v in meta.items():
                if isinstance(v, (str, int, float, bool)):
                    cleaned[k] = v
                elif isinstance(v, list):
                    # åˆ—è¡¨è½¬ä¸º JSON å­—ç¬¦ä¸²
                    cleaned[k] = json.dumps(v)
                elif v is not None:
                    cleaned[k] = str(v)
            cleaned_metadatas.append(cleaned)
        
        # åˆ†æ‰¹æ·»åŠ ï¼ˆChroma æ‰¹æ¬¡é™åˆ¶ï¼‰
        batch_size = 500
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_documents = documents[i:i + batch_size]
            batch_metadatas = cleaned_metadatas[i:i + batch_size]
            
            await asyncio.to_thread(
                self._collection.add,
                ids=batch_ids,
                embeddings=batch_embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas,
            )
    
    async def query(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ Chroma"""
        result = await asyncio.to_thread(
            self._collection.query,
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where,
            include=["documents", "metadatas", "distances"],
        )
        
        return {
            "ids": result["ids"][0] if result["ids"] else [],
            "documents": result["documents"][0] if result["documents"] else [],
            "metadatas": result["metadatas"][0] if result["metadatas"] else [],
            "distances": result["distances"][0] if result["distances"] else [],
        }
    
    async def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        if self._client and self._collection:
            await asyncio.to_thread(
                self._client.delete_collection,
                name=self.collection_name,
            )
    
    async def get_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        if self._collection:
            return await asyncio.to_thread(self._collection.count)
        return 0


class InMemoryVectorStore(VectorStore):
    """å†…å­˜å‘é‡å­˜å‚¨ï¼ˆç”¨äºæµ‹è¯•æˆ–å°é¡¹ç›®ï¼‰"""
    
    def __init__(self, collection_name: str):
        self.collection_name = collection_name
        self._documents: Dict[str, Dict[str, Any]] = {}
    
    async def initialize(self):
        """åˆå§‹åŒ–"""
        logger.info(f"InMemory vector store '{self.collection_name}' initialized")
    
    async def add_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ·»åŠ æ–‡æ¡£"""
        for id_, emb, doc, meta in zip(ids, embeddings, documents, metadatas):
            self._documents[id_] = {
                "embedding": emb,
                "document": doc,
                "metadata": meta,
            }
    
    async def query(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        import math
        
        def cosine_similarity(a: List[float], b: List[float]) -> float:
            dot = sum(x * y for x, y in zip(a, b))
            norm_a = math.sqrt(sum(x * x for x in a))
            norm_b = math.sqrt(sum(x * x for x in b))
            if norm_a == 0 or norm_b == 0:
                return 0.0
            return dot / (norm_a * norm_b)
        
        results = []
        for id_, data in self._documents.items():
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if where:
                match = True
                for k, v in where.items():
                    if data["metadata"].get(k) != v:
                        match = False
                        break
                if not match:
                    continue
            
            similarity = cosine_similarity(query_embedding, data["embedding"])
            results.append({
                "id": id_,
                "document": data["document"],
                "metadata": data["metadata"],
                "distance": 1 - similarity,  # è½¬æ¢ä¸ºè·ç¦»
            })
        
        # æŒ‰è·ç¦»æ’åº
        results.sort(key=lambda x: x["distance"])
        results = results[:n_results]
        
        return {
            "ids": [r["id"] for r in results],
            "documents": [r["document"] for r in results],
            "metadatas": [r["metadata"] for r in results],
            "distances": [r["distance"] for r in results],
        }
    
    async def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        self._documents.clear()
    
    async def get_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        return len(self._documents)


class CodeIndexer:
    """
    ä»£ç ç´¢å¼•å™¨
    å°†ä»£ç æ–‡ä»¶åˆ†å—ã€åµŒå…¥å¹¶ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
    """

    def __init__(
        self,
        collection_name: str,
        embedding_service: Optional[EmbeddingService] = None,
        vector_store: Optional[VectorStore] = None,
        splitter: Optional[CodeSplitter] = None,
        persist_directory: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨

        Args:
            collection_name: å‘é‡é›†åˆåç§°
            embedding_service: åµŒå…¥æœåŠ¡
            vector_store: å‘é‡å­˜å‚¨
            splitter: ä»£ç åˆ†å—å™¨
            persist_directory: æŒä¹…åŒ–ç›®å½•
        """
        self.collection_name = collection_name
        self.embedding_service = embedding_service or EmbeddingService()
        self.splitter = splitter or CodeSplitter()

        # ğŸ”¥ ä» embedding_service è·å–é…ç½®ï¼Œç”¨äºå­˜å‚¨åˆ° collection å…ƒæ•°æ®
        embedding_config = {
            "provider": getattr(self.embedding_service, 'provider', 'openai'),
            "model": getattr(self.embedding_service, 'model', 'text-embedding-3-small'),
            "dimension": getattr(self.embedding_service, 'dimension', 1536),
            "base_url": getattr(self.embedding_service, 'base_url', None),
        }

        # åˆ›å»ºå‘é‡å­˜å‚¨
        if vector_store:
            self.vector_store = vector_store
        else:
            try:
                self.vector_store = ChromaVectorStore(
                    collection_name=collection_name,
                    persist_directory=persist_directory,
                    embedding_config=embedding_config,  # ğŸ”¥ ä¼ é€’ embedding é…ç½®
                )
            except ImportError:
                logger.warning("Chroma not available, using in-memory store")
                self.vector_store = InMemoryVectorStore(collection_name=collection_name)

        self._initialized = False
    
    async def initialize(self):
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        if not self._initialized:
            await self.vector_store.initialize()
            self._initialized = True
    
    async def index_directory(
        self,
        directory: str,
        exclude_patterns: Optional[List[str]] = None,
        include_patterns: Optional[List[str]] = None,
        progress_callback: Optional[Callable[[IndexingProgress], None]] = None,
    ) -> AsyncGenerator[IndexingProgress, None]:
        """
        ç´¢å¼•ç›®å½•ä¸­çš„ä»£ç æ–‡ä»¶
        
        Args:
            directory: ç›®å½•è·¯å¾„
            exclude_patterns: æ’é™¤æ¨¡å¼
            include_patterns: åŒ…å«æ¨¡å¼
            progress_callback: è¿›åº¦å›è°ƒ
            
        Yields:
            ç´¢å¼•è¿›åº¦
        """
        await self.initialize()
        
        progress = IndexingProgress()
        exclude_patterns = exclude_patterns or []
        
        # æ”¶é›†æ–‡ä»¶
        files = self._collect_files(directory, exclude_patterns, include_patterns)
        progress.total_files = len(files)
        
        logger.info(f"Found {len(files)} files to index in {directory}")
        yield progress
        
        all_chunks: List[CodeChunk] = []
        
        # åˆ†å—å¤„ç†æ–‡ä»¶
        for file_path in files:
            progress.current_file = file_path
            
            try:
                relative_path = os.path.relpath(file_path, directory)
                
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                if not content.strip():
                    progress.processed_files += 1
                    continue
                
                # é™åˆ¶æ–‡ä»¶å¤§å°
                if len(content) > 500000:  # 500KB
                    content = content[:500000]
                
                # åˆ†å—
                chunks = self.splitter.split_file(content, relative_path)
                all_chunks.extend(chunks)
                
                progress.processed_files += 1
                progress.total_chunks = len(all_chunks)
                
                if progress_callback:
                    progress_callback(progress)
                yield progress
                
            except Exception as e:
                logger.warning(f"Error processing {file_path}: {e}")
                progress.errors.append(f"{file_path}: {str(e)}")
                progress.processed_files += 1
        
        logger.info(f"Created {len(all_chunks)} chunks from {len(files)} files")
        
        # æ‰¹é‡åµŒå…¥å’Œç´¢å¼•
        if all_chunks:
            await self._index_chunks(all_chunks, progress)
        
        progress.indexed_chunks = len(all_chunks)
        yield progress
    
    async def index_files(
        self,
        files: List[Dict[str, str]],
        base_path: str = "",
        progress_callback: Optional[Callable[[IndexingProgress], None]] = None,
    ) -> AsyncGenerator[IndexingProgress, None]:
        """
        ç´¢å¼•æ–‡ä»¶åˆ—è¡¨
        
        Args:
            files: æ–‡ä»¶åˆ—è¡¨ [{"path": "...", "content": "..."}]
            base_path: åŸºç¡€è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒ
            
        Yields:
            ç´¢å¼•è¿›åº¦
        """
        await self.initialize()
        
        progress = IndexingProgress()
        progress.total_files = len(files)
        
        all_chunks: List[CodeChunk] = []
        
        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")
            
            progress.current_file = file_path
            
            try:
                if not content.strip():
                    progress.processed_files += 1
                    continue
                
                # é™åˆ¶æ–‡ä»¶å¤§å°
                if len(content) > 500000:
                    content = content[:500000]
                
                # åˆ†å—
                chunks = self.splitter.split_file(content, file_path)
                all_chunks.extend(chunks)
                
                progress.processed_files += 1
                progress.total_chunks = len(all_chunks)
                
                if progress_callback:
                    progress_callback(progress)
                yield progress
                
            except Exception as e:
                logger.warning(f"Error processing {file_path}: {e}")
                progress.errors.append(f"{file_path}: {str(e)}")
                progress.processed_files += 1
        
        # æ‰¹é‡åµŒå…¥å’Œç´¢å¼•
        if all_chunks:
            await self._index_chunks(all_chunks, progress)
        
        progress.indexed_chunks = len(all_chunks)
        yield progress
    
    async def _index_chunks(self, chunks: List[CodeChunk], progress: IndexingProgress):
        """ç´¢å¼•ä»£ç å—"""
        # å‡†å¤‡åµŒå…¥æ–‡æœ¬
        texts = [chunk.to_embedding_text() for chunk in chunks]
        
        logger.info(f"Generating embeddings for {len(texts)} chunks...")
        
        # æ‰¹é‡åµŒå…¥
        embeddings = await self.embedding_service.embed_batch(texts, batch_size=50)
        
        # å‡†å¤‡å…ƒæ•°æ®
        ids = [chunk.id for chunk in chunks]
        documents = [chunk.content for chunk in chunks]
        metadatas = [chunk.to_dict() for chunk in chunks]
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        logger.info(f"Adding {len(chunks)} chunks to vector store...")
        await self.vector_store.add_documents(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
        )
        
        logger.info(f"Indexed {len(chunks)} chunks successfully")
    
    def _collect_files(
        self,
        directory: str,
        exclude_patterns: List[str],
        include_patterns: Optional[List[str]],
    ) -> List[str]:
        """æ”¶é›†éœ€è¦ç´¢å¼•çš„æ–‡ä»¶"""
        import fnmatch
        
        files = []
        
        for root, dirs, filenames in os.walk(directory):
            # è¿‡æ»¤ç›®å½•
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
            
            for filename in filenames:
                # æ£€æŸ¥æ‰©å±•å
                ext = os.path.splitext(filename)[1].lower()
                if ext not in TEXT_EXTENSIONS:
                    continue
                
                # æ£€æŸ¥æ’é™¤æ–‡ä»¶
                if filename in EXCLUDE_FILES:
                    continue
                
                file_path = os.path.join(root, filename)
                relative_path = os.path.relpath(file_path, directory)
                
                # æ£€æŸ¥æ’é™¤æ¨¡å¼
                excluded = False
                for pattern in exclude_patterns:
                    if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(filename, pattern):
                        excluded = True
                        break
                
                if excluded:
                    continue
                
                # æ£€æŸ¥åŒ…å«æ¨¡å¼
                if include_patterns:
                    included = False
                    for pattern in include_patterns:
                        if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(filename, pattern):
                            included = True
                            break
                    if not included:
                        continue
                
                files.append(file_path)
        
        return files
    
    async def get_chunk_count(self) -> int:
        """è·å–å·²ç´¢å¼•çš„ä»£ç å—æ•°é‡"""
        await self.initialize()
        return await self.vector_store.get_count()
    
    async def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        await self.initialize()
        await self.vector_store.delete_collection()
        self._initialized = False

