"""
ä»£ç ç´¢å¼•å™¨
å°†ä»£ç åˆ†å—å¹¶ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“

ğŸ”¥ v2.0 æ”¹è¿›ï¼š
- æ”¯æŒåµŒå…¥æ¨¡å‹å˜æ›´æ£€æµ‹å’Œè‡ªåŠ¨é‡å»º
- æ”¯æŒå¢é‡ç´¢å¼•æ›´æ–°ï¼ˆåŸºäºæ–‡ä»¶ hashï¼‰
- æ”¯æŒç´¢å¼•ç‰ˆæœ¬æ§åˆ¶å’ŒçŠ¶æ€æŸ¥è¯¢
"""

import os
import asyncio
import logging
import hashlib
import time
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable, Set, Tuple
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
import json

from .splitter import CodeSplitter, CodeChunk
from .embeddings import EmbeddingService

logger = logging.getLogger(__name__)

# ç´¢å¼•ç‰ˆæœ¬å·ï¼ˆå½“ç´¢å¼•æ ¼å¼å˜åŒ–æ—¶é€’å¢ï¼‰
INDEX_VERSION = "2.0"


# æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶æ‰©å±•å
TEXT_EXTENSIONS = {
    ".py", ".js", ".ts", ".tsx", ".jsx", ".java", ".go", ".rs",
    ".cpp", ".c", ".h", ".cc", ".hh", ".cs", ".php", ".rb",
    ".kt", ".swift", ".sql", ".sh", ".json", ".yml", ".yaml",
    ".xml", ".html", ".css", ".vue", ".svelte", ".md",
}

# æ’é™¤çš„ç›®å½•
EXCLUDE_DIRS = {
    "node_modules", "vendor", "dist", "build", ".git",
    "__pycache__", ".pytest_cache", "coverage", ".nyc_output",
    ".vscode", ".idea", ".vs", "target", "out", "bin", "obj",
    "__MACOSX", ".next", ".nuxt", "venv", "env", ".env",
}

# æ’é™¤çš„æ–‡ä»¶
EXCLUDE_FILES = {
    ".DS_Store", "package-lock.json", "yarn.lock", "pnpm-lock.yaml",
    "Cargo.lock", "poetry.lock", "composer.lock", "Gemfile.lock",
}


class IndexUpdateMode(Enum):
    """ç´¢å¼•æ›´æ–°æ¨¡å¼"""
    FULL = "full"           # å…¨é‡é‡å»ºï¼šåˆ é™¤æ—§ç´¢å¼•ï¼Œå®Œå…¨é‡æ–°ç´¢å¼•
    INCREMENTAL = "incremental"  # å¢é‡æ›´æ–°ï¼šåªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶
    SMART = "smart"         # æ™ºèƒ½æ¨¡å¼ï¼šæ ¹æ®æƒ…å†µè‡ªåŠ¨é€‰æ‹©


@dataclass
class IndexStatus:
    """ç´¢å¼•çŠ¶æ€ä¿¡æ¯"""
    collection_name: str
    exists: bool = False
    index_version: str = ""
    chunk_count: int = 0
    file_count: int = 0
    created_at: float = 0.0
    updated_at: float = 0.0
    embedding_provider: str = ""
    embedding_model: str = ""
    embedding_dimension: int = 0
    project_hash: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "collection_name": self.collection_name,
            "exists": self.exists,
            "index_version": self.index_version,
            "chunk_count": self.chunk_count,
            "file_count": self.file_count,
            "created_at": self.created_at,
            "updated_at": self.updated_at,
            "embedding_provider": self.embedding_provider,
            "embedding_model": self.embedding_model,
            "embedding_dimension": self.embedding_dimension,
            "project_hash": self.project_hash,
        }


@dataclass
class IndexingProgress:
    """ç´¢å¼•è¿›åº¦"""
    total_files: int = 0
    processed_files: int = 0
    total_chunks: int = 0
    indexed_chunks: int = 0
    current_file: str = ""
    errors: List[str] = None
    # ğŸ”¥ æ–°å¢ï¼šå¢é‡æ›´æ–°ç»Ÿè®¡
    added_files: int = 0
    updated_files: int = 0
    deleted_files: int = 0
    skipped_files: int = 0
    update_mode: str = "full"

    def __post_init__(self):
        if self.errors is None:
            self.errors = []

    @property
    def progress_percentage(self) -> float:
        if self.total_files == 0:
            return 0.0
        return (self.processed_files / self.total_files) * 100


@dataclass
class IndexingResult:
    """ç´¢å¼•ç»“æœ"""
    success: bool
    total_files: int
    indexed_files: int
    total_chunks: int
    errors: List[str]
    collection_name: str


class VectorStore:
    """å‘é‡å­˜å‚¨æŠ½è±¡åŸºç±»"""

    async def initialize(self):
        """åˆå§‹åŒ–å­˜å‚¨"""
        pass

    async def add_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ·»åŠ æ–‡æ¡£"""
        raise NotImplementedError

    async def upsert_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ›´æ–°æˆ–æ’å…¥æ–‡æ¡£"""
        raise NotImplementedError

    async def delete_by_file_path(self, file_path: str) -> int:
        """åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£ï¼Œè¿”å›åˆ é™¤æ•°é‡"""
        raise NotImplementedError

    async def delete_by_ids(self, ids: List[str]) -> int:
        """åˆ é™¤æŒ‡å®š ID çš„æ–‡æ¡£"""
        raise NotImplementedError

    async def query(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢"""
        raise NotImplementedError

    async def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        raise NotImplementedError

    async def get_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        raise NotImplementedError

    async def get_all_file_paths(self) -> Set[str]:
        """è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ä»¶è·¯å¾„"""
        raise NotImplementedError

    async def get_file_hashes(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰æ–‡ä»¶çš„ hash æ˜ å°„ {file_path: hash}"""
        raise NotImplementedError

    def get_collection_metadata(self) -> Dict[str, Any]:
        """è·å– collection å…ƒæ•°æ®"""
        raise NotImplementedError


class ChromaVectorStore(VectorStore):
    """
    Chroma å‘é‡å­˜å‚¨

    ğŸ”¥ v2.0 æ”¹è¿›ï¼š
    - æ”¯æŒ embedding é…ç½®å˜æ›´æ£€æµ‹
    - æ”¯æŒå¢é‡æ›´æ–°ï¼ˆupsertã€deleteï¼‰
    - æ”¯æŒæ–‡ä»¶çº§åˆ«çš„ç´¢å¼•ç®¡ç†
    """

    def __init__(
        self,
        collection_name: str,
        persist_directory: Optional[str] = None,
        embedding_config: Optional[Dict[str, Any]] = None,
    ):
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_config = embedding_config or {}
        self._client = None
        self._collection = None
        self._is_new_collection = False

    async def initialize(self, force_recreate: bool = False):
        """
        åˆå§‹åŒ– Chroma

        Args:
            force_recreate: æ˜¯å¦å¼ºåˆ¶é‡å»º collection
        """
        try:
            import chromadb
            from chromadb.config import Settings

            if self.persist_directory:
                self._client = chromadb.PersistentClient(
                    path=self.persist_directory,
                    settings=Settings(anonymized_telemetry=False),
                )
            else:
                self._client = chromadb.Client(
                    settings=Settings(anonymized_telemetry=False),
                )

            # æ£€æŸ¥ collection æ˜¯å¦å­˜åœ¨
            existing_collections = [c.name for c in self._client.list_collections()]
            collection_exists = self.collection_name in existing_collections

            # å¦‚æœéœ€è¦å¼ºåˆ¶é‡å»ºï¼Œå…ˆåˆ é™¤
            if force_recreate and collection_exists:
                logger.info(f"ğŸ—‘ï¸ å¼ºåˆ¶é‡å»º: åˆ é™¤æ—§ collection '{self.collection_name}'")
                self._client.delete_collection(name=self.collection_name)
                collection_exists = False

            # æ„å»º collection å…ƒæ•°æ®
            current_time = time.time()
            collection_metadata = {
                "hnsw:space": "cosine",
                "index_version": INDEX_VERSION,
            }

            if self.embedding_config:
                collection_metadata["embedding_provider"] = self.embedding_config.get("provider", "openai")
                collection_metadata["embedding_model"] = self.embedding_config.get("model", "text-embedding-3-small")
                collection_metadata["embedding_dimension"] = self.embedding_config.get("dimension", 1536)
                if self.embedding_config.get("base_url"):
                    collection_metadata["embedding_base_url"] = self.embedding_config.get("base_url")

            if collection_exists:
                # è·å–ç°æœ‰ collection
                self._collection = self._client.get_collection(name=self.collection_name)
                self._is_new_collection = False
                logger.info(f"ğŸ“‚ è·å–ç°æœ‰ collection '{self.collection_name}'")
            else:
                # åˆ›å»ºæ–° collection
                collection_metadata["created_at"] = current_time
                collection_metadata["updated_at"] = current_time
                self._collection = self._client.create_collection(
                    name=self.collection_name,
                    metadata=collection_metadata,
                )
                self._is_new_collection = True
                logger.info(f"âœ¨ åˆ›å»ºæ–° collection '{self.collection_name}'")

        except ImportError:
            raise ImportError("chromadb is required. Install with: pip install chromadb")

    @property
    def is_new_collection(self) -> bool:
        """æ˜¯å¦æ˜¯æ–°åˆ›å»ºçš„ collection"""
        return self._is_new_collection

    def get_embedding_config(self) -> Dict[str, Any]:
        """è·å– collection çš„ embedding é…ç½®"""
        if not self._collection:
            return {}

        metadata = self._collection.metadata or {}
        return {
            "provider": metadata.get("embedding_provider"),
            "model": metadata.get("embedding_model"),
            "dimension": metadata.get("embedding_dimension"),
            "base_url": metadata.get("embedding_base_url"),
        }

    def get_collection_metadata(self) -> Dict[str, Any]:
        """è·å– collection å®Œæ•´å…ƒæ•°æ®"""
        if not self._collection:
            return {}
        return dict(self._collection.metadata or {})

    def _clean_metadatas(self, metadatas: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ¸…ç†å…ƒæ•°æ®ï¼Œç¡®ä¿ç¬¦åˆ Chroma è¦æ±‚"""
        cleaned_metadatas = []
        for meta in metadatas:
            cleaned = {}
            for k, v in meta.items():
                if isinstance(v, (str, int, float, bool)):
                    cleaned[k] = v
                elif isinstance(v, list):
                    cleaned[k] = json.dumps(v)
                elif v is not None:
                    cleaned[k] = str(v)
            cleaned_metadatas.append(cleaned)
        return cleaned_metadatas

    async def add_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ·»åŠ æ–‡æ¡£åˆ° Chroma"""
        if not ids:
            return

        cleaned_metadatas = self._clean_metadatas(metadatas)

        # åˆ†æ‰¹æ·»åŠ ï¼ˆChroma æ‰¹æ¬¡é™åˆ¶ï¼‰
        batch_size = 500
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_documents = documents[i:i + batch_size]
            batch_metadatas = cleaned_metadatas[i:i + batch_size]

            await asyncio.to_thread(
                self._collection.add,
                ids=batch_ids,
                embeddings=batch_embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas,
            )

    async def upsert_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ›´æ–°æˆ–æ’å…¥æ–‡æ¡£ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰"""
        if not ids:
            return

        cleaned_metadatas = self._clean_metadatas(metadatas)

        # åˆ†æ‰¹ upsert
        batch_size = 500
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i:i + batch_size]
            batch_embeddings = embeddings[i:i + batch_size]
            batch_documents = documents[i:i + batch_size]
            batch_metadatas = cleaned_metadatas[i:i + batch_size]

            await asyncio.to_thread(
                self._collection.upsert,
                ids=batch_ids,
                embeddings=batch_embeddings,
                documents=batch_documents,
                metadatas=batch_metadatas,
            )

    async def delete_by_file_path(self, file_path: str) -> int:
        """åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£"""
        if not self._collection:
            return 0

        try:
            # æŸ¥è¯¢è¯¥æ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£
            result = await asyncio.to_thread(
                self._collection.get,
                where={"file_path": file_path},
            )

            ids_to_delete = result.get("ids", [])
            if ids_to_delete:
                await asyncio.to_thread(
                    self._collection.delete,
                    ids=ids_to_delete,
                )
                logger.debug(f"åˆ é™¤æ–‡ä»¶ '{file_path}' çš„ {len(ids_to_delete)} ä¸ªæ–‡æ¡£")

            return len(ids_to_delete)
        except Exception as e:
            logger.warning(f"åˆ é™¤æ–‡ä»¶æ–‡æ¡£å¤±è´¥: {e}")
            return 0

    async def delete_by_ids(self, ids: List[str]) -> int:
        """åˆ é™¤æŒ‡å®š ID çš„æ–‡æ¡£"""
        if not self._collection or not ids:
            return 0

        try:
            await asyncio.to_thread(
                self._collection.delete,
                ids=ids,
            )
            return len(ids)
        except Exception as e:
            logger.warning(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return 0

    async def query(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ Chroma"""
        result = await asyncio.to_thread(
            self._collection.query,
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where,
            include=["documents", "metadatas", "distances"],
        )

        return {
            "ids": result["ids"][0] if result["ids"] else [],
            "documents": result["documents"][0] if result["documents"] else [],
            "metadatas": result["metadatas"][0] if result["metadatas"] else [],
            "distances": result["distances"][0] if result["distances"] else [],
        }

    async def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        if self._client and self._collection:
            await asyncio.to_thread(
                self._client.delete_collection,
                name=self.collection_name,
            )
            self._collection = None

    async def get_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        if self._collection:
            return await asyncio.to_thread(self._collection.count)
        return 0

    async def get_all_file_paths(self) -> Set[str]:
        """è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ä»¶è·¯å¾„"""
        if not self._collection:
            return set()

        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£çš„å…ƒæ•°æ®
            result = await asyncio.to_thread(
                self._collection.get,
                include=["metadatas"],
            )

            file_paths = set()
            for meta in result.get("metadatas", []):
                if meta and "file_path" in meta:
                    file_paths.add(meta["file_path"])

            return file_paths
        except Exception as e:
            logger.warning(f"è·å–æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return set()

    async def get_file_hashes(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰æ–‡ä»¶çš„ hash æ˜ å°„ {file_path: file_hash}"""
        if not self._collection:
            return {}

        try:
            result = await asyncio.to_thread(
                self._collection.get,
                include=["metadatas"],
            )

            file_hashes = {}
            for meta in result.get("metadatas", []):
                if meta:
                    file_path = meta.get("file_path")
                    file_hash = meta.get("file_hash")
                    if file_path and file_hash:
                        # åŒä¸€æ–‡ä»¶å¯èƒ½æœ‰å¤šä¸ª chunkï¼Œhash åº”è¯¥ç›¸åŒ
                        file_hashes[file_path] = file_hash

            return file_hashes
        except Exception as e:
            logger.warning(f"è·å–æ–‡ä»¶ hash å¤±è´¥: {e}")
            return {}

    async def update_collection_metadata(self, updates: Dict[str, Any]):
        """æ›´æ–° collection å…ƒæ•°æ®"""
        if not self._collection:
            return

        try:
            current_metadata = dict(self._collection.metadata or {})
            current_metadata.update(updates)
            current_metadata["updated_at"] = time.time()

            # Chroma ä¸æ”¯æŒç›´æ¥æ›´æ–°å…ƒæ•°æ®ï¼Œéœ€è¦é€šè¿‡ä¿®æ”¹ collection
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ modify æ–¹æ³•
            await asyncio.to_thread(
                self._collection.modify,
                metadata=current_metadata,
            )
        except Exception as e:
            logger.warning(f"æ›´æ–° collection å…ƒæ•°æ®å¤±è´¥: {e}")


class InMemoryVectorStore(VectorStore):
    """å†…å­˜å‘é‡å­˜å‚¨ï¼ˆç”¨äºæµ‹è¯•æˆ–å°é¡¹ç›®ï¼‰"""

    def __init__(self, collection_name: str, embedding_config: Optional[Dict[str, Any]] = None):
        self.collection_name = collection_name
        self.embedding_config = embedding_config or {}
        self._documents: Dict[str, Dict[str, Any]] = {}
        self._metadata: Dict[str, Any] = {
            "created_at": time.time(),
            "index_version": INDEX_VERSION,
        }
        self._is_new_collection = True

    async def initialize(self, force_recreate: bool = False):
        """åˆå§‹åŒ–"""
        if force_recreate:
            self._documents.clear()
            self._is_new_collection = True
        logger.info(f"InMemory vector store '{self.collection_name}' initialized")

    @property
    def is_new_collection(self) -> bool:
        return self._is_new_collection

    def get_embedding_config(self) -> Dict[str, Any]:
        return self.embedding_config

    def get_collection_metadata(self) -> Dict[str, Any]:
        return self._metadata

    async def add_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ·»åŠ æ–‡æ¡£"""
        for id_, emb, doc, meta in zip(ids, embeddings, documents, metadatas):
            self._documents[id_] = {
                "embedding": emb,
                "document": doc,
                "metadata": meta,
            }
        self._is_new_collection = False

    async def upsert_documents(
        self,
        ids: List[str],
        embeddings: List[List[float]],
        documents: List[str],
        metadatas: List[Dict[str, Any]],
    ):
        """æ›´æ–°æˆ–æ’å…¥æ–‡æ¡£"""
        await self.add_documents(ids, embeddings, documents, metadatas)

    async def delete_by_file_path(self, file_path: str) -> int:
        """åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£"""
        ids_to_delete = [
            id_ for id_, data in self._documents.items()
            if data["metadata"].get("file_path") == file_path
        ]
        for id_ in ids_to_delete:
            del self._documents[id_]
        return len(ids_to_delete)

    async def delete_by_ids(self, ids: List[str]) -> int:
        """åˆ é™¤æŒ‡å®š ID çš„æ–‡æ¡£"""
        count = 0
        for id_ in ids:
            if id_ in self._documents:
                del self._documents[id_]
                count += 1
        return count

    async def query(
        self,
        query_embedding: List[float],
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        import math

        def cosine_similarity(a: List[float], b: List[float]) -> float:
            dot = sum(x * y for x, y in zip(a, b))
            norm_a = math.sqrt(sum(x * x for x in a))
            norm_b = math.sqrt(sum(x * x for x in b))
            if norm_a == 0 or norm_b == 0:
                return 0.0
            return dot / (norm_a * norm_b)

        results = []
        for id_, data in self._documents.items():
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶
            if where:
                match = True
                for k, v in where.items():
                    if data["metadata"].get(k) != v:
                        match = False
                        break
                if not match:
                    continue

            similarity = cosine_similarity(query_embedding, data["embedding"])
            results.append({
                "id": id_,
                "document": data["document"],
                "metadata": data["metadata"],
                "distance": 1 - similarity,
            })

        results.sort(key=lambda x: x["distance"])
        results = results[:n_results]

        return {
            "ids": [r["id"] for r in results],
            "documents": [r["document"] for r in results],
            "metadatas": [r["metadata"] for r in results],
            "distances": [r["distance"] for r in results],
        }

    async def delete_collection(self):
        """åˆ é™¤é›†åˆ"""
        self._documents.clear()

    async def get_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        return len(self._documents)

    async def get_all_file_paths(self) -> Set[str]:
        """è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ä»¶è·¯å¾„"""
        return {
            data["metadata"].get("file_path")
            for data in self._documents.values()
            if data["metadata"].get("file_path")
        }

    async def get_file_hashes(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰æ–‡ä»¶çš„ hash æ˜ å°„"""
        file_hashes = {}
        for data in self._documents.values():
            file_path = data["metadata"].get("file_path")
            file_hash = data["metadata"].get("file_hash")
            if file_path and file_hash:
                file_hashes[file_path] = file_hash
        return file_hashes

    async def update_collection_metadata(self, updates: Dict[str, Any]):
        """æ›´æ–° collection å…ƒæ•°æ®"""
        self._metadata.update(updates)
        self._metadata["updated_at"] = time.time()


class CodeIndexer:
    """
    ä»£ç ç´¢å¼•å™¨
    å°†ä»£ç æ–‡ä»¶åˆ†å—ã€åµŒå…¥å¹¶ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“

    ğŸ”¥ v2.0 æ”¹è¿›ï¼š
    - è‡ªåŠ¨æ£€æµ‹ embedding æ¨¡å‹å˜æ›´å¹¶é‡å»ºç´¢å¼•
    - æ”¯æŒå¢é‡ç´¢å¼•æ›´æ–°ï¼ˆåŸºäºæ–‡ä»¶ hashï¼‰
    - æ”¯æŒç´¢å¼•çŠ¶æ€æŸ¥è¯¢
    """

    def __init__(
        self,
        collection_name: str,
        embedding_service: Optional[EmbeddingService] = None,
        vector_store: Optional[VectorStore] = None,
        splitter: Optional[CodeSplitter] = None,
        persist_directory: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨

        Args:
            collection_name: å‘é‡é›†åˆåç§°
            embedding_service: åµŒå…¥æœåŠ¡
            vector_store: å‘é‡å­˜å‚¨
            splitter: ä»£ç åˆ†å—å™¨
            persist_directory: æŒä¹…åŒ–ç›®å½•
        """
        self.collection_name = collection_name
        self.embedding_service = embedding_service or EmbeddingService()
        self.splitter = splitter or CodeSplitter()
        self.persist_directory = persist_directory

        # ä» embedding_service è·å–é…ç½®
        self.embedding_config = {
            "provider": getattr(self.embedding_service, 'provider', 'openai'),
            "model": getattr(self.embedding_service, 'model', 'text-embedding-3-small'),
            "dimension": getattr(self.embedding_service, 'dimension', 1536),
            "base_url": getattr(self.embedding_service, 'base_url', None),
        }

        # åˆ›å»ºå‘é‡å­˜å‚¨
        if vector_store:
            self.vector_store = vector_store
        else:
            try:
                self.vector_store = ChromaVectorStore(
                    collection_name=collection_name,
                    persist_directory=persist_directory,
                    embedding_config=self.embedding_config,
                )
            except ImportError:
                logger.warning("Chroma not available, using in-memory store")
                self.vector_store = InMemoryVectorStore(
                    collection_name=collection_name,
                    embedding_config=self.embedding_config,
                )

        self._initialized = False
        self._needs_rebuild = False
        self._rebuild_reason = ""

    async def initialize(self, force_rebuild: bool = False) -> Tuple[bool, str]:
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨ï¼Œæ£€æµ‹æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•

        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º

        Returns:
            (needs_rebuild, reason) - æ˜¯å¦éœ€è¦é‡å»ºåŠåŸå› 
        """
        if self._initialized and not force_rebuild:
            return self._needs_rebuild, self._rebuild_reason

        # å…ˆåˆå§‹åŒ– vector_storeï¼ˆä¸å¼ºåˆ¶é‡å»ºï¼Œåªæ˜¯è·å–ç°æœ‰ collectionï¼‰
        await self.vector_store.initialize(force_recreate=False)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        self._needs_rebuild, self._rebuild_reason = await self._check_rebuild_needed()

        if force_rebuild:
            self._needs_rebuild = True
            self._rebuild_reason = "ç”¨æˆ·å¼ºåˆ¶é‡å»º"

        # å¦‚æœéœ€è¦é‡å»ºï¼Œé‡æ–°åˆå§‹åŒ– vector_storeï¼ˆå¼ºåˆ¶é‡å»ºï¼‰
        if self._needs_rebuild:
            logger.info(f"ğŸ”„ éœ€è¦é‡å»ºç´¢å¼•: {self._rebuild_reason}")
            await self.vector_store.initialize(force_recreate=True)

        self._initialized = True
        return self._needs_rebuild, self._rebuild_reason

    async def _check_rebuild_needed(self) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•

        Returns:
            (needs_rebuild, reason)
        """
        # å¦‚æœæ˜¯æ–° collectionï¼Œä¸éœ€è¦é‡å»ºï¼ˆå› ä¸ºæœ¬æ¥å°±æ˜¯ç©ºçš„ï¼‰
        if hasattr(self.vector_store, 'is_new_collection') and self.vector_store.is_new_collection:
            return False, ""

        # è·å–ç°æœ‰ collection çš„é…ç½®
        stored_config = self.vector_store.get_embedding_config()
        stored_metadata = self.vector_store.get_collection_metadata()

        # æ£€æŸ¥ç´¢å¼•ç‰ˆæœ¬
        stored_version = stored_metadata.get("index_version", "1.0")
        if stored_version != INDEX_VERSION:
            return True, f"ç´¢å¼•ç‰ˆæœ¬å˜æ›´: {stored_version} -> {INDEX_VERSION}"

        # æ£€æŸ¥ embedding æä¾›å•†
        stored_provider = stored_config.get("provider")
        current_provider = self.embedding_config.get("provider")
        if stored_provider and current_provider and stored_provider != current_provider:
            return True, f"Embedding æä¾›å•†å˜æ›´: {stored_provider} -> {current_provider}"

        # æ£€æŸ¥ embedding æ¨¡å‹
        stored_model = stored_config.get("model")
        current_model = self.embedding_config.get("model")
        if stored_model and current_model and stored_model != current_model:
            return True, f"Embedding æ¨¡å‹å˜æ›´: {stored_model} -> {current_model}"

        # æ£€æŸ¥ç»´åº¦
        stored_dimension = stored_config.get("dimension")
        current_dimension = self.embedding_config.get("dimension")
        if stored_dimension and current_dimension and stored_dimension != current_dimension:
            return True, f"Embedding ç»´åº¦å˜æ›´: {stored_dimension} -> {current_dimension}"

        return False, ""

    async def get_index_status(self) -> IndexStatus:
        """è·å–ç´¢å¼•çŠ¶æ€"""
        await self.initialize()

        metadata = self.vector_store.get_collection_metadata()
        embedding_config = self.vector_store.get_embedding_config()
        chunk_count = await self.vector_store.get_count()
        file_paths = await self.vector_store.get_all_file_paths()

        return IndexStatus(
            collection_name=self.collection_name,
            exists=chunk_count > 0,
            index_version=metadata.get("index_version", ""),
            chunk_count=chunk_count,
            file_count=len(file_paths),
            created_at=metadata.get("created_at", 0),
            updated_at=metadata.get("updated_at", 0),
            embedding_provider=embedding_config.get("provider", ""),
            embedding_model=embedding_config.get("model", ""),
            embedding_dimension=embedding_config.get("dimension", 0),
            project_hash=metadata.get("project_hash", ""),
        )

    async def smart_index_directory(
        self,
        directory: str,
        exclude_patterns: Optional[List[str]] = None,
        include_patterns: Optional[List[str]] = None,
        update_mode: IndexUpdateMode = IndexUpdateMode.SMART,
        progress_callback: Optional[Callable[[IndexingProgress], None]] = None,
    ) -> AsyncGenerator[IndexingProgress, None]:
        """
        æ™ºèƒ½ç´¢å¼•ç›®å½•

        Args:
            directory: ç›®å½•è·¯å¾„
            exclude_patterns: æ’é™¤æ¨¡å¼
            include_patterns: åŒ…å«æ¨¡å¼
            update_mode: æ›´æ–°æ¨¡å¼
            progress_callback: è¿›åº¦å›è°ƒ

        Yields:
            ç´¢å¼•è¿›åº¦
        """
        # åˆå§‹åŒ–å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        needs_rebuild, rebuild_reason = await self.initialize()

        progress = IndexingProgress()
        exclude_patterns = exclude_patterns or []

        # ç¡®å®šå®é™…çš„æ›´æ–°æ¨¡å¼
        if update_mode == IndexUpdateMode.SMART:
            if needs_rebuild:
                actual_mode = IndexUpdateMode.FULL
                logger.info(f"ğŸ”„ æ™ºèƒ½æ¨¡å¼: é€‰æ‹©å…¨é‡é‡å»º (åŸå› : {rebuild_reason})")
            else:
                actual_mode = IndexUpdateMode.INCREMENTAL
                logger.info("ğŸ“ æ™ºèƒ½æ¨¡å¼: é€‰æ‹©å¢é‡æ›´æ–°")
        else:
            actual_mode = update_mode

        progress.update_mode = actual_mode.value

        if actual_mode == IndexUpdateMode.FULL:
            # å…¨é‡é‡å»º
            async for p in self._full_index(directory, exclude_patterns, include_patterns, progress, progress_callback):
                yield p
        else:
            # å¢é‡æ›´æ–°
            async for p in self._incremental_index(directory, exclude_patterns, include_patterns, progress, progress_callback):
                yield p

    async def _full_index(
        self,
        directory: str,
        exclude_patterns: List[str],
        include_patterns: Optional[List[str]],
        progress: IndexingProgress,
        progress_callback: Optional[Callable[[IndexingProgress], None]],
    ) -> AsyncGenerator[IndexingProgress, None]:
        """å…¨é‡ç´¢å¼•"""
        logger.info("ğŸ”„ å¼€å§‹å…¨é‡ç´¢å¼•...")

        # æ”¶é›†æ–‡ä»¶
        files = self._collect_files(directory, exclude_patterns, include_patterns)
        progress.total_files = len(files)

        logger.info(f"ğŸ“ å‘ç° {len(files)} ä¸ªæ–‡ä»¶å¾…ç´¢å¼•")
        yield progress

        all_chunks: List[CodeChunk] = []
        file_hashes: Dict[str, str] = {}

        # åˆ†å—å¤„ç†æ–‡ä»¶
        for file_path in files:
            progress.current_file = file_path

            try:
                relative_path = os.path.relpath(file_path, directory)

                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                if not content.strip():
                    progress.processed_files += 1
                    progress.skipped_files += 1
                    continue

                # è®¡ç®—æ–‡ä»¶ hash
                file_hash = hashlib.md5(content.encode()).hexdigest()
                file_hashes[relative_path] = file_hash

                # é™åˆ¶æ–‡ä»¶å¤§å°
                if len(content) > 500000:
                    content = content[:500000]

                # åˆ†å—
                chunks = self.splitter.split_file(content, relative_path)

                # ä¸ºæ¯ä¸ª chunk æ·»åŠ  file_hash
                for chunk in chunks:
                    chunk.metadata["file_hash"] = file_hash

                all_chunks.extend(chunks)

                progress.processed_files += 1
                progress.added_files += 1
                progress.total_chunks = len(all_chunks)

                if progress_callback:
                    progress_callback(progress)
                yield progress

            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                progress.errors.append(f"{file_path}: {str(e)}")
                progress.processed_files += 1

        logger.info(f"ğŸ“ åˆ›å»ºäº† {len(all_chunks)} ä¸ªä»£ç å—")

        # æ‰¹é‡åµŒå…¥å’Œç´¢å¼•
        if all_chunks:
            await self._index_chunks(all_chunks, progress, use_upsert=False)

        # æ›´æ–° collection å…ƒæ•°æ®
        project_hash = hashlib.md5(json.dumps(sorted(file_hashes.items())).encode()).hexdigest()
        await self.vector_store.update_collection_metadata({
            "project_hash": project_hash,
            "file_count": len(file_hashes),
        })

        progress.indexed_chunks = len(all_chunks)
        logger.info(f"âœ… å…¨é‡ç´¢å¼•å®Œæˆ: {progress.added_files} ä¸ªæ–‡ä»¶, {len(all_chunks)} ä¸ªä»£ç å—")
        yield progress

    async def _incremental_index(
        self,
        directory: str,
        exclude_patterns: List[str],
        include_patterns: Optional[List[str]],
        progress: IndexingProgress,
        progress_callback: Optional[Callable[[IndexingProgress], None]],
    ) -> AsyncGenerator[IndexingProgress, None]:
        """å¢é‡ç´¢å¼•"""
        logger.info("ğŸ“ å¼€å§‹å¢é‡ç´¢å¼•...")

        # è·å–å·²ç´¢å¼•æ–‡ä»¶çš„ hash
        indexed_file_hashes = await self.vector_store.get_file_hashes()
        indexed_files = set(indexed_file_hashes.keys())

        # æ”¶é›†å½“å‰æ–‡ä»¶
        current_files = self._collect_files(directory, exclude_patterns, include_patterns)
        current_file_map: Dict[str, str] = {}  # relative_path -> absolute_path

        for file_path in current_files:
            relative_path = os.path.relpath(file_path, directory)
            current_file_map[relative_path] = file_path

        current_file_set = set(current_file_map.keys())

        # è®¡ç®—å·®å¼‚
        files_to_add = current_file_set - indexed_files
        files_to_delete = indexed_files - current_file_set
        files_to_check = current_file_set & indexed_files

        # æ£€æŸ¥éœ€è¦æ›´æ–°çš„æ–‡ä»¶ï¼ˆhash å˜åŒ–ï¼‰
        files_to_update: Set[str] = set()
        for relative_path in files_to_check:
            file_path = current_file_map[relative_path]
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                current_hash = hashlib.md5(content.encode()).hexdigest()
                if current_hash != indexed_file_hashes.get(relative_path):
                    files_to_update.add(relative_path)
            except Exception:
                files_to_update.add(relative_path)

        total_operations = len(files_to_add) + len(files_to_delete) + len(files_to_update)
        progress.total_files = total_operations

        logger.info(f"ğŸ“Š å¢é‡æ›´æ–°: æ–°å¢ {len(files_to_add)}, åˆ é™¤ {len(files_to_delete)}, æ›´æ–° {len(files_to_update)}")
        yield progress

        # åˆ é™¤å·²ç§»é™¤çš„æ–‡ä»¶
        for relative_path in files_to_delete:
            progress.current_file = f"åˆ é™¤: {relative_path}"
            deleted_count = await self.vector_store.delete_by_file_path(relative_path)
            progress.deleted_files += 1
            progress.processed_files += 1
            logger.debug(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶ '{relative_path}' çš„ {deleted_count} ä¸ªä»£ç å—")

            if progress_callback:
                progress_callback(progress)
            yield progress

        # å¤„ç†æ–°å¢å’Œæ›´æ–°çš„æ–‡ä»¶
        files_to_process = files_to_add | files_to_update
        all_chunks: List[CodeChunk] = []
        file_hashes: Dict[str, str] = dict(indexed_file_hashes)

        for relative_path in files_to_process:
            file_path = current_file_map[relative_path]
            progress.current_file = relative_path
            is_update = relative_path in files_to_update

            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                if not content.strip():
                    progress.processed_files += 1
                    progress.skipped_files += 1
                    continue

                # å¦‚æœæ˜¯æ›´æ–°ï¼Œå…ˆåˆ é™¤æ—§çš„
                if is_update:
                    await self.vector_store.delete_by_file_path(relative_path)

                # è®¡ç®—æ–‡ä»¶ hash
                file_hash = hashlib.md5(content.encode()).hexdigest()
                file_hashes[relative_path] = file_hash

                # é™åˆ¶æ–‡ä»¶å¤§å°
                if len(content) > 500000:
                    content = content[:500000]

                # åˆ†å—
                chunks = self.splitter.split_file(content, relative_path)

                # ä¸ºæ¯ä¸ª chunk æ·»åŠ  file_hash
                for chunk in chunks:
                    chunk.metadata["file_hash"] = file_hash

                all_chunks.extend(chunks)

                progress.processed_files += 1
                if is_update:
                    progress.updated_files += 1
                else:
                    progress.added_files += 1
                progress.total_chunks += len(chunks)

                if progress_callback:
                    progress_callback(progress)
                yield progress

            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                progress.errors.append(f"{file_path}: {str(e)}")
                progress.processed_files += 1

        # æ‰¹é‡åµŒå…¥å’Œç´¢å¼•æ–°çš„ä»£ç å—
        if all_chunks:
            await self._index_chunks(all_chunks, progress, use_upsert=True)

        # æ›´æ–° collection å…ƒæ•°æ®
        # ç§»é™¤å·²åˆ é™¤æ–‡ä»¶çš„ hash
        for relative_path in files_to_delete:
            file_hashes.pop(relative_path, None)

        project_hash = hashlib.md5(json.dumps(sorted(file_hashes.items())).encode()).hexdigest()
        await self.vector_store.update_collection_metadata({
            "project_hash": project_hash,
            "file_count": len(file_hashes),
        })

        progress.indexed_chunks = len(all_chunks)
        logger.info(
            f"âœ… å¢é‡ç´¢å¼•å®Œæˆ: æ–°å¢ {progress.added_files}, "
            f"æ›´æ–° {progress.updated_files}, åˆ é™¤ {progress.deleted_files}"
        )
        yield progress

    # ä¿ç•™åŸæœ‰çš„ index_directory æ–¹æ³•ä½œä¸ºå…¼å®¹
    async def index_directory(
        self,
        directory: str,
        exclude_patterns: Optional[List[str]] = None,
        include_patterns: Optional[List[str]] = None,
        progress_callback: Optional[Callable[[IndexingProgress], None]] = None,
    ) -> AsyncGenerator[IndexingProgress, None]:
        """
        ç´¢å¼•ç›®å½•ï¼ˆä½¿ç”¨æ™ºèƒ½æ¨¡å¼ï¼‰

        Args:
            directory: ç›®å½•è·¯å¾„
            exclude_patterns: æ’é™¤æ¨¡å¼
            include_patterns: åŒ…å«æ¨¡å¼
            progress_callback: è¿›åº¦å›è°ƒ

        Yields:
            ç´¢å¼•è¿›åº¦
        """
        async for progress in self.smart_index_directory(
            directory=directory,
            exclude_patterns=exclude_patterns,
            include_patterns=include_patterns,
            update_mode=IndexUpdateMode.SMART,
            progress_callback=progress_callback,
        ):
            yield progress

    async def index_files(
        self,
        files: List[Dict[str, str]],
        base_path: str = "",
        progress_callback: Optional[Callable[[IndexingProgress], None]] = None,
    ) -> AsyncGenerator[IndexingProgress, None]:
        """
        ç´¢å¼•æ–‡ä»¶åˆ—è¡¨

        Args:
            files: æ–‡ä»¶åˆ—è¡¨ [{"path": "...", "content": "..."}]
            base_path: åŸºç¡€è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒ

        Yields:
            ç´¢å¼•è¿›åº¦
        """
        await self.initialize()

        progress = IndexingProgress()
        progress.total_files = len(files)

        all_chunks: List[CodeChunk] = []

        for file_info in files:
            file_path = file_info.get("path", "")
            content = file_info.get("content", "")

            progress.current_file = file_path

            try:
                if not content.strip():
                    progress.processed_files += 1
                    progress.skipped_files += 1
                    continue

                # è®¡ç®—æ–‡ä»¶ hash
                file_hash = hashlib.md5(content.encode()).hexdigest()

                # é™åˆ¶æ–‡ä»¶å¤§å°
                if len(content) > 500000:
                    content = content[:500000]

                # åˆ†å—
                chunks = self.splitter.split_file(content, file_path)

                # ä¸ºæ¯ä¸ª chunk æ·»åŠ  file_hash
                for chunk in chunks:
                    chunk.metadata["file_hash"] = file_hash

                all_chunks.extend(chunks)

                progress.processed_files += 1
                progress.added_files += 1
                progress.total_chunks = len(all_chunks)

                if progress_callback:
                    progress_callback(progress)
                yield progress

            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                progress.errors.append(f"{file_path}: {str(e)}")
                progress.processed_files += 1

        # æ‰¹é‡åµŒå…¥å’Œç´¢å¼•
        if all_chunks:
            await self._index_chunks(all_chunks, progress, use_upsert=True)

        progress.indexed_chunks = len(all_chunks)
        yield progress

    async def _index_chunks(
        self,
        chunks: List[CodeChunk],
        progress: IndexingProgress,
        use_upsert: bool = False,
    ):
        """ç´¢å¼•ä»£ç å—"""
        if not chunks:
            return

        # å‡†å¤‡åµŒå…¥æ–‡æœ¬
        texts = [chunk.to_embedding_text() for chunk in chunks]

        logger.info(f"ğŸ”¢ ç”Ÿæˆ {len(texts)} ä¸ªä»£ç å—çš„åµŒå…¥å‘é‡...")

        # æ‰¹é‡åµŒå…¥
        embeddings = await self.embedding_service.embed_batch(texts, batch_size=50)

        # å‡†å¤‡å…ƒæ•°æ®
        ids = [chunk.id for chunk in chunks]
        documents = [chunk.content for chunk in chunks]
        metadatas = [chunk.to_dict() for chunk in chunks]

        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        logger.info(f"ğŸ’¾ æ·»åŠ  {len(chunks)} ä¸ªä»£ç å—åˆ°å‘é‡å­˜å‚¨...")

        if use_upsert:
            await self.vector_store.upsert_documents(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
            )
        else:
            await self.vector_store.add_documents(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
            )

        logger.info(f"âœ… ç´¢å¼• {len(chunks)} ä¸ªä»£ç å—æˆåŠŸ")

    def _collect_files(
        self,
        directory: str,
        exclude_patterns: List[str],
        include_patterns: Optional[List[str]],
    ) -> List[str]:
        """æ”¶é›†éœ€è¦ç´¢å¼•çš„æ–‡ä»¶"""
        import fnmatch

        files = []

        for root, dirs, filenames in os.walk(directory):
            # è¿‡æ»¤ç›®å½•
            dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

            for filename in filenames:
                # æ£€æŸ¥æ‰©å±•å
                ext = os.path.splitext(filename)[1].lower()
                if ext not in TEXT_EXTENSIONS:
                    continue

                # æ£€æŸ¥æ’é™¤æ–‡ä»¶
                if filename in EXCLUDE_FILES:
                    continue

                file_path = os.path.join(root, filename)
                relative_path = os.path.relpath(file_path, directory)

                # æ£€æŸ¥æ’é™¤æ¨¡å¼
                excluded = False
                for pattern in exclude_patterns:
                    if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(filename, pattern):
                        excluded = True
                        break

                if excluded:
                    continue

                # æ£€æŸ¥åŒ…å«æ¨¡å¼
                if include_patterns:
                    included = False
                    for pattern in include_patterns:
                        if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(filename, pattern):
                            included = True
                            break
                    if not included:
                        continue

                files.append(file_path)

        return files

    async def get_chunk_count(self) -> int:
        """è·å–å·²ç´¢å¼•çš„ä»£ç å—æ•°é‡"""
        await self.initialize()
        return await self.vector_store.get_count()

    async def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        await self.initialize()
        await self.vector_store.delete_collection()
        self._initialized = False

    async def delete_file(self, file_path: str) -> int:
        """
        åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„ç´¢å¼•

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            åˆ é™¤çš„ä»£ç å—æ•°é‡
        """
        await self.initialize()
        return await self.vector_store.delete_by_file_path(file_path)

    async def rebuild(self, directory: str, **kwargs) -> AsyncGenerator[IndexingProgress, None]:
        """
        å¼ºåˆ¶é‡å»ºç´¢å¼•

        Args:
            directory: ç›®å½•è·¯å¾„
            **kwargs: ä¼ é€’ç»™ smart_index_directory çš„å…¶ä»–å‚æ•°

        Yields:
            ç´¢å¼•è¿›åº¦
        """
        # å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        self._initialized = False
        await self.initialize(force_rebuild=True)

        async for progress in self.smart_index_directory(
            directory=directory,
            update_mode=IndexUpdateMode.FULL,
            **kwargs,
        ):
            yield progress


