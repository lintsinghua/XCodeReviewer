"""
ä»£ç æ£€ç´¢å™¨
æ”¯æŒè¯­ä¹‰æ£€ç´¢å’Œæ··åˆæ£€ç´¢
"""

import re
import asyncio
import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field

from .embeddings import EmbeddingService
from .indexer import VectorStore, ChromaVectorStore, InMemoryVectorStore
from .splitter import CodeChunk, ChunkType

logger = logging.getLogger(__name__)


@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    chunk_id: str
    content: str
    file_path: str
    language: str
    chunk_type: str
    line_start: int
    line_end: int
    score: float  # ç›¸ä¼¼åº¦åˆ†æ•° (0-1, è¶Šé«˜è¶Šç›¸ä¼¼)
    
    # å¯é€‰çš„å…ƒæ•°æ®
    name: Optional[str] = None
    parent_name: Optional[str] = None
    signature: Optional[str] = None
    security_indicators: List[str] = field(default_factory=list)
    
    # åŸå§‹å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "chunk_id": self.chunk_id,
            "content": self.content,
            "file_path": self.file_path,
            "language": self.language,
            "chunk_type": self.chunk_type,
            "line_start": self.line_start,
            "line_end": self.line_end,
            "score": self.score,
            "name": self.name,
            "parent_name": self.parent_name,
            "signature": self.signature,
            "security_indicators": self.security_indicators,
        }
    
    def to_context_string(self, include_metadata: bool = True) -> str:
        """è½¬æ¢ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ˆç”¨äº LLM è¾“å…¥ï¼‰"""
        parts = []
        
        if include_metadata:
            header = f"File: {self.file_path}"
            if self.line_start and self.line_end:
                header += f" (lines {self.line_start}-{self.line_end})"
            if self.name:
                header += f"\n{self.chunk_type.title()}: {self.name}"
            if self.parent_name:
                header += f" in {self.parent_name}"
            parts.append(header)
        
        parts.append(f"```{self.language}\n{self.content}\n```")
        
        return "\n".join(parts)


class CodeRetriever:
    """
    ä»£ç æ£€ç´¢å™¨
    æ”¯æŒè¯­ä¹‰æ£€ç´¢ã€å…³é”®å­—æ£€ç´¢å’Œæ··åˆæ£€ç´¢

    ğŸ”¥ è‡ªåŠ¨å…¼å®¹ä¸åŒç»´åº¦çš„å‘é‡ï¼š
    - æŸ¥è¯¢æ—¶è‡ªåŠ¨æ£€æµ‹ collection çš„ embedding é…ç½®
    - åŠ¨æ€åˆ›å»ºå¯¹åº”çš„ embedding æœåŠ¡
    """

    def __init__(
        self,
        collection_name: str,
        embedding_service: Optional[EmbeddingService] = None,
        vector_store: Optional[VectorStore] = None,
        persist_directory: Optional[str] = None,
        api_key: Optional[str] = None,  # ğŸ”¥ æ–°å¢ï¼šç”¨äºåŠ¨æ€åˆ›å»º embedding æœåŠ¡
    ):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨

        Args:
            collection_name: å‘é‡é›†åˆåç§°
            embedding_service: åµŒå…¥æœåŠ¡ï¼ˆå¯é€‰ï¼Œä¼šæ ¹æ® collection é…ç½®è‡ªåŠ¨åˆ›å»ºï¼‰
            vector_store: å‘é‡å­˜å‚¨
            persist_directory: æŒä¹…åŒ–ç›®å½•
            api_key: API Keyï¼ˆç”¨äºåŠ¨æ€åˆ›å»º embedding æœåŠ¡ï¼‰
        """
        self.collection_name = collection_name
        self._provided_embedding_service = embedding_service  # ç”¨æˆ·æä¾›çš„ embedding æœåŠ¡
        self.embedding_service = embedding_service  # å®é™…ä½¿ç”¨çš„ embedding æœåŠ¡
        self._api_key = api_key

        # åˆ›å»ºå‘é‡å­˜å‚¨
        if vector_store:
            self.vector_store = vector_store
        else:
            try:
                self.vector_store = ChromaVectorStore(
                    collection_name=collection_name,
                    persist_directory=persist_directory,
                )
            except ImportError:
                logger.warning("Chroma not available, using in-memory store")
                self.vector_store = InMemoryVectorStore(collection_name=collection_name)

        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶é€‚é… collection çš„ embedding é…ç½®"""
        if self._initialized:
            return

        await self.vector_store.initialize()

        # ğŸ”¥ è‡ªåŠ¨æ£€æµ‹ collection çš„ embedding é…ç½®
        if hasattr(self.vector_store, 'get_embedding_config'):
            stored_config = self.vector_store.get_embedding_config()
            stored_provider = stored_config.get("provider")
            stored_model = stored_config.get("model")
            stored_dimension = stored_config.get("dimension")
            stored_base_url = stored_config.get("base_url")

            # ğŸ”¥ å¦‚æœæ²¡æœ‰å­˜å‚¨çš„é…ç½®ï¼ˆæ—§çš„ collectionï¼‰ï¼Œå°è¯•é€šè¿‡ç»´åº¦æ¨æ–­
            if not stored_provider or not stored_model:
                inferred = await self._infer_embedding_config_from_dimension()
                if inferred:
                    stored_provider = inferred.get("provider")
                    stored_model = inferred.get("model")
                    stored_dimension = inferred.get("dimension")
                    logger.info(f"ğŸ“Š ä»å‘é‡ç»´åº¦æ¨æ–­ embedding é…ç½®: {stored_provider}/{stored_model}")

            if stored_provider and stored_model:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨ä¸åŒçš„ embedding æœåŠ¡
                current_provider = getattr(self.embedding_service, 'provider', None) if self.embedding_service else None
                current_model = getattr(self.embedding_service, 'model', None) if self.embedding_service else None

                if current_provider != stored_provider or current_model != stored_model:
                    logger.info(
                        f"ğŸ”„ Collection ä½¿ç”¨çš„ embedding é…ç½®ä¸å½“å‰ä¸åŒ: "
                        f"{stored_provider}/{stored_model} (ç»´åº¦: {stored_dimension}) vs "
                        f"{current_provider}/{current_model}"
                    )
                    logger.info(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ° collection çš„ embedding é…ç½®")

                    # åŠ¨æ€åˆ›å»ºå¯¹åº”çš„ embedding æœåŠ¡
                    api_key = self._api_key
                    if not api_key and self._provided_embedding_service:
                        api_key = getattr(self._provided_embedding_service, 'api_key', None)

                    self.embedding_service = EmbeddingService(
                        provider=stored_provider,
                        model=stored_model,
                        api_key=api_key,
                        base_url=stored_base_url,
                    )
                    logger.info(f"âœ… å·²åˆ‡æ¢åˆ°: {stored_provider}/{stored_model}")

        # å¦‚æœä»ç„¶æ²¡æœ‰ embedding æœåŠ¡ï¼Œåˆ›å»ºé»˜è®¤çš„
        if not self.embedding_service:
            self.embedding_service = EmbeddingService()

        self._initialized = True

    async def _infer_embedding_config_from_dimension(self) -> Optional[Dict[str, Any]]:
        """
        ğŸ”¥ ä»å‘é‡ç»´åº¦æ¨æ–­ embedding é…ç½®ï¼ˆç”¨äºå¤„ç†æ—§çš„ collectionï¼‰

        Returns:
            æ¨æ–­çš„ embedding é…ç½®ï¼Œå¦‚æœæ— æ³•æ¨æ–­åˆ™è¿”å› None
        """
        try:
            # è·å–ä¸€ä¸ªæ ·æœ¬å‘é‡æ¥æ£€æŸ¥ç»´åº¦
            if hasattr(self.vector_store, '_collection') and self.vector_store._collection:
                count = await self.vector_store.get_count()
                if count > 0:
                    sample = await asyncio.to_thread(
                        self.vector_store._collection.peek,
                        limit=1
                    )
                    embeddings = sample.get("embeddings")
                    if embeddings is not None and len(embeddings) > 0:
                        dim = len(embeddings[0])

                        # ğŸ”¥ æ ¹æ®ç»´åº¦æ¨æ–­æ¨¡å‹ï¼ˆä¼˜å…ˆé€‰æ‹©å¸¸ç”¨æ¨¡å‹ï¼‰
                        dimension_mapping = {
                            # OpenAI ç³»åˆ—
                            1536: {"provider": "openai", "model": "text-embedding-3-small", "dimension": 1536},
                            3072: {"provider": "openai", "model": "text-embedding-3-large", "dimension": 3072},

                            # HuggingFace ç³»åˆ—
                            1024: {"provider": "huggingface", "model": "BAAI/bge-m3", "dimension": 1024},
                            384: {"provider": "huggingface", "model": "sentence-transformers/all-MiniLM-L6-v2", "dimension": 384},

                            # Ollama ç³»åˆ—
                            768: {"provider": "ollama", "model": "nomic-embed-text", "dimension": 768},

                            # Jina ç³»åˆ—
                            512: {"provider": "jina", "model": "jina-embeddings-v2-small-en", "dimension": 512},

                            # Cohere ç³»åˆ—
                            # 1024 å·²è¢« HuggingFace å ç”¨ï¼ŒCohere ç»´åº¦ç›¸åŒæ—¶ä¼šé»˜è®¤ä½¿ç”¨ HuggingFace
                        }

                        inferred = dimension_mapping.get(dim)
                        if inferred:
                            logger.info(f"ğŸ“Š æ£€æµ‹åˆ°å‘é‡ç»´åº¦ {dim}ï¼Œæ¨æ–­ä¸º: {inferred['provider']}/{inferred['model']}")
                        return inferred
        except Exception as e:
            logger.warning(f"æ— æ³•æ¨æ–­ embedding é…ç½®: {e}")

        return None

    def get_collection_embedding_config(self) -> Dict[str, Any]:
        """
        è·å– collection å­˜å‚¨çš„ embedding é…ç½®

        Returns:
            åŒ…å« provider, model, dimension, base_url çš„å­—å…¸
        """
        if hasattr(self.vector_store, 'get_embedding_config'):
            return self.vector_store.get_embedding_config()
        return {}
    
    async def retrieve(
        self,
        query: str,
        top_k: int = 10,
        filter_file_path: Optional[str] = None,
        filter_language: Optional[str] = None,
        filter_chunk_type: Optional[str] = None,
        min_score: float = 0.0,
    ) -> List[RetrievalResult]:
        """
        è¯­ä¹‰æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            filter_file_path: æ–‡ä»¶è·¯å¾„è¿‡æ»¤
            filter_language: è¯­è¨€è¿‡æ»¤
            filter_chunk_type: å—ç±»å‹è¿‡æ»¤
            min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        await self.initialize()
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self.embedding_service.embed(query)
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where = {}
        if filter_file_path:
            where["file_path"] = filter_file_path
        if filter_language:
            where["language"] = filter_language
        if filter_chunk_type:
            where["chunk_type"] = filter_chunk_type
        
        # æŸ¥è¯¢å‘é‡å­˜å‚¨
        raw_results = await self.vector_store.query(
            query_embedding=query_embedding,
            n_results=top_k * 2,  # å¤šæŸ¥ä¸€äº›ï¼Œåé¢è¿‡æ»¤
            where=where if where else None,
        )
        
        # è½¬æ¢ç»“æœ
        results = []
        for i, (id_, doc, meta, dist) in enumerate(zip(
            raw_results["ids"],
            raw_results["documents"],
            raw_results["metadatas"],
            raw_results["distances"],
        )):
            # å°†è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (ä½™å¼¦è·ç¦»)
            score = 1 - dist
            
            if score < min_score:
                continue
            
            # è§£æå®‰å…¨æŒ‡æ ‡ï¼ˆå¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
            security_indicators = meta.get("security_indicators", [])
            if isinstance(security_indicators, str):
                try:
                    import json
                    security_indicators = json.loads(security_indicators)
                except:
                    security_indicators = []
            
            result = RetrievalResult(
                chunk_id=id_,
                content=doc,
                file_path=meta.get("file_path", ""),
                language=meta.get("language", "text"),
                chunk_type=meta.get("chunk_type", "unknown"),
                line_start=meta.get("line_start", 0),
                line_end=meta.get("line_end", 0),
                score=score,
                name=meta.get("name"),
                parent_name=meta.get("parent_name"),
                signature=meta.get("signature"),
                security_indicators=security_indicators,
                metadata=meta,
            )
            results.append(result)
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶æˆªå–
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:top_k]
    
    async def retrieve_by_file(
        self,
        file_path: str,
        top_k: int = 50,
    ) -> List[RetrievalResult]:
        """
        æŒ‰æ–‡ä»¶è·¯å¾„æ£€ç´¢
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            top_k: è¿”å›æ•°é‡
            
        Returns:
            è¯¥æ–‡ä»¶çš„æ‰€æœ‰ä»£ç å—
        """
        await self.initialize()
        
        # ä½¿ç”¨ä¸€ä¸ªé€šç”¨æŸ¥è¯¢
        query_embedding = await self.embedding_service.embed(f"code in {file_path}")
        
        raw_results = await self.vector_store.query(
            query_embedding=query_embedding,
            n_results=top_k,
            where={"file_path": file_path},
        )
        
        results = []
        for id_, doc, meta, dist in zip(
            raw_results["ids"],
            raw_results["documents"],
            raw_results["metadatas"],
            raw_results["distances"],
        ):
            result = RetrievalResult(
                chunk_id=id_,
                content=doc,
                file_path=meta.get("file_path", ""),
                language=meta.get("language", "text"),
                chunk_type=meta.get("chunk_type", "unknown"),
                line_start=meta.get("line_start", 0),
                line_end=meta.get("line_end", 0),
                score=1 - dist,
                name=meta.get("name"),
                parent_name=meta.get("parent_name"),
                metadata=meta,
            )
            results.append(result)
        
        # æŒ‰è¡Œå·æ’åº
        results.sort(key=lambda x: x.line_start)
        return results
    
    async def retrieve_security_related(
        self,
        vulnerability_type: Optional[str] = None,
        top_k: int = 20,
    ) -> List[RetrievalResult]:
        """
        æ£€ç´¢ä¸å®‰å…¨ç›¸å…³çš„ä»£ç 
        
        Args:
            vulnerability_type: æ¼æ´ç±»å‹ï¼ˆå¦‚ sql_injection, xss ç­‰ï¼‰
            top_k: è¿”å›æ•°é‡
            
        Returns:
            å®‰å…¨ç›¸å…³çš„ä»£ç å—
        """
        # æ ¹æ®æ¼æ´ç±»å‹æ„å»ºæŸ¥è¯¢
        security_queries = {
            "sql_injection": "SQL query execute database user input",
            "xss": "HTML render user input innerHTML template",
            "command_injection": "system exec command shell subprocess",
            "path_traversal": "file path read open user input",
            "ssrf": "HTTP request URL user input fetch",
            "deserialization": "deserialize pickle yaml load object",
            "auth_bypass": "authentication login password token session",
            "hardcoded_secret": "password secret key token credential",
        }
        
        if vulnerability_type and vulnerability_type in security_queries:
            query = security_queries[vulnerability_type]
        else:
            query = "security vulnerability dangerous function user input"
        
        return await self.retrieve(query, top_k=top_k)
    
    async def retrieve_function_context(
        self,
        function_name: str,
        file_path: Optional[str] = None,
        include_callers: bool = True,
        include_callees: bool = True,
        top_k: int = 10,
    ) -> Dict[str, List[RetrievalResult]]:
        """
        æ£€ç´¢å‡½æ•°ä¸Šä¸‹æ–‡
        
        Args:
            function_name: å‡½æ•°å
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            include_callers: æ˜¯å¦åŒ…å«è°ƒç”¨è€…
            include_callees: æ˜¯å¦åŒ…å«è¢«è°ƒç”¨è€…
            top_k: æ¯ç±»è¿”å›æ•°é‡
            
        Returns:
            åŒ…å«å‡½æ•°å®šä¹‰ã€è°ƒç”¨è€…ã€è¢«è°ƒç”¨è€…çš„å­—å…¸
        """
        context = {
            "definition": [],
            "callers": [],
            "callees": [],
        }
        
        # æŸ¥æ‰¾å‡½æ•°å®šä¹‰
        definition_query = f"function definition {function_name}"
        definitions = await self.retrieve(
            definition_query,
            top_k=5,
            filter_file_path=file_path,
        )
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„å®šä¹‰
        for result in definitions:
            if result.name == function_name or function_name in (result.content or ""):
                context["definition"].append(result)
        
        if include_callers:
            # æŸ¥æ‰¾è°ƒç”¨æ­¤å‡½æ•°çš„ä»£ç 
            caller_query = f"calls {function_name} invoke {function_name}"
            callers = await self.retrieve(caller_query, top_k=top_k)
            
            for result in callers:
                # æ£€æŸ¥æ˜¯å¦çœŸçš„è°ƒç”¨äº†è¿™ä¸ªå‡½æ•°
                if re.search(rf'\b{re.escape(function_name)}\s*\(', result.content):
                    if result not in context["definition"]:
                        context["callers"].append(result)
        
        if include_callees and context["definition"]:
            # ä»å‡½æ•°å®šä¹‰ä¸­æå–è°ƒç”¨çš„å…¶ä»–å‡½æ•°
            for definition in context["definition"]:
                calls = re.findall(r'\b(\w+)\s*\(', definition.content)
                unique_calls = list(set(calls))[:5]  # é™åˆ¶æ•°é‡
                
                for call in unique_calls:
                    if call == function_name:
                        continue
                    callees = await self.retrieve(
                        f"function {call} definition",
                        top_k=2,
                    )
                    context["callees"].extend(callees)
        
        return context
    
    async def retrieve_similar_code(
        self,
        code_snippet: str,
        top_k: int = 5,
        exclude_file: Optional[str] = None,
    ) -> List[RetrievalResult]:
        """
        æ£€ç´¢ç›¸ä¼¼çš„ä»£ç 
        
        Args:
            code_snippet: ä»£ç ç‰‡æ®µ
            top_k: è¿”å›æ•°é‡
            exclude_file: æ’é™¤çš„æ–‡ä»¶
            
        Returns:
            ç›¸ä¼¼ä»£ç åˆ—è¡¨
        """
        results = await self.retrieve(
            f"similar code: {code_snippet}",
            top_k=top_k * 2,
        )
        
        if exclude_file:
            results = [r for r in results if r.file_path != exclude_file]
        
        return results[:top_k]
    
    async def hybrid_retrieve(
        self,
        query: str,
        keywords: Optional[List[str]] = None,
        top_k: int = 10,
        semantic_weight: float = 0.7,
    ) -> List[RetrievalResult]:
        """
        æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰ + å…³é”®å­—ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            keywords: é¢å¤–çš„å…³é”®å­—
            top_k: è¿”å›æ•°é‡
            semantic_weight: è¯­ä¹‰æ£€ç´¢æƒé‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # è¯­ä¹‰æ£€ç´¢
        semantic_results = await self.retrieve(query, top_k=top_k * 2)
        
        # å¦‚æœæœ‰å…³é”®å­—ï¼Œè¿›è¡Œå…³é”®å­—è¿‡æ»¤/å¢å¼º
        if keywords:
            keyword_pattern = '|'.join(re.escape(kw) for kw in keywords)
            
            enhanced_results = []
            for result in semantic_results:
                # è®¡ç®—å…³é”®å­—åŒ¹é…åº¦
                matches = len(re.findall(keyword_pattern, result.content, re.IGNORECASE))
                keyword_score = min(1.0, matches / len(keywords))
                
                # æ··åˆåˆ†æ•°
                hybrid_score = (
                    semantic_weight * result.score +
                    (1 - semantic_weight) * keyword_score
                )
                
                result.score = hybrid_score
                enhanced_results.append(result)
            
            enhanced_results.sort(key=lambda x: x.score, reverse=True)
            return enhanced_results[:top_k]
        
        return semantic_results[:top_k]
    
    def format_results_for_llm(
        self,
        results: List[RetrievalResult],
        max_tokens: int = 4000,
        include_metadata: bool = True,
    ) -> str:
        """
        å°†æ£€ç´¢ç»“æœæ ¼å¼åŒ–ä¸º LLM è¾“å…¥
        
        Args:
            results: æ£€ç´¢ç»“æœ
            max_tokens: æœ€å¤§ Token æ•°
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        if not results:
            return "No relevant code found."
        
        parts = []
        total_tokens = 0
        
        for i, result in enumerate(results):
            context = result.to_context_string(include_metadata=include_metadata)
            estimated_tokens = len(context) // 4
            
            if total_tokens + estimated_tokens > max_tokens:
                break
            
            parts.append(f"### Code Block {i + 1} (Score: {result.score:.2f})\n{context}")
            total_tokens += estimated_tokens
        
        return "\n\n".join(parts)

