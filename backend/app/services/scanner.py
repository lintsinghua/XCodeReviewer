"""
ä»“åº“æ‰«ææœåŠ¡ - æ”¯æŒGitHubå’ŒGitLabä»“åº“æ‰«æ
"""

import asyncio
import httpx
from typing import List, Dict, Any, Optional
from datetime import datetime
from urllib.parse import urlparse, quote
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.audit import AuditTask, AuditIssue
from app.models.project import Project
from app.services.llm.service import LLMService
from app.core.config import settings


# æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶æ‰©å±•å
TEXT_EXTENSIONS = [
    ".js", ".ts", ".tsx", ".jsx", ".py", ".java", ".go", ".rs", 
    ".cpp", ".c", ".h", ".cc", ".hh", ".cs", ".php", ".rb", 
    ".kt", ".swift", ".sql", ".sh", ".json", ".yml", ".yaml"
]

# æ’é™¤çš„ç›®å½•å’Œæ–‡ä»¶æ¨¡å¼
EXCLUDE_PATTERNS = [
    "node_modules/", "vendor/", "dist/", "build/", ".git/",
    "__pycache__/", ".pytest_cache/", "coverage/", ".nyc_output/",
    ".vscode/", ".idea/", ".vs/", "target/", "out/",
    "__MACOSX/", ".DS_Store", "package-lock.json", "yarn.lock",
    "pnpm-lock.yaml", ".min.js", ".min.css", ".map"
]


def is_text_file(path: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬æ–‡ä»¶"""
    return any(path.lower().endswith(ext) for ext in TEXT_EXTENSIONS)


def should_exclude(path: str, exclude_patterns: List[str] = None) -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤è¯¥æ–‡ä»¶"""
    all_patterns = EXCLUDE_PATTERNS + (exclude_patterns or [])
    return any(pattern in path for pattern in all_patterns)


def get_language_from_path(path: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„è·å–è¯­è¨€ç±»å‹"""
    ext = path.split('.')[-1].lower() if '.' in path else ''
    language_map = {
        'js': 'javascript', 'jsx': 'javascript',
        'ts': 'typescript', 'tsx': 'typescript',
        'py': 'python', 'java': 'java', 'go': 'go',
        'rs': 'rust', 'cpp': 'cpp', 'c': 'cpp',
        'cc': 'cpp', 'h': 'cpp', 'hh': 'cpp',
        'cs': 'csharp', 'php': 'php', 'rb': 'ruby',
        'kt': 'kotlin', 'swift': 'swift'
    }
    return language_map.get(ext, 'text')


class TaskControlManager:
    """ä»»åŠ¡æ§åˆ¶ç®¡ç†å™¨ - ç”¨äºå–æ¶ˆè¿è¡Œä¸­çš„ä»»åŠ¡"""
    
    def __init__(self):
        self._cancelled_tasks: set = set()
    
    def cancel_task(self, task_id: str):
        """å–æ¶ˆä»»åŠ¡"""
        self._cancelled_tasks.add(task_id)
        print(f"ğŸ›‘ ä»»åŠ¡ {task_id} å·²æ ‡è®°ä¸ºå–æ¶ˆ")
    
    def is_cancelled(self, task_id: str) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ"""
        return task_id in self._cancelled_tasks
    
    def cleanup_task(self, task_id: str):
        """æ¸…ç†å·²å®Œæˆä»»åŠ¡çš„æ§åˆ¶çŠ¶æ€"""
        self._cancelled_tasks.discard(task_id)


# å…¨å±€ä»»åŠ¡æ§åˆ¶å™¨
task_control = TaskControlManager()


async def github_api(url: str, token: str = None) -> Any:
    """è°ƒç”¨GitHub API"""
    headers = {"Accept": "application/vnd.github+json"}
    t = token or settings.GITHUB_TOKEN
    if t:
        headers["Authorization"] = f"Bearer {t}"
    
    async with httpx.AsyncClient(timeout=30) as client:
        response = await client.get(url, headers=headers)
        if response.status_code == 403:
            raise Exception("GitHub API 403ï¼šè¯·é…ç½® GITHUB_TOKEN æˆ–ç¡®è®¤ä»“åº“æƒé™/é¢‘ç‡é™åˆ¶")
        if response.status_code != 200:
            raise Exception(f"GitHub API {response.status_code}: {url}")
        return response.json()


async def gitlab_api(url: str, token: str = None) -> Any:
    """è°ƒç”¨GitLab API"""
    headers = {"Content-Type": "application/json"}
    t = token or settings.GITLAB_TOKEN
    if t:
        headers["PRIVATE-TOKEN"] = t
    
    async with httpx.AsyncClient(timeout=30) as client:
        response = await client.get(url, headers=headers)
        if response.status_code == 401:
            raise Exception("GitLab API 401ï¼šè¯·é…ç½® GITLAB_TOKEN æˆ–ç¡®è®¤ä»“åº“æƒé™")
        if response.status_code == 403:
            raise Exception("GitLab API 403ï¼šè¯·ç¡®è®¤ä»“åº“æƒé™/é¢‘ç‡é™åˆ¶")
        if response.status_code != 200:
            raise Exception(f"GitLab API {response.status_code}: {url}")
        return response.json()


async def fetch_file_content(url: str, headers: Dict[str, str] = None) -> Optional[str]:
    """è·å–æ–‡ä»¶å†…å®¹"""
    async with httpx.AsyncClient(timeout=30) as client:
        try:
            response = await client.get(url, headers=headers or {})
            if response.status_code == 200:
                return response.text
        except Exception as e:
            print(f"è·å–æ–‡ä»¶å†…å®¹å¤±è´¥: {url}, é”™è¯¯: {e}")
    return None


async def get_github_files(repo_url: str, branch: str, token: str = None) -> List[Dict[str, str]]:
    """è·å–GitHubä»“åº“æ–‡ä»¶åˆ—è¡¨"""
    # è§£æä»“åº“URL
    match = repo_url.rstrip('/').rstrip('.git')
    if 'github.com/' in match:
        parts = match.split('github.com/')[-1].split('/')
        if len(parts) >= 2:
            owner, repo = parts[0], parts[1]
        else:
            raise Exception("GitHub ä»“åº“ URL æ ¼å¼é”™è¯¯")
    else:
        raise Exception("GitHub ä»“åº“ URL æ ¼å¼é”™è¯¯")
    
    # è·å–ä»“åº“æ–‡ä»¶æ ‘
    tree_url = f"https://api.github.com/repos/{owner}/{repo}/git/trees/{quote(branch)}?recursive=1"
    tree_data = await github_api(tree_url, token)
    
    files = []
    for item in tree_data.get("tree", []):
        if item.get("type") == "blob" and is_text_file(item["path"]) and not should_exclude(item["path"]):
            size = item.get("size", 0)
            if size <= settings.MAX_FILE_SIZE_BYTES:
                files.append({
                    "path": item["path"],
                    "url": f"https://raw.githubusercontent.com/{owner}/{repo}/{quote(branch)}/{item['path']}"
                })
    
    return files


async def get_gitlab_files(repo_url: str, branch: str, token: str = None) -> List[Dict[str, str]]:
    """è·å–GitLabä»“åº“æ–‡ä»¶åˆ—è¡¨"""
    parsed = urlparse(repo_url)
    base = f"{parsed.scheme}://{parsed.netloc}"
    
    # ä»URLä¸­æå–tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    extracted_token = token
    if parsed.username:
        if parsed.username == 'oauth2' and parsed.password:
            extracted_token = parsed.password
        elif parsed.username and not parsed.password:
            extracted_token = parsed.username
    
    # è§£æé¡¹ç›®è·¯å¾„
    path = parsed.path.strip('/').rstrip('.git')
    if not path:
        raise Exception("GitLab ä»“åº“ URL æ ¼å¼é”™è¯¯")
    
    project_path = quote(path, safe='')
    
    # è·å–ä»“åº“æ–‡ä»¶æ ‘
    tree_url = f"{base}/api/v4/projects/{project_path}/repository/tree?ref={quote(branch)}&recursive=true&per_page=100"
    tree_data = await gitlab_api(tree_url, extracted_token)
    
    files = []
    for item in tree_data:
        if item.get("type") == "blob" and is_text_file(item["path"]) and not should_exclude(item["path"]):
            files.append({
                "path": item["path"],
                "url": f"{base}/api/v4/projects/{project_path}/repository/files/{quote(item['path'], safe='')}/raw?ref={quote(branch)}",
                "token": extracted_token
            })
    
    return files


async def scan_repo_task(task_id: str, db_session_factory, user_config: dict = None):
    """
    åå°ä»“åº“æ‰«æä»»åŠ¡
    
    Args:
        task_id: ä»»åŠ¡ID
        db_session_factory: æ•°æ®åº“ä¼šè¯å·¥å‚
        user_config: ç”¨æˆ·é…ç½®å­—å…¸ï¼ˆåŒ…å«llmConfigå’ŒotherConfigï¼‰
    """
    async with db_session_factory() as db:
        task = await db.get(AuditTask, task_id)
        if not task:
            return

        try:
            # 1. æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            task.status = "running"
            task.started_at = datetime.utcnow()
            await db.commit()
            
            # åˆ›å»ºä½¿ç”¨ç”¨æˆ·é…ç½®çš„LLMæœåŠ¡å®ä¾‹
            llm_service = LLMService(user_config=user_config or {})

            # 2. è·å–é¡¹ç›®ä¿¡æ¯
            project = await db.get(Project, task.project_id)
            if not project or not project.repository_url:
                raise Exception("ä»“åº“åœ°å€ä¸å­˜åœ¨")

            repo_url = project.repository_url
            branch = task.branch_name or project.default_branch or "main"
            repo_type = project.repository_type or "other"

            print(f"ğŸš€ å¼€å§‹æ‰«æä»“åº“: {repo_url}, åˆ†æ”¯: {branch}, ç±»å‹: {repo_type}")

            # 3. è·å–æ–‡ä»¶åˆ—è¡¨
            # ä»ç”¨æˆ·é…ç½®ä¸­è¯»å– GitHub/GitLab Tokenï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œç„¶åä½¿ç”¨ç³»ç»Ÿé…ç½®ï¼‰
            user_other_config = (user_config or {}).get('otherConfig', {})
            github_token = user_other_config.get('githubToken') or settings.GITHUB_TOKEN
            gitlab_token = user_other_config.get('gitlabToken') or settings.GITLAB_TOKEN
            
            files: List[Dict[str, str]] = []
            extracted_gitlab_token = None
            
            if repo_type == "github":
                files = await get_github_files(repo_url, branch, github_token)
            elif repo_type == "gitlab":
                files = await get_gitlab_files(repo_url, branch, gitlab_token)
                # GitLabæ–‡ä»¶å¯èƒ½å¸¦æœ‰token
                if files and 'token' in files[0]:
                    extracted_gitlab_token = files[0].get('token')
            else:
                raise Exception("ä¸æ”¯æŒçš„ä»“åº“ç±»å‹ï¼Œä»…æ”¯æŒ GitHub å’Œ GitLab ä»“åº“")

            # é™åˆ¶æ–‡ä»¶æ•°é‡
            files = files[:settings.MAX_ANALYZE_FILES]
            
            task.total_files = len(files)
            await db.commit()

            print(f"ğŸ“Š è·å–åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åˆ†æ")

            # 4. åˆ†ææ–‡ä»¶
            total_issues = 0
            total_lines = 0
            quality_scores = []
            scanned_files = 0
            failed_files = 0
            consecutive_failures = 0
            MAX_CONSECUTIVE_FAILURES = 5

            for file_info in files:
                # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
                if task_control.is_cancelled(task_id):
                    print(f"ğŸ›‘ ä»»åŠ¡ {task_id} å·²è¢«ç”¨æˆ·å–æ¶ˆ")
                    task.status = "cancelled"
                    task.completed_at = datetime.utcnow()
                    await db.commit()
                    task_control.cleanup_task(task_id)
                    return

                # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:
                    print(f"âŒ ä»»åŠ¡ {task_id}: è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œåœæ­¢åˆ†æ")
                    raise Exception(f"è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œå¯èƒ½æ˜¯ LLM API æœåŠ¡å¼‚å¸¸")

                try:
                    # è·å–æ–‡ä»¶å†…å®¹
                    headers = {}
                    # ä½¿ç”¨æå–çš„ GitLab token æˆ–ç”¨æˆ·é…ç½®çš„ token
                    token_to_use = extracted_gitlab_token or gitlab_token
                    if token_to_use:
                        headers["PRIVATE-TOKEN"] = token_to_use
                    
                    content = await fetch_file_content(file_info["url"], headers)
                    if not content:
                        continue
                    
                    if len(content) > settings.MAX_FILE_SIZE_BYTES:
                        continue
                    
                    total_lines += content.count('\n') + 1
                    language = get_language_from_path(file_info["path"])
                    
                    # LLMåˆ†æ
                    analysis = await llm_service.analyze_code(content, language)
                    
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦å–æ¶ˆï¼ˆLLMåˆ†æåï¼‰
                    if task_control.is_cancelled(task_id):
                        print(f"ğŸ›‘ ä»»åŠ¡ {task_id} åœ¨LLMåˆ†æåè¢«å–æ¶ˆ")
                        task.status = "cancelled"
                        task.completed_at = datetime.utcnow()
                        await db.commit()
                        task_control.cleanup_task(task_id)
                        return
                    
                    # ä¿å­˜é—®é¢˜
                    issues = analysis.get("issues", [])
                    for issue in issues:
                        audit_issue = AuditIssue(
                            task_id=task.id,
                            file_path=file_info["path"],
                            line_number=issue.get("line", 1),
                            column_number=issue.get("column"),
                            issue_type=issue.get("type", "maintainability"),
                            severity=issue.get("severity", "low"),
                            title=issue.get("title", "Issue"),
                            message=issue.get("description") or issue.get("title", "Issue"),
                            suggestion=issue.get("suggestion"),
                            code_snippet=issue.get("code_snippet"),
                            ai_explanation=issue.get("ai_explanation"),
                            status="open"
                        )
                        db.add(audit_issue)
                        total_issues += 1
                    
                    if "quality_score" in analysis:
                        quality_scores.append(analysis["quality_score"])
                    
                    consecutive_failures = 0  # æˆåŠŸåé‡ç½®
                    scanned_files += 1
                    
                    # æ›´æ–°è¿›åº¦
                    task.scanned_files = scanned_files
                    task.total_lines = total_lines
                    task.issues_count = total_issues
                    await db.commit()
                    
                    print(f"ğŸ“ˆ ä»»åŠ¡ {task_id}: è¿›åº¦ {scanned_files}/{len(files)} ({int(scanned_files/len(files)*100)}%)")
                    
                    # è¯·æ±‚é—´éš”
                    await asyncio.sleep(settings.LLM_GAP_MS / 1000)
                    
                except Exception as file_error:
                    failed_files += 1
                    consecutive_failures += 1
                    print(f"âŒ åˆ†ææ–‡ä»¶å¤±è´¥ ({file_info['path']}): {file_error}")
                    await asyncio.sleep(settings.LLM_GAP_MS / 1000)

            # 5. å®Œæˆä»»åŠ¡
            avg_quality_score = sum(quality_scores) / len(quality_scores) if quality_scores else 100.0
            
            task.status = "completed"
            task.completed_at = datetime.utcnow()
            task.scanned_files = scanned_files
            task.total_lines = total_lines
            task.issues_count = total_issues
            task.quality_score = avg_quality_score
            await db.commit()
            
            print(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ: æ‰«æ {scanned_files} ä¸ªæ–‡ä»¶, å‘ç° {total_issues} ä¸ªé—®é¢˜, è´¨é‡åˆ† {avg_quality_score:.1f}")
            task_control.cleanup_task(task_id)

        except Exception as e:
            print(f"âŒ æ‰«æå¤±è´¥: {e}")
            task.status = "failed"
            task.completed_at = datetime.utcnow()
            await db.commit()
            task_control.cleanup_task(task_id)
