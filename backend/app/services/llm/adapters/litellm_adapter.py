"""
LiteLLM ç»Ÿä¸€é€‚é…å™¨
æ”¯æŒé€šè¿‡ LiteLLM è°ƒç”¨å¤šä¸ª LLM æä¾›å•†ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ OpenAI å…¼å®¹æ ¼å¼
"""

from typing import Dict, Any, Optional
from ..base_adapter import BaseLLMAdapter
from ..types import (
    LLMConfig,
    LLMRequest,
    LLMResponse,
    LLMUsage,
    LLMProvider,
    LLMError,
    DEFAULT_BASE_URLS,
)


class LiteLLMAdapter(BaseLLMAdapter):
    """
    LiteLLM ç»Ÿä¸€é€‚é…å™¨
    
    æ”¯æŒçš„æä¾›å•†:
    - OpenAI (openai/gpt-4o-mini)
    - Claude (anthropic/claude-3-5-sonnet-20241022)
    - Gemini (gemini/gemini-1.5-flash)
    - DeepSeek (deepseek/deepseek-chat)
    - Qwen (qwen/qwen-turbo) - é€šè¿‡ OpenAI å…¼å®¹æ¨¡å¼
    - Zhipu (zhipu/glm-4-flash) - é€šè¿‡ OpenAI å…¼å®¹æ¨¡å¼
    - Moonshot (moonshot/moonshot-v1-8k) - é€šè¿‡ OpenAI å…¼å®¹æ¨¡å¼
    - Ollama (ollama/llama3)
    """

    # LiteLLM æ¨¡å‹å‰ç¼€æ˜ å°„
    PROVIDER_PREFIX_MAP = {
        LLMProvider.OPENAI: "openai",
        LLMProvider.CLAUDE: "anthropic",
        LLMProvider.GEMINI: "gemini",
        LLMProvider.DEEPSEEK: "deepseek",
        LLMProvider.QWEN: "openai",  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        LLMProvider.ZHIPU: "openai",  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        LLMProvider.MOONSHOT: "openai",  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        LLMProvider.OLLAMA: "ollama",
    }

    # éœ€è¦è‡ªå®šä¹‰ base_url çš„æä¾›å•†
    CUSTOM_BASE_URL_PROVIDERS = {
        LLMProvider.QWEN,
        LLMProvider.ZHIPU,
        LLMProvider.MOONSHOT,
        LLMProvider.DEEPSEEK,
    }

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self._litellm_model = self._get_litellm_model()
        self._api_base = self._get_api_base()

    def _get_litellm_model(self) -> str:
        """è·å– LiteLLM æ ¼å¼çš„æ¨¡å‹åç§°"""
        provider = self.config.provider
        model = self.config.model

        # å¯¹äºä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼çš„æä¾›å•†ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹å
        if provider in self.CUSTOM_BASE_URL_PROVIDERS:
            return model

        # å¯¹äºåŸç”Ÿæ”¯æŒçš„æä¾›å•†ï¼Œæ·»åŠ å‰ç¼€
        prefix = self.PROVIDER_PREFIX_MAP.get(provider, "openai")
        
        # æ£€æŸ¥æ¨¡å‹åæ˜¯å¦å·²ç»åŒ…å«å‰ç¼€
        if "/" in model:
            return model
        
        return f"{prefix}/{model}"

    def _get_api_base(self) -> Optional[str]:
        """è·å– API åŸºç¡€ URL"""
        # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„ base_url
        if self.config.base_url:
            return self.config.base_url

        # å¯¹äºéœ€è¦è‡ªå®šä¹‰ base_url çš„æä¾›å•†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if self.config.provider in self.CUSTOM_BASE_URL_PROVIDERS:
            return DEFAULT_BASE_URLS.get(self.config.provider)

        # Ollama ä½¿ç”¨æœ¬åœ°åœ°å€
        if self.config.provider == LLMProvider.OLLAMA:
            return DEFAULT_BASE_URLS.get(LLMProvider.OLLAMA, "http://localhost:11434")

        return None

    async def complete(self, request: LLMRequest) -> LLMResponse:
        """ä½¿ç”¨ LiteLLM å‘é€è¯·æ±‚"""
        try:
            await self.validate_config()
            return await self.retry(lambda: self._send_request(request))
        except Exception as error:
            self.handle_error(error, f"LiteLLM ({self.config.provider.value}) APIè°ƒç”¨å¤±è´¥")

    async def _send_request(self, request: LLMRequest) -> LLMResponse:
        """å‘é€è¯·æ±‚åˆ° LiteLLM"""
        import litellm
        
        # ç¦ç”¨ LiteLLM çš„ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡éƒ½å®é™…è°ƒç”¨ API
        litellm.cache = None
        
        # ç¦ç”¨ LiteLLM è‡ªåŠ¨æ·»åŠ çš„ reasoning_effort å‚æ•°
        # è¿™å¯ä»¥é˜²æ­¢æ¨¡å‹åç§°è¢«é”™è¯¯è§£æä¸º effort å‚æ•°
        litellm.drop_params = True
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]

        # æ„å»ºè¯·æ±‚å‚æ•°
        kwargs: Dict[str, Any] = {
            "model": self._litellm_model,
            "messages": messages,
            "temperature": request.temperature if request.temperature is not None else self.config.temperature,
            "max_tokens": request.max_tokens if request.max_tokens is not None else self.config.max_tokens,
            "top_p": request.top_p if request.top_p is not None else self.config.top_p,
        }

        # è®¾ç½® API Key
        if self.config.api_key and self.config.api_key != "ollama":
            kwargs["api_key"] = self.config.api_key

        # è®¾ç½® API Base URL
        if self._api_base:
            kwargs["api_base"] = self._api_base
            print(f"ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰ API Base: {self._api_base}")

        # è®¾ç½®è¶…æ—¶
        kwargs["timeout"] = self.config.timeout

        # å¯¹äº OpenAI æä¾›å•†ï¼Œæ·»åŠ é¢å¤–å‚æ•°
        if self.config.provider == LLMProvider.OPENAI:
            kwargs["frequency_penalty"] = self.config.frequency_penalty
            kwargs["presence_penalty"] = self.config.presence_penalty

        try:
            # è°ƒç”¨ LiteLLM
            response = await litellm.acompletion(**kwargs)
        except litellm.exceptions.AuthenticationError as e:
            raise LLMError(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ: {str(e)}", self.config.provider, 401)
        except litellm.exceptions.RateLimitError as e:
            raise LLMError(f"API è°ƒç”¨é¢‘ç‡è¶…é™: {str(e)}", self.config.provider, 429)
        except litellm.exceptions.APIConnectionError as e:
            raise LLMError(f"æ— æ³•è¿æ¥åˆ° API æœåŠ¡: {str(e)}", self.config.provider)
        except litellm.exceptions.APIError as e:
            raise LLMError(f"API é”™è¯¯: {str(e)}", self.config.provider, getattr(e, 'status_code', None))
        except Exception as e:
            # æ•è·å…¶ä»–å¼‚å¸¸å¹¶é‡æ–°æŠ›å‡º
            error_msg = str(e)
            if "invalid_api_key" in error_msg.lower() or "incorrect api key" in error_msg.lower():
                raise LLMError(f"API Key æ— æ•ˆ: {error_msg}", self.config.provider, 401)
            elif "authentication" in error_msg.lower():
                raise LLMError(f"è®¤è¯å¤±è´¥: {error_msg}", self.config.provider, 401)
            raise

        # è§£æå“åº”
        if not response:
            raise LLMError("API è¿”å›ç©ºå“åº”", self.config.provider)
            
        choice = response.choices[0] if response.choices else None
        if not choice:
            raise LLMError("APIå“åº”æ ¼å¼å¼‚å¸¸: ç¼ºå°‘choiceså­—æ®µ", self.config.provider)

        usage = None
        if hasattr(response, "usage") and response.usage:
            usage = LLMUsage(
                prompt_tokens=response.usage.prompt_tokens or 0,
                completion_tokens=response.usage.completion_tokens or 0,
                total_tokens=response.usage.total_tokens or 0,
            )

        return LLMResponse(
            content=choice.message.content or "",
            model=response.model,
            usage=usage,
            finish_reason=choice.finish_reason,
        )

    async def validate_config(self) -> bool:
        """éªŒè¯é…ç½®"""
        # Ollama ä¸éœ€è¦ API Key
        if self.config.provider == LLMProvider.OLLAMA:
            if not self.config.model:
                raise LLMError("æœªæŒ‡å®š Ollama æ¨¡å‹", LLMProvider.OLLAMA)
            return True

        # å…¶ä»–æä¾›å•†éœ€è¦ API Key
        if not self.config.api_key:
            raise LLMError(
                f"API Keyæœªé…ç½® ({self.config.provider.value})",
                self.config.provider,
            )

        if not self.config.model:
            raise LLMError(
                f"æœªæŒ‡å®šæ¨¡å‹ ({self.config.provider.value})",
                self.config.provider,
            )

        return True

    @classmethod
    def supports_provider(cls, provider: LLMProvider) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒæŒ‡å®šçš„æä¾›å•†"""
        return provider in cls.PROVIDER_PREFIX_MAP
