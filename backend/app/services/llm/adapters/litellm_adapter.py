"""
LiteLLM ç»Ÿä¸€é€‚é…å™¨
æ”¯æŒé€šè¿‡ LiteLLM è°ƒç”¨å¤šä¸ª LLM æä¾›å•†ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ OpenAI å…¼å®¹æ ¼å¼

å¢å¼ºåŠŸèƒ½:
- Prompt Caching: ä¸ºæ”¯æŒçš„ LLMï¼ˆå¦‚ Claudeï¼‰æ·»åŠ ç¼“å­˜æ ‡è®°
- æ™ºèƒ½é‡è¯•: æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
- æµå¼è¾“å‡º: æ”¯æŒé€ token è¿”å›
"""

import logging
from typing import Dict, Any, Optional, List
from ..base_adapter import BaseLLMAdapter
from ..types import (
    LLMConfig,
    LLMRequest,
    LLMResponse,
    LLMUsage,
    LLMProvider,
    LLMError,
    DEFAULT_BASE_URLS,
)
from ..prompt_cache import prompt_cache_manager, estimate_tokens

logger = logging.getLogger(__name__)


class LiteLLMAdapter(BaseLLMAdapter):
    """
    LiteLLM ç»Ÿä¸€é€‚é…å™¨
    
    æ”¯æŒçš„æä¾›å•†:
    - OpenAI (openai/gpt-4o-mini)
    - Claude (anthropic/claude-3-5-sonnet-20241022)
    - Gemini (gemini/gemini-1.5-flash)
    - DeepSeek (deepseek/deepseek-chat)
    - Qwen (qwen/qwen-turbo) - é€šè¿‡ OpenAI å…¼å®¹æ¨¡å¼
    - Zhipu (zhipu/glm-4-flash) - é€šè¿‡ OpenAI å…¼å®¹æ¨¡å¼
    - Moonshot (moonshot/moonshot-v1-8k) - é€šè¿‡ OpenAI å…¼å®¹æ¨¡å¼
    - Ollama (ollama/llama3)
    """

    # LiteLLM æ¨¡å‹å‰ç¼€æ˜ å°„
    PROVIDER_PREFIX_MAP = {
        LLMProvider.OPENAI: "openai",
        LLMProvider.CLAUDE: "anthropic",
        LLMProvider.GEMINI: "gemini",
        LLMProvider.DEEPSEEK: "deepseek",
        LLMProvider.QWEN: "openai",  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        LLMProvider.ZHIPU: "openai",  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        LLMProvider.MOONSHOT: "openai",  # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼
        LLMProvider.OLLAMA: "ollama",
    }

    # éœ€è¦è‡ªå®šä¹‰ base_url çš„æä¾›å•†
    CUSTOM_BASE_URL_PROVIDERS = {
        LLMProvider.QWEN,
        LLMProvider.ZHIPU,
        LLMProvider.MOONSHOT,
        LLMProvider.DEEPSEEK,
    }

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self._litellm_model = self._get_litellm_model()
        self._api_base = self._get_api_base()

    def _get_litellm_model(self) -> str:
        """è·å– LiteLLM æ ¼å¼çš„æ¨¡å‹åç§°"""
        provider = self.config.provider
        model = self.config.model

        # æ£€æŸ¥æ¨¡å‹åæ˜¯å¦å·²ç»åŒ…å«å‰ç¼€
        if "/" in model:
            return model

        # è·å– provider å‰ç¼€
        prefix = self.PROVIDER_PREFIX_MAP.get(provider, "openai")
        
        return f"{prefix}/{model}"

    def _get_api_base(self) -> Optional[str]:
        """è·å– API åŸºç¡€ URL"""
        # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®çš„ base_url
        if self.config.base_url:
            return self.config.base_url

        # å¯¹äºéœ€è¦è‡ªå®šä¹‰ base_url çš„æä¾›å•†ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if self.config.provider in self.CUSTOM_BASE_URL_PROVIDERS:
            return DEFAULT_BASE_URLS.get(self.config.provider)

        # Ollama ä½¿ç”¨æœ¬åœ°åœ°å€
        if self.config.provider == LLMProvider.OLLAMA:
            return DEFAULT_BASE_URLS.get(LLMProvider.OLLAMA, "http://localhost:11434")

        return None

    async def complete(self, request: LLMRequest) -> LLMResponse:
        """ä½¿ç”¨ LiteLLM å‘é€è¯·æ±‚"""
        try:
            await self.validate_config()
            return await self.retry(lambda: self._send_request(request))
        except Exception as error:
            self.handle_error(error, f"LiteLLM ({self.config.provider.value}) APIè°ƒç”¨å¤±è´¥")

    async def _send_request(self, request: LLMRequest) -> LLMResponse:
        """å‘é€è¯·æ±‚åˆ° LiteLLM"""
        import litellm
        
        # ç¦ç”¨ LiteLLM çš„ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡éƒ½å®é™…è°ƒç”¨ API
        litellm.cache = None
        
        # ç¦ç”¨ LiteLLM è‡ªåŠ¨æ·»åŠ çš„ reasoning_effort å‚æ•°
        # è¿™å¯ä»¥é˜²æ­¢æ¨¡å‹åç§°è¢«é”™è¯¯è§£æä¸º effort å‚æ•°
        litellm.drop_params = True
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        
        # ğŸ”¥ Prompt Caching: ä¸ºæ”¯æŒçš„ LLM æ·»åŠ ç¼“å­˜æ ‡è®°
        cache_enabled = False
        if self.config.provider == LLMProvider.CLAUDE:
            # ä¼°ç®—ç³»ç»Ÿæç¤ºè¯ token æ•°
            system_tokens = 0
            for msg in messages:
                if msg.get("role") == "system":
                    system_tokens += estimate_tokens(msg.get("content", ""))
            
            messages, cache_enabled = prompt_cache_manager.process_messages(
                messages=messages,
                model=self.config.model,
                provider=self.config.provider.value,
                system_prompt_tokens=system_tokens,
            )
            
            if cache_enabled:
                logger.debug(f"ğŸ”¥ Prompt Caching enabled for {self.config.model}")

        # æ„å»ºè¯·æ±‚å‚æ•°
        kwargs: Dict[str, Any] = {
            "model": self._litellm_model,
            "messages": messages,
            "temperature": request.temperature if request.temperature is not None else self.config.temperature,
            "max_tokens": request.max_tokens if request.max_tokens is not None else self.config.max_tokens,
            "top_p": request.top_p if request.top_p is not None else self.config.top_p,
        }

        # è®¾ç½® API Key
        if self.config.api_key and self.config.api_key != "ollama":
            kwargs["api_key"] = self.config.api_key

        # è®¾ç½® API Base URL
        if self._api_base:
            kwargs["api_base"] = self._api_base
            print(f"ğŸ”— ä½¿ç”¨è‡ªå®šä¹‰ API Base: {self._api_base}")

        # è®¾ç½®è¶…æ—¶
        kwargs["timeout"] = self.config.timeout

        # å¯¹äº OpenAI æä¾›å•†ï¼Œæ·»åŠ é¢å¤–å‚æ•°
        if self.config.provider == LLMProvider.OPENAI:
            kwargs["frequency_penalty"] = self.config.frequency_penalty
            kwargs["presence_penalty"] = self.config.presence_penalty

        try:
            # è°ƒç”¨ LiteLLM
            response = await litellm.acompletion(**kwargs)
        except litellm.exceptions.AuthenticationError as e:
            raise LLMError(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸ: {str(e)}", self.config.provider, 401)
        except litellm.exceptions.RateLimitError as e:
            raise LLMError(f"API è°ƒç”¨é¢‘ç‡è¶…é™: {str(e)}", self.config.provider, 429)
        except litellm.exceptions.APIConnectionError as e:
            raise LLMError(f"æ— æ³•è¿æ¥åˆ° API æœåŠ¡: {str(e)}", self.config.provider)
        except litellm.exceptions.APIError as e:
            raise LLMError(f"API é”™è¯¯: {str(e)}", self.config.provider, getattr(e, 'status_code', None))
        except Exception as e:
            # æ•è·å…¶ä»–å¼‚å¸¸å¹¶é‡æ–°æŠ›å‡º
            error_msg = str(e)
            if "invalid_api_key" in error_msg.lower() or "incorrect api key" in error_msg.lower():
                raise LLMError(f"API Key æ— æ•ˆ: {error_msg}", self.config.provider, 401)
            elif "authentication" in error_msg.lower():
                raise LLMError(f"è®¤è¯å¤±è´¥: {error_msg}", self.config.provider, 401)
            raise

        # è§£æå“åº”
        if not response:
            raise LLMError("API è¿”å›ç©ºå“åº”", self.config.provider)
            
        choice = response.choices[0] if response.choices else None
        if not choice:
            raise LLMError("APIå“åº”æ ¼å¼å¼‚å¸¸: ç¼ºå°‘choiceså­—æ®µ", self.config.provider)

        usage = None
        if hasattr(response, "usage") and response.usage:
            usage = LLMUsage(
                prompt_tokens=response.usage.prompt_tokens or 0,
                completion_tokens=response.usage.completion_tokens or 0,
                total_tokens=response.usage.total_tokens or 0,
            )
            
            # ğŸ”¥ æ›´æ–° Prompt Cache ç»Ÿè®¡
            if cache_enabled and hasattr(response.usage, "cache_creation_input_tokens"):
                prompt_cache_manager.update_stats(
                    cache_creation_input_tokens=getattr(response.usage, "cache_creation_input_tokens", 0),
                    cache_read_input_tokens=getattr(response.usage, "cache_read_input_tokens", 0),
                    total_input_tokens=response.usage.prompt_tokens or 0,
                )

        return LLMResponse(
            content=choice.message.content or "",
            model=response.model,
            usage=usage,
            finish_reason=choice.finish_reason,
        )

    async def stream_complete(self, request: LLMRequest):
        """
        æµå¼è°ƒç”¨ LLMï¼Œé€ token è¿”å›
        
        Yields:
            dict: {"type": "token", "content": str} æˆ– {"type": "done", "content": str, "usage": dict}
        """
        import litellm
        
        await self.validate_config()
        
        litellm.cache = None
        litellm.drop_params = True
        
        messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        
        # ğŸ”¥ ä¼°ç®—è¾“å…¥ token æ•°é‡ï¼ˆç”¨äºåœ¨æ— æ³•è·å–çœŸå® usage æ—¶è¿›è¡Œä¼°ç®—ï¼‰
        input_tokens_estimate = sum(estimate_tokens(msg["content"]) for msg in messages)
        
        kwargs = {
            "model": self._litellm_model,
            "messages": messages,
            "temperature": request.temperature if request.temperature is not None else self.config.temperature,
            "max_tokens": request.max_tokens if request.max_tokens is not None else self.config.max_tokens,
            "top_p": request.top_p if request.top_p is not None else self.config.top_p,
            "stream": True,  # å¯ç”¨æµå¼è¾“å‡º
        }
        
        # ğŸ”¥ å¯¹äºæ”¯æŒçš„æ¨¡å‹ï¼Œè¯·æ±‚åœ¨æµå¼è¾“å‡ºä¸­åŒ…å« usage ä¿¡æ¯
        # OpenAI API æ”¯æŒ stream_options
        if self.config.provider in [LLMProvider.OPENAI, LLMProvider.DEEPSEEK]:
            kwargs["stream_options"] = {"include_usage": True}
        
        if self.config.api_key and self.config.api_key != "ollama":
            kwargs["api_key"] = self.config.api_key
        
        if self._api_base:
            kwargs["api_base"] = self._api_base
        
        kwargs["timeout"] = self.config.timeout
        
        accumulated_content = ""
        final_usage = None  # ğŸ”¥ å­˜å‚¨æœ€ç»ˆçš„ usage ä¿¡æ¯
        
        try:
            response = await litellm.acompletion(**kwargs)
            
            async for chunk in response:
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯ï¼ˆæŸäº› API ä¼šåœ¨æœ€åçš„ chunk ä¸­åŒ…å«ï¼‰
                if hasattr(chunk, "usage") and chunk.usage:
                    final_usage = {
                        "prompt_tokens": chunk.usage.prompt_tokens or 0,
                        "completion_tokens": chunk.usage.completion_tokens or 0,
                        "total_tokens": chunk.usage.total_tokens or 0,
                    }
                    logger.debug(f"Got usage from chunk: {final_usage}")
                
                if not chunk.choices:
                    continue
                
                delta = chunk.choices[0].delta
                content = getattr(delta, "content", "") or ""
                finish_reason = chunk.choices[0].finish_reason
                
                if content:
                    accumulated_content += content
                    yield {
                        "type": "token",
                        "content": content,
                        "accumulated": accumulated_content,
                    }
                else:
                    # Log when we get a chunk without content
                    logger.debug(f"Chunk with no content: {chunk}")

                if finish_reason:
                    # æµå¼å®Œæˆ
                    # ğŸ”¥ å¦‚æœæ²¡æœ‰ä» chunk è·å–åˆ° usageï¼Œè¿›è¡Œä¼°ç®—
                    if not final_usage:
                        output_tokens_estimate = estimate_tokens(accumulated_content)
                        final_usage = {
                            "prompt_tokens": input_tokens_estimate,
                            "completion_tokens": output_tokens_estimate,
                            "total_tokens": input_tokens_estimate + output_tokens_estimate,
                        }
                        logger.debug(f"Estimated usage: {final_usage}")
                    
                    yield {
                        "type": "done",
                        "content": accumulated_content,
                        "usage": final_usage,
                        "finish_reason": finish_reason,
                    }
                    break
                    
        except Exception as e:
            # ğŸ”¥ å³ä½¿å‡ºé”™ï¼Œä¹Ÿå°è¯•è¿”å›ä¼°ç®—çš„ usage
            output_tokens_estimate = estimate_tokens(accumulated_content) if accumulated_content else 0
            yield {
                "type": "error",
                "error": str(e),
                "accumulated": accumulated_content,
                "usage": {
                    "prompt_tokens": input_tokens_estimate,
                    "completion_tokens": output_tokens_estimate,
                    "total_tokens": input_tokens_estimate + output_tokens_estimate,
                } if accumulated_content else None,
            }

    async def validate_config(self) -> bool:
        """éªŒè¯é…ç½®"""
        # Ollama ä¸éœ€è¦ API Key
        if self.config.provider == LLMProvider.OLLAMA:
            if not self.config.model:
                raise LLMError("æœªæŒ‡å®š Ollama æ¨¡å‹", LLMProvider.OLLAMA)
            return True

        # å…¶ä»–æä¾›å•†éœ€è¦ API Key
        if not self.config.api_key:
            raise LLMError(
                f"API Keyæœªé…ç½® ({self.config.provider.value})",
                self.config.provider,
            )

        # check for placeholder keys
        if "sk-your-" in self.config.api_key or "***" in self.config.api_key:
             raise LLMError(
                f"æ— æ•ˆçš„ API Key (ä½¿ç”¨äº†å ä½ç¬¦): {self.config.api_key[:10]}...",
                self.config.provider,
                401
            )

        if not self.config.model:
            raise LLMError(
                f"æœªæŒ‡å®šæ¨¡å‹ ({self.config.provider.value})",
                self.config.provider,
            )

        return True

    @classmethod
    def supports_provider(cls, provider: LLMProvider) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¯æŒæŒ‡å®šçš„æä¾›å•†"""
        return provider in cls.PROVIDER_PREFIX_MAP
