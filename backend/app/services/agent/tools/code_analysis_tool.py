"""
ä»£ç åˆ†æå·¥å…·
ä½¿ç”¨ LLM æ·±åº¦åˆ†æä»£ç å®‰å…¨é—®é¢˜
"""

import json
import logging
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field

from .base import AgentTool, ToolResult

logger = logging.getLogger(__name__)


class CodeAnalysisInput(BaseModel):
    """ä»£ç åˆ†æè¾“å…¥"""
    code: str = Field(description="è¦åˆ†æçš„ä»£ç å†…å®¹")
    file_path: str = Field(default="unknown", description="æ–‡ä»¶è·¯å¾„")
    language: str = Field(default="python", description="ç¼–ç¨‹è¯­è¨€")
    focus: Optional[str] = Field(
        default=None,
        description="é‡ç‚¹å…³æ³¨çš„æ¼æ´ç±»å‹ï¼Œå¦‚ sql_injection, xss, command_injection"
    )
    context: Optional[str] = Field(
        default=None,
        description="é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¦‚ç›¸å…³çš„å…¶ä»–ä»£ç ç‰‡æ®µ"
    )


class CodeAnalysisTool(AgentTool):
    """
    ä»£ç åˆ†æå·¥å…·
    ä½¿ç”¨ LLM å¯¹ä»£ç è¿›è¡Œæ·±åº¦å®‰å…¨åˆ†æ
    """
    
    def __init__(self, llm_service):
        """
        åˆå§‹åŒ–ä»£ç åˆ†æå·¥å…·
        
        Args:
            llm_service: LLM æœåŠ¡å®ä¾‹
        """
        super().__init__()
        self.llm_service = llm_service
    
    @property
    def name(self) -> str:
        return "code_analysis"
    
    @property
    def description(self) -> str:
        return """æ·±åº¦åˆ†æä»£ç å®‰å…¨é—®é¢˜ã€‚
ä½¿ç”¨ LLM å¯¹ä»£ç è¿›è¡Œå…¨é¢çš„å®‰å…¨å®¡è®¡ï¼Œè¯†åˆ«æ½œåœ¨æ¼æ´ã€‚

ä½¿ç”¨åœºæ™¯:
- å¯¹ç–‘ä¼¼æœ‰é—®é¢˜çš„ä»£ç è¿›è¡Œæ·±å…¥åˆ†æ
- åˆ†æå¤æ‚çš„ä¸šåŠ¡é€»è¾‘æ¼æ´
- è¿½è¸ªæ•°æ®æµå’Œæ±¡ç‚¹ä¼ æ’­
- ç”Ÿæˆè¯¦ç»†çš„æ¼æ´æŠ¥å‘Šå’Œä¿®å¤å»ºè®®

è¾“å…¥:
- code: è¦åˆ†æçš„ä»£ç 
- file_path: æ–‡ä»¶è·¯å¾„
- language: ç¼–ç¨‹è¯­è¨€
- focus: å¯é€‰ï¼Œé‡ç‚¹å…³æ³¨çš„æ¼æ´ç±»å‹
- context: å¯é€‰ï¼Œé¢å¤–çš„ä¸Šä¸‹æ–‡ä»£ç 

è¿™ä¸ªå·¥å…·ä¼šæ¶ˆè€—è¾ƒå¤šçš„ Tokenï¼Œå»ºè®®åœ¨ç¡®è®¤æœ‰ç–‘ä¼¼é—®é¢˜åä½¿ç”¨ã€‚"""
    
    @property
    def args_schema(self):
        return CodeAnalysisInput
    
    async def _execute(
        self,
        code: str,
        file_path: str = "unknown",
        language: str = "python",
        focus: Optional[str] = None,
        context: Optional[str] = None,
        **kwargs
    ) -> ToolResult:
        """æ‰§è¡Œä»£ç åˆ†æ"""
        import asyncio
        
        try:
            # é™åˆ¶ä»£ç é•¿åº¦ï¼Œé¿å…è¶…æ—¶
            max_code_length = 50000  # çº¦ 50KB
            if len(code) > max_code_length:
                code = code[:max_code_length] + "\n\n... (ä»£ç å·²æˆªæ–­ï¼Œä»…åˆ†æå‰ 50000 å­—ç¬¦)"
            
            # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼ˆ5åˆ†é’Ÿï¼‰
            try:
                analysis = await asyncio.wait_for(
                    self.llm_service.analyze_code(code, language),
                    timeout=300.0  # 5åˆ†é’Ÿè¶…æ—¶
                )
            except asyncio.TimeoutError:
                return ToolResult(
                    success=False,
                    error="ä»£ç åˆ†æè¶…æ—¶ï¼ˆè¶…è¿‡5åˆ†é’Ÿï¼‰ã€‚ä»£ç å¯èƒ½è¿‡é•¿æˆ–è¿‡äºå¤æ‚ï¼Œè¯·å°è¯•åˆ†æè¾ƒå°çš„ä»£ç ç‰‡æ®µã€‚",
                )
            
            issues = analysis.get("issues", [])
            
            if not issues:
                return ToolResult(
                    success=True,
                    data="ä»£ç åˆ†æå®Œæˆï¼Œæœªå‘ç°æ˜æ˜¾çš„å®‰å…¨é—®é¢˜ã€‚\n\n"
                         f"è´¨é‡è¯„åˆ†: {analysis.get('quality_score', 'N/A')}\n"
                         f"æ–‡ä»¶: {file_path}",
                    metadata={
                        "file_path": file_path,
                        "issues_count": 0,
                        "quality_score": analysis.get("quality_score"),
                    }
                )
            
            # æ ¼å¼åŒ–è¾“å‡º
            output_parts = [f"ğŸ” ä»£ç åˆ†æç»“æœ - {file_path}\n"]
            output_parts.append(f"å‘ç° {len(issues)} ä¸ªé—®é¢˜:\n")
            
            for i, issue in enumerate(issues):
                severity_icon = {
                    "critical": "ğŸ”´",
                    "high": "ğŸŸ ", 
                    "medium": "ğŸŸ¡",
                    "low": "ğŸŸ¢"
                }.get(issue.get("severity", ""), "âšª")
                
                output_parts.append(f"\n{severity_icon} é—®é¢˜ {i+1}: {issue.get('title', 'Unknown')}")
                output_parts.append(f"   ç±»å‹: {issue.get('type', 'unknown')}")
                output_parts.append(f"   ä¸¥é‡ç¨‹åº¦: {issue.get('severity', 'unknown')}")
                output_parts.append(f"   è¡Œå·: {issue.get('line', 'N/A')}")
                output_parts.append(f"   æè¿°: {issue.get('description', '')}")
                
                if issue.get("code_snippet"):
                    output_parts.append(f"   ä»£ç ç‰‡æ®µ:\n   ```\n   {issue.get('code_snippet')}\n   ```")
                
                if issue.get("suggestion"):
                    output_parts.append(f"   ä¿®å¤å»ºè®®: {issue.get('suggestion')}")
                
                if issue.get("ai_explanation"):
                    output_parts.append(f"   AIè§£é‡Š: {issue.get('ai_explanation')}")
            
            output_parts.append(f"\nè´¨é‡è¯„åˆ†: {analysis.get('quality_score', 'N/A')}/100")
            
            return ToolResult(
                success=True,
                data="\n".join(output_parts),
                metadata={
                    "file_path": file_path,
                    "issues_count": len(issues),
                    "quality_score": analysis.get("quality_score"),
                    "issues": issues,
                }
            )
            
        except Exception as e:
            import traceback
            logger.error(f"ä»£ç åˆ†æå¤±è´¥: {e}")
            logger.error(f"LLM Provider: {self.llm_service.config.provider.value if self.llm_service.config else 'N/A'}")
            logger.error(f"LLM Model: {self.llm_service.config.model if self.llm_service.config else 'N/A'}")
            logger.error(f"API Key å‰ç¼€: {self.llm_service.config.api_key[:10] + '...' if self.llm_service.config and self.llm_service.config.api_key else 'N/A'}")
            logger.error(traceback.format_exc())
            return ToolResult(
                success=False,
                error=f"ä»£ç åˆ†æå¤±è´¥: {str(e)}",
            )


class DataFlowAnalysisInput(BaseModel):
    """æ•°æ®æµåˆ†æè¾“å…¥"""
    source_code: str = Field(description="åŒ…å«æ•°æ®æºçš„ä»£ç ")
    sink_code: Optional[str] = Field(default=None, description="åŒ…å«æ•°æ®æ±‡çš„ä»£ç ï¼ˆå¦‚å±é™©å‡½æ•°ï¼‰")
    variable_name: str = Field(description="è¦è¿½è¸ªçš„å˜é‡å")
    file_path: str = Field(default="unknown", description="æ–‡ä»¶è·¯å¾„")


class DataFlowAnalysisTool(AgentTool):
    """
    æ•°æ®æµåˆ†æå·¥å…·
    è¿½è¸ªå˜é‡ä»æºåˆ°æ±‡çš„æ•°æ®æµ
    """
    
    def __init__(self, llm_service):
        super().__init__()
        self.llm_service = llm_service
    
    @property
    def name(self) -> str:
        return "dataflow_analysis"
    
    @property
    def description(self) -> str:
        return """åˆ†æä»£ç ä¸­çš„æ•°æ®æµï¼Œè¿½è¸ªå˜é‡ä»æºï¼ˆå¦‚ç”¨æˆ·è¾“å…¥ï¼‰åˆ°æ±‡ï¼ˆå¦‚å±é™©å‡½æ•°ï¼‰çš„è·¯å¾„ã€‚

ä½¿ç”¨åœºæ™¯:
- è¿½è¸ªç”¨æˆ·è¾“å…¥å¦‚ä½•æµå‘å±é™©å‡½æ•°
- åˆ†æå˜é‡æ˜¯å¦ç»è¿‡å‡€åŒ–å¤„ç†
- è¯†åˆ«æ±¡ç‚¹ä¼ æ’­è·¯å¾„

è¾“å…¥:
- source_code: åŒ…å«æ•°æ®æºçš„ä»£ç 
- sink_code: åŒ…å«æ•°æ®æ±‡çš„ä»£ç ï¼ˆå¯é€‰ï¼‰
- variable_name: è¦è¿½è¸ªçš„å˜é‡å
- file_path: æ–‡ä»¶è·¯å¾„"""
    
    @property
    def args_schema(self):
        return DataFlowAnalysisInput
    
    async def _execute(
        self,
        source_code: str,
        variable_name: str,
        sink_code: Optional[str] = None,
        file_path: str = "unknown",
        **kwargs
    ) -> ToolResult:
        """æ‰§è¡Œæ•°æ®æµåˆ†æ - å¢å¼ºç‰ˆï¼Œå¸¦è¶…æ—¶ä¿æŠ¤å’Œå›é€€é€»è¾‘"""
        import asyncio
        import re
        
        # ğŸ”¥ é¦–å…ˆå°è¯•åŸºäºè§„åˆ™çš„å¿«é€Ÿåˆ†æï¼ˆä¸ä¾èµ– LLMï¼‰
        quick_analysis = self._quick_pattern_analysis(source_code, variable_name, sink_code)
        
        try:
            # æ„å»ºåˆ†æ prompt
            analysis_prompt = f"""åˆ†æä»¥ä¸‹ä»£ç ä¸­å˜é‡ '{variable_name}' çš„æ•°æ®æµã€‚

æºä»£ç :
```
{source_code}
```
"""
            if sink_code:
                analysis_prompt += f"""
æ±‡ä»£ç ï¼ˆå¯èƒ½çš„å±é™©å‡½æ•°ï¼‰:
```
{sink_code}
```
"""

            analysis_prompt += f"""
è¯·åˆ†æ:
1. å˜é‡ '{variable_name}' çš„æ¥æºæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆç”¨æˆ·è¾“å…¥ã€é…ç½®ã€æ•°æ®åº“ç­‰ï¼‰
2. å˜é‡åœ¨ä¼ é€’è¿‡ç¨‹ä¸­æ˜¯å¦ç»è¿‡äº†å‡€åŒ–/éªŒè¯ï¼Ÿ
3. å˜é‡æœ€ç»ˆæµå‘äº†å“ªäº›å±é™©å‡½æ•°ï¼Ÿ
4. æ˜¯å¦å­˜åœ¨å®‰å…¨é£é™©ï¼Ÿ

è¯·è¿”å› JSON æ ¼å¼çš„åˆ†æç»“æœï¼ŒåŒ…å«:
- source_type: æ•°æ®æºç±»å‹
- sanitized: æ˜¯å¦ç»è¿‡å‡€åŒ–
- sanitization_methods: ä½¿ç”¨çš„å‡€åŒ–æ–¹æ³•
- dangerous_sinks: æµå‘çš„å±é™©å‡½æ•°åˆ—è¡¨
- risk_level: é£é™©ç­‰çº§ (high/medium/low/none)
- explanation: è¯¦ç»†è§£é‡Š
- recommendation: å»ºè®®
"""
            
            # ğŸ”¥ æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼ˆ2åˆ†é’Ÿï¼‰
            try:
                result = await asyncio.wait_for(
                    self.llm_service.analyze_code_with_custom_prompt(
                        code=source_code,
                        language="text",
                        custom_prompt=analysis_prompt,
                    ),
                    timeout=120.0  # 2åˆ†é’Ÿè¶…æ—¶
                )
            except asyncio.TimeoutError:
                logger.warning(f"æ•°æ®æµåˆ†æ LLM è°ƒç”¨è¶…æ—¶ï¼Œä½¿ç”¨å¿«é€Ÿåˆ†æç»“æœ")
                return self._format_quick_analysis_result(quick_analysis, variable_name, file_path, "LLMè°ƒç”¨è¶…æ—¶ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ")
            
            # ğŸ”¥ æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if not result or (isinstance(result, dict) and not result.get("source_type") and not result.get("risk_level")):
                logger.warning(f"æ•°æ®æµåˆ†æ LLM è¿”å›æ— æ•ˆç»“æœï¼Œä½¿ç”¨å¿«é€Ÿåˆ†æç»“æœ")
                return self._format_quick_analysis_result(quick_analysis, variable_name, file_path, "LLMè¿”å›æ— æ•ˆï¼Œä½¿ç”¨è§„åˆ™åˆ†æ")
            
            # æ ¼å¼åŒ–è¾“å‡º
            output_parts = [f"ğŸ“Š æ•°æ®æµåˆ†æç»“æœ - å˜é‡: {variable_name}\n"]
            
            if isinstance(result, dict):
                if result.get("source_type"):
                    output_parts.append(f"æ•°æ®æº: {result.get('source_type')}")
                if result.get("sanitized") is not None:
                    sanitized = "âœ… æ˜¯" if result.get("sanitized") else "âŒ å¦"
                    output_parts.append(f"æ˜¯å¦å‡€åŒ–: {sanitized}")
                if result.get("sanitization_methods"):
                    methods = result.get('sanitization_methods', [])
                    if isinstance(methods, list):
                        output_parts.append(f"å‡€åŒ–æ–¹æ³•: {', '.join(methods)}")
                    else:
                        output_parts.append(f"å‡€åŒ–æ–¹æ³•: {methods}")
                if result.get("dangerous_sinks"):
                    sinks = result.get('dangerous_sinks', [])
                    if isinstance(sinks, list):
                        output_parts.append(f"å±é™©å‡½æ•°: {', '.join(sinks)}")
                    else:
                        output_parts.append(f"å±é™©å‡½æ•°: {sinks}")
                if result.get("risk_level"):
                    risk_icons = {"high": "ğŸ”´", "medium": "ğŸŸ ", "low": "ğŸŸ¡", "none": "ğŸŸ¢"}
                    icon = risk_icons.get(result.get("risk_level", ""), "âšª")
                    output_parts.append(f"é£é™©ç­‰çº§: {icon} {result.get('risk_level', '').upper()}")
                if result.get("explanation"):
                    output_parts.append(f"\nåˆ†æ: {result.get('explanation')}")
                if result.get("recommendation"):
                    output_parts.append(f"\nå»ºè®®: {result.get('recommendation')}")
            else:
                output_parts.append(str(result))
            
            return ToolResult(
                success=True,
                data="\n".join(output_parts),
                metadata={
                    "variable": variable_name,
                    "file_path": file_path,
                    "analysis": result,
                }
            )
            
        except Exception as e:
            logger.error(f"æ•°æ®æµåˆ†æå¤±è´¥: {e}")
            # ğŸ”¥ å›é€€åˆ°å¿«é€Ÿåˆ†æ
            return self._format_quick_analysis_result(
                quick_analysis, 
                variable_name, 
                file_path, 
                f"LLMè°ƒç”¨å¤±è´¥({str(e)[:50]}...)ï¼Œä½¿ç”¨è§„åˆ™åˆ†æ"
            )
    
    def _quick_pattern_analysis(
        self, 
        source_code: str, 
        variable_name: str,
        sink_code: Optional[str] = None
    ) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„å¿«é€Ÿæ•°æ®æµåˆ†æï¼ˆä¸ä¾èµ– LLMï¼‰"""
        import re
        
        result = {
            "source_type": "unknown",
            "sanitized": False,
            "sanitization_methods": [],
            "dangerous_sinks": [],
            "risk_level": "low",
        }
        
        code_to_analyze = source_code + (sink_code or "")
        
        # æ£€æµ‹æ•°æ®æºç±»å‹
        source_patterns = {
            "user_input_get": r'\$_GET\[',
            "user_input_post": r'\$_POST\[',
            "user_input_request": r'\$_REQUEST\[',
            "user_input_cookie": r'\$_COOKIE\[',
            "request_param": r'request\.(GET|POST|args|form|data)',
            "input_func": r'\binput\s*\(',
        }
        
        for source_name, pattern in source_patterns.items():
            if re.search(pattern, source_code, re.IGNORECASE):
                result["source_type"] = source_name
                break
        
        # æ£€æµ‹å‡€åŒ–æ–¹æ³•
        sanitize_patterns = [
            (r'htmlspecialchars\s*\(', "htmlspecialchars"),
            (r'mysqli_real_escape_string\s*\(', "mysqli_escape"),
            (r'addslashes\s*\(', "addslashes"),
            (r'strip_tags\s*\(', "strip_tags"),
            (r'filter_var\s*\(', "filter_var"),
            (r'escape\s*\(', "escape"),
            (r'sanitize', "sanitize"),
            (r'validate', "validate"),
        ]
        
        for pattern, name in sanitize_patterns:
            if re.search(pattern, code_to_analyze, re.IGNORECASE):
                result["sanitized"] = True
                result["sanitization_methods"].append(name)
        
        # æ£€æµ‹å±é™© sink
        sink_patterns = [
            (r'mysql_query\s*\(', "mysql_query"),
            (r'mysqli_query\s*\(', "mysqli_query"),
            (r'execute\s*\(', "execute"),
            (r'shell_exec\s*\(', "shell_exec"),
            (r'system\s*\(', "system"),
            (r'exec\s*\(', "exec"),
            (r'eval\s*\(', "eval"),
            (r'include\s*\(', "include"),
            (r'require\s*\(', "require"),
            (r'file_get_contents\s*\(', "file_get_contents"),
            (r'echo\s+', "echo"),
            (r'print\s*\(', "print"),
        ]
        
        for pattern, name in sink_patterns:
            if re.search(pattern, code_to_analyze, re.IGNORECASE):
                result["dangerous_sinks"].append(name)
        
        # è®¡ç®—é£é™©ç­‰çº§
        if result["source_type"].startswith("user_input") and result["dangerous_sinks"]:
            if not result["sanitized"]:
                result["risk_level"] = "high"
            else:
                result["risk_level"] = "medium"
        elif result["dangerous_sinks"]:
            result["risk_level"] = "medium"
        
        return result
    
    def _format_quick_analysis_result(
        self, 
        analysis: Dict[str, Any], 
        variable_name: str,
        file_path: str,
        note: str
    ) -> ToolResult:
        """æ ¼å¼åŒ–å¿«é€Ÿåˆ†æç»“æœ"""
        output_parts = [f"ğŸ“Š æ•°æ®æµåˆ†æç»“æœ - å˜é‡: {variable_name}"]
        output_parts.append(f"âš ï¸ æ³¨æ„: {note}\n")
        
        output_parts.append(f"æ•°æ®æº: {analysis.get('source_type', 'unknown')}")
        output_parts.append(f"æ˜¯å¦å‡€åŒ–: {'âœ… æ˜¯' if analysis.get('sanitized') else 'âŒ å¦'}")
        
        if analysis.get("sanitization_methods"):
            output_parts.append(f"å‡€åŒ–æ–¹æ³•: {', '.join(analysis['sanitization_methods'])}")
        
        if analysis.get("dangerous_sinks"):
            output_parts.append(f"å±é™©å‡½æ•°: {', '.join(analysis['dangerous_sinks'])}")
        
        risk_icons = {"high": "ğŸ”´", "medium": "ğŸŸ ", "low": "ğŸŸ¡", "none": "ğŸŸ¢"}
        risk = analysis.get("risk_level", "low")
        output_parts.append(f"é£é™©ç­‰çº§: {risk_icons.get(risk, 'âšª')} {risk.upper()}")
        
        return ToolResult(
            success=True,
            data="\n".join(output_parts),
            metadata={
                "variable": variable_name,
                "file_path": file_path,
                "analysis": analysis,
                "fallback_used": True,
            }
        )


class VulnerabilityValidationInput(BaseModel):
    """æ¼æ´éªŒè¯è¾“å…¥"""
    code: str = Field(description="å¯èƒ½å­˜åœ¨æ¼æ´çš„ä»£ç ")
    vulnerability_type: str = Field(description="æ¼æ´ç±»å‹")
    file_path: str = Field(default="unknown", description="æ–‡ä»¶è·¯å¾„")
    line_number: Optional[int] = Field(default=None, description="è¡Œå·")
    context: Optional[str] = Field(default=None, description="é¢å¤–ä¸Šä¸‹æ–‡")


class VulnerabilityValidationTool(AgentTool):
    """
    æ¼æ´éªŒè¯å·¥å…·
    éªŒè¯ç–‘ä¼¼æ¼æ´æ˜¯å¦çœŸå®å­˜åœ¨
    """
    
    def __init__(self, llm_service):
        super().__init__()
        self.llm_service = llm_service
    
    @property
    def name(self) -> str:
        return "vulnerability_validation"
    
    @property
    def description(self) -> str:
        return """éªŒè¯ç–‘ä¼¼æ¼æ´æ˜¯å¦çœŸå®å­˜åœ¨ã€‚
å¯¹å‘ç°çš„æ½œåœ¨æ¼æ´è¿›è¡Œæ·±å…¥åˆ†æï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£çš„å®‰å…¨é—®é¢˜ã€‚

è¾“å…¥:
- code: åŒ…å«ç–‘ä¼¼æ¼æ´çš„ä»£ç 
- vulnerability_type: æ¼æ´ç±»å‹ï¼ˆå¦‚ sql_injection, xss ç­‰ï¼‰
- file_path: æ–‡ä»¶è·¯å¾„
- line_number: å¯é€‰ï¼Œè¡Œå·
- context: å¯é€‰ï¼Œé¢å¤–çš„ä¸Šä¸‹æ–‡ä»£ç 

è¾“å‡º:
- éªŒè¯ç»“æœï¼ˆç¡®è®¤/å¯èƒ½/è¯¯æŠ¥ï¼‰
- è¯¦ç»†åˆ†æ
- åˆ©ç”¨æ¡ä»¶
- PoC æ€è·¯ï¼ˆå¦‚æœç¡®è®¤å­˜åœ¨æ¼æ´ï¼‰"""
    
    @property
    def args_schema(self):
        return VulnerabilityValidationInput
    
    async def _execute(
        self,
        code: str,
        vulnerability_type: str,
        file_path: str = "unknown",
        line_number: Optional[int] = None,
        context: Optional[str] = None,
        **kwargs
    ) -> ToolResult:
        """æ‰§è¡Œæ¼æ´éªŒè¯"""
        try:
            validation_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®‰å…¨ç ”ç©¶å‘˜ï¼Œè¯·éªŒè¯ä»¥ä¸‹ä»£ç ä¸­æ˜¯å¦çœŸçš„å­˜åœ¨ {vulnerability_type} æ¼æ´ã€‚

ä»£ç :
```
{code}
```

{f'é¢å¤–ä¸Šä¸‹æ–‡:' + chr(10) + '```' + chr(10) + context + chr(10) + '```' if context else ''}

è¯·åˆ†æ:
1. è¿™æ®µä»£ç æ˜¯å¦çœŸçš„å­˜åœ¨ {vulnerability_type} æ¼æ´ï¼Ÿ
2. æ¼æ´çš„åˆ©ç”¨æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ
3. æ”»å‡»è€…å¦‚ä½•åˆ©ç”¨è¿™ä¸ªæ¼æ´ï¼Ÿ
4. è¿™æ˜¯å¦å¯èƒ½æ˜¯è¯¯æŠ¥ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

è¯·è¿”å› JSON æ ¼å¼:
{{
    "is_vulnerable": true/false/null (nullè¡¨ç¤ºæ— æ³•ç¡®å®š),
    "confidence": 0.0-1.0,
    "verdict": "confirmed/likely/unlikely/false_positive",
    "exploitation_conditions": ["æ¡ä»¶1", "æ¡ä»¶2"],
    "attack_vector": "æ”»å‡»å‘é‡æè¿°",
    "poc_idea": "PoCæ€è·¯ï¼ˆå¦‚æœå­˜åœ¨æ¼æ´ï¼‰",
    "false_positive_reason": "å¦‚æœæ˜¯è¯¯æŠ¥ï¼Œè¯´æ˜åŸå› ",
    "detailed_analysis": "è¯¦ç»†åˆ†æ"
}}
"""
            
            result = await self.llm_service.analyze_code_with_custom_prompt(
                code=code,
                language="text",
                custom_prompt=validation_prompt,
            )
            
            # æ ¼å¼åŒ–è¾“å‡º
            output_parts = [f"ğŸ” æ¼æ´éªŒè¯ç»“æœ - {vulnerability_type}\n"]
            output_parts.append(f"æ–‡ä»¶: {file_path}")
            if line_number:
                output_parts.append(f"è¡Œå·: {line_number}")
            output_parts.append("")
            
            if isinstance(result, dict):
                # éªŒè¯ç»“æœ
                verdict_icons = {
                    "confirmed": "ğŸ”´ ç¡®è®¤å­˜åœ¨æ¼æ´",
                    "likely": "ğŸŸ  å¯èƒ½å­˜åœ¨æ¼æ´",
                    "unlikely": "ğŸŸ¡ å¯èƒ½æ˜¯è¯¯æŠ¥",
                    "false_positive": "ğŸŸ¢ è¯¯æŠ¥",
                }
                verdict = result.get("verdict", "unknown")
                output_parts.append(f"åˆ¤å®š: {verdict_icons.get(verdict, verdict)}")
                
                if result.get("confidence"):
                    output_parts.append(f"ç½®ä¿¡åº¦: {result.get('confidence') * 100:.0f}%")
                
                if result.get("exploitation_conditions"):
                    output_parts.append(f"\nåˆ©ç”¨æ¡ä»¶:")
                    for cond in result.get("exploitation_conditions", []):
                        output_parts.append(f"  - {cond}")
                
                if result.get("attack_vector"):
                    output_parts.append(f"\næ”»å‡»å‘é‡: {result.get('attack_vector')}")
                
                if result.get("poc_idea") and verdict in ["confirmed", "likely"]:
                    output_parts.append(f"\nPoCæ€è·¯: {result.get('poc_idea')}")
                
                if result.get("false_positive_reason") and verdict in ["unlikely", "false_positive"]:
                    output_parts.append(f"\nè¯¯æŠ¥åŸå› : {result.get('false_positive_reason')}")
                
                if result.get("detailed_analysis"):
                    output_parts.append(f"\nè¯¦ç»†åˆ†æ:\n{result.get('detailed_analysis')}")
            else:
                output_parts.append(str(result))
            
            return ToolResult(
                success=True,
                data="\n".join(output_parts),
                metadata={
                    "vulnerability_type": vulnerability_type,
                    "file_path": file_path,
                    "line_number": line_number,
                    "validation": result,
                }
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                error=f"æ¼æ´éªŒè¯å¤±è´¥: {str(e)}",
            )

