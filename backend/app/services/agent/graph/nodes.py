"""
LangGraph èŠ‚ç‚¹å®ç°
æ¯ä¸ªèŠ‚ç‚¹å°è£…ä¸€ä¸ª Agent çš„æ‰§è¡Œé€»è¾‘

åä½œå¢å¼ºï¼šèŠ‚ç‚¹ä¹‹é—´é€šè¿‡ TaskHandoff ä¼ é€’ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡å’Œæ´å¯Ÿ
"""

from typing import Dict, Any, List, Optional
import logging

logger = logging.getLogger(__name__)

# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
def get_audit_state_type():
    from .audit_graph import AuditState
    return AuditState


class BaseNode:
    """èŠ‚ç‚¹åŸºç±»"""
    
    def __init__(self, agent=None, event_emitter=None):
        self.agent = agent
        self.event_emitter = event_emitter
    
    async def emit_event(self, event_type: str, message: str, **kwargs):
        """å‘å°„äº‹ä»¶"""
        if self.event_emitter:
            try:
                await self.event_emitter.emit_info(message)
            except Exception as e:
                logger.warning(f"Failed to emit event: {e}")
    
    def _extract_handoff_from_state(self, state: Dict[str, Any], from_phase: str):
        """ä»çŠ¶æ€ä¸­æå–å‰åº Agent çš„ handoff"""
        handoff_data = state.get(f"{from_phase}_handoff")
        if handoff_data:
            from ..agents.base import TaskHandoff
            return TaskHandoff.from_dict(handoff_data)
        return None


class ReconNode(BaseNode):
    """
    ä¿¡æ¯æ”¶é›†èŠ‚ç‚¹
    
    è¾“å…¥: project_root, project_info, config
    è¾“å‡º: tech_stack, entry_points, high_risk_areas, dependencies, recon_handoff
    """
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¿¡æ¯æ”¶é›†"""
        await self.emit_event("phase_start", "ğŸ” å¼€å§‹ä¿¡æ¯æ”¶é›†é˜¶æ®µ")
        
        try:
            # è°ƒç”¨ Recon Agent
            result = await self.agent.run({
                "project_info": state["project_info"],
                "config": state["config"],
            })
            
            if result.success and result.data:
                data = result.data
                
                # ğŸ”¥ åˆ›å»ºäº¤æ¥ä¿¡æ¯ç»™ Analysis Agent
                handoff = self.agent.create_handoff(
                    to_agent="Analysis",
                    summary=f"é¡¹ç›®ä¿¡æ¯æ”¶é›†å®Œæˆã€‚å‘ç° {len(data.get('entry_points', []))} ä¸ªå…¥å£ç‚¹ï¼Œ{len(data.get('high_risk_areas', []))} ä¸ªé«˜é£é™©åŒºåŸŸã€‚",
                    key_findings=data.get("initial_findings", []),
                    suggested_actions=[
                        {
                            "type": "deep_analysis",
                            "description": f"æ·±å…¥åˆ†æé«˜é£é™©åŒºåŸŸ: {', '.join(data.get('high_risk_areas', [])[:5])}",
                            "priority": "high",
                        },
                        {
                            "type": "entry_point_audit",
                            "description": "å®¡è®¡æ‰€æœ‰å…¥å£ç‚¹çš„è¾“å…¥éªŒè¯",
                            "priority": "high",
                        },
                    ],
                    attention_points=[
                        f"æŠ€æœ¯æ ˆ: {data.get('tech_stack', {}).get('frameworks', [])}",
                        f"ä¸»è¦è¯­è¨€: {data.get('tech_stack', {}).get('languages', [])}",
                    ],
                    priority_areas=data.get("high_risk_areas", [])[:10],
                    context_data={
                        "tech_stack": data.get("tech_stack", {}),
                        "entry_points": data.get("entry_points", []),
                        "dependencies": data.get("dependencies", {}),
                    },
                )
                
                await self.emit_event(
                    "phase_complete",
                    f"âœ… ä¿¡æ¯æ”¶é›†å®Œæˆ: å‘ç° {len(data.get('entry_points', []))} ä¸ªå…¥å£ç‚¹"
                )
                
                return {
                    "tech_stack": data.get("tech_stack", {}),
                    "entry_points": data.get("entry_points", []),
                    "high_risk_areas": data.get("high_risk_areas", []),
                    "dependencies": data.get("dependencies", {}),
                    "current_phase": "recon_complete",
                    "findings": data.get("initial_findings", []),
                    # ğŸ”¥ ä¿å­˜äº¤æ¥ä¿¡æ¯
                    "recon_handoff": handoff.to_dict(),
                    "events": [{
                        "type": "recon_complete",
                        "data": {
                            "entry_points_count": len(data.get("entry_points", [])),
                            "high_risk_areas_count": len(data.get("high_risk_areas", [])),
                            "handoff_summary": handoff.summary,
                        }
                    }],
                }
            else:
                return {
                    "error": result.error or "Recon failed",
                    "current_phase": "error",
                }
                
        except Exception as e:
            logger.error(f"Recon node failed: {e}", exc_info=True)
            return {
                "error": str(e),
                "current_phase": "error",
            }


class AnalysisNode(BaseNode):
    """
    æ¼æ´åˆ†æèŠ‚ç‚¹
    
    è¾“å…¥: tech_stack, entry_points, high_risk_areas, recon_handoff
    è¾“å‡º: findings (ç´¯åŠ ), should_continue_analysis, analysis_handoff
    """
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¼æ´åˆ†æ"""
        iteration = state.get("iteration", 0) + 1
        
        await self.emit_event(
            "phase_start",
            f"ğŸ”¬ å¼€å§‹æ¼æ´åˆ†æé˜¶æ®µ (è¿­ä»£ {iteration})"
        )
        
        try:
            # ğŸ”¥ æå– Recon çš„äº¤æ¥ä¿¡æ¯
            recon_handoff = self._extract_handoff_from_state(state, "recon")
            if recon_handoff:
                self.agent.receive_handoff(recon_handoff)
                await self.emit_event(
                    "handoff_received",
                    f"ğŸ“¨ æ”¶åˆ° Recon Agent äº¤æ¥: {recon_handoff.summary[:50]}..."
                )
            
            # æ„å»ºåˆ†æè¾“å…¥
            analysis_input = {
                "phase_name": "analysis",
                "project_info": state["project_info"],
                "config": state["config"],
                "plan": {
                    "high_risk_areas": state.get("high_risk_areas", []),
                },
                "previous_results": {
                    "recon": {
                        "data": {
                            "tech_stack": state.get("tech_stack", {}),
                            "entry_points": state.get("entry_points", []),
                            "high_risk_areas": state.get("high_risk_areas", []),
                        }
                    }
                },
                # ğŸ”¥ ä¼ é€’äº¤æ¥ä¿¡æ¯
                "handoff": recon_handoff,
            }
            
            # è°ƒç”¨ Analysis Agent
            result = await self.agent.run(analysis_input)
            
            if result.success and result.data:
                new_findings = result.data.get("findings", [])
                logger.info(f"[AnalysisNode] Agent returned {len(new_findings)} findings")
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­åˆ†æ
                should_continue = (
                    len(new_findings) >= 5 and 
                    iteration < state.get("max_iterations", 3)
                )
                
                # ğŸ”¥ åˆ›å»ºäº¤æ¥ä¿¡æ¯ç»™ Verification Agent
                # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦
                severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
                for f in new_findings:
                    if isinstance(f, dict):
                        sev = f.get("severity", "medium")
                        severity_counts[sev] = severity_counts.get(sev, 0) + 1
                
                handoff = self.agent.create_handoff(
                    to_agent="Verification",
                    summary=f"æ¼æ´åˆ†æå®Œæˆã€‚å‘ç° {len(new_findings)} ä¸ªæ½œåœ¨æ¼æ´ (Critical: {severity_counts['critical']}, High: {severity_counts['high']}, Medium: {severity_counts['medium']}, Low: {severity_counts['low']})",
                    key_findings=new_findings[:20],  # ä¼ é€’å‰20ä¸ªå‘ç°
                    suggested_actions=[
                        {
                            "type": "verify_critical",
                            "description": "ä¼˜å…ˆéªŒè¯ Critical å’Œ High çº§åˆ«çš„æ¼æ´",
                            "priority": "critical",
                        },
                        {
                            "type": "poc_generation",
                            "description": "ä¸ºç¡®è®¤çš„æ¼æ´ç”Ÿæˆ PoC",
                            "priority": "high",
                        },
                    ],
                    attention_points=[
                        f"å…± {severity_counts['critical']} ä¸ª Critical çº§åˆ«æ¼æ´éœ€è¦ç«‹å³éªŒè¯",
                        f"å…± {severity_counts['high']} ä¸ª High çº§åˆ«æ¼æ´éœ€è¦ä¼˜å…ˆéªŒè¯",
                        "æ³¨æ„æ£€æŸ¥æ˜¯å¦æœ‰è¯¯æŠ¥ï¼Œç‰¹åˆ«æ˜¯é™æ€åˆ†æå·¥å…·çš„ç»“æœ",
                    ],
                    priority_areas=[
                        f.get("file_path", "") for f in new_findings 
                        if f.get("severity") in ["critical", "high"]
                    ][:10],
                    context_data={
                        "severity_distribution": severity_counts,
                        "total_findings": len(new_findings),
                        "iteration": iteration,
                    },
                )
                
                await self.emit_event(
                    "phase_complete",
                    f"âœ… åˆ†æè¿­ä»£ {iteration} å®Œæˆ: å‘ç° {len(new_findings)} ä¸ªæ½œåœ¨æ¼æ´"
                )
                
                return {
                    "findings": new_findings,
                    "iteration": iteration,
                    "should_continue_analysis": should_continue,
                    "current_phase": "analysis_complete",
                    # ğŸ”¥ ä¿å­˜äº¤æ¥ä¿¡æ¯
                    "analysis_handoff": handoff.to_dict(),
                    "events": [{
                        "type": "analysis_iteration",
                        "data": {
                            "iteration": iteration,
                            "findings_count": len(new_findings),
                            "severity_distribution": severity_counts,
                            "handoff_summary": handoff.summary,
                        }
                    }],
                }
            else:
                return {
                    "iteration": iteration,
                    "should_continue_analysis": False,
                    "current_phase": "analysis_complete",
                }
                
        except Exception as e:
            logger.error(f"Analysis node failed: {e}", exc_info=True)
            return {
                "error": str(e),
                "should_continue_analysis": False,
                "current_phase": "error",
            }


class VerificationNode(BaseNode):
    """
    æ¼æ´éªŒè¯èŠ‚ç‚¹
    
    è¾“å…¥: findings, analysis_handoff
    è¾“å‡º: verified_findings, false_positives, verification_handoff
    """
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¼æ´éªŒè¯"""
        findings = state.get("findings", [])
        logger.info(f"[VerificationNode] Received {len(findings)} findings to verify")
        
        if not findings:
            return {
                "verified_findings": [],
                "false_positives": [],
                "current_phase": "verification_complete",
            }
        
        await self.emit_event(
            "phase_start",
            f"ğŸ” å¼€å§‹æ¼æ´éªŒè¯é˜¶æ®µ ({len(findings)} ä¸ªå¾…éªŒè¯)"
        )
        
        try:
            # ğŸ”¥ æå– Analysis çš„äº¤æ¥ä¿¡æ¯
            analysis_handoff = self._extract_handoff_from_state(state, "analysis")
            if analysis_handoff:
                self.agent.receive_handoff(analysis_handoff)
                await self.emit_event(
                    "handoff_received",
                    f"ğŸ“¨ æ”¶åˆ° Analysis Agent äº¤æ¥: {analysis_handoff.summary[:50]}..."
                )
            
            # æ„å»ºéªŒè¯è¾“å…¥
            verification_input = {
                "previous_results": {
                    "analysis": {
                        "data": {
                            "findings": findings,
                        }
                    }
                },
                "config": state["config"],
                # ğŸ”¥ ä¼ é€’äº¤æ¥ä¿¡æ¯
                "handoff": analysis_handoff,
            }
            
            # è°ƒç”¨ Verification Agent
            result = await self.agent.run(verification_input)
            
            if result.success and result.data:
                all_verified_findings = result.data.get("findings", [])
                verified = [f for f in all_verified_findings if f.get("is_verified")]
                false_pos = [f.get("id", f.get("title", "unknown")) for f in all_verified_findings
                           if f.get("verdict") == "false_positive"]

                # ğŸ”¥ CRITICAL FIX: ç”¨éªŒè¯ç»“æœæ›´æ–°åŸå§‹ findings
                # åˆ›å»º findings çš„æ›´æ–°æ˜ å°„ï¼ŒåŸºäº (file_path, line_start, vulnerability_type)
                verified_map = {}
                for vf in all_verified_findings:
                    key = (
                        vf.get("file_path", ""),
                        vf.get("line_start", 0),
                        vf.get("vulnerability_type", ""),
                    )
                    verified_map[key] = vf

                # åˆå¹¶éªŒè¯ç»“æœåˆ°åŸå§‹ findings
                updated_findings = []
                seen_keys = set()

                # é¦–å…ˆå¤„ç†åŸå§‹ findingsï¼Œç”¨éªŒè¯ç»“æœæ›´æ–°
                for f in findings:
                    if not isinstance(f, dict):
                        continue
                    key = (
                        f.get("file_path", ""),
                        f.get("line_start", 0),
                        f.get("vulnerability_type", ""),
                    )
                    if key in verified_map:
                        # ä½¿ç”¨éªŒè¯åçš„ç‰ˆæœ¬
                        updated_findings.append(verified_map[key])
                        seen_keys.add(key)
                    else:
                        # ä¿ç•™åŸå§‹ï¼ˆæœªéªŒè¯ï¼‰
                        updated_findings.append(f)
                        seen_keys.add(key)

                # æ·»åŠ éªŒè¯ç»“æœä¸­çš„æ–°å‘ç°ï¼ˆå¦‚æœæœ‰ï¼‰
                for key, vf in verified_map.items():
                    if key not in seen_keys:
                        updated_findings.append(vf)

                logger.info(f"[VerificationNode] Updated findings: {len(updated_findings)} total, {len(verified)} verified")

                # ğŸ”¥ åˆ›å»ºäº¤æ¥ä¿¡æ¯ç»™ Report èŠ‚ç‚¹
                handoff = self.agent.create_handoff(
                    to_agent="Report",
                    summary=f"æ¼æ´éªŒè¯å®Œæˆã€‚{len(verified)} ä¸ªæ¼æ´å·²ç¡®è®¤ï¼Œ{len(false_pos)} ä¸ªè¯¯æŠ¥å·²æ’é™¤ã€‚",
                    key_findings=verified,
                    suggested_actions=[
                        {
                            "type": "generate_report",
                            "description": "ç”Ÿæˆè¯¦ç»†çš„å®‰å…¨å®¡è®¡æŠ¥å‘Š",
                            "priority": "high",
                        },
                        {
                            "type": "remediation_plan",
                            "description": "ä¸ºç¡®è®¤çš„æ¼æ´åˆ¶å®šä¿®å¤è®¡åˆ’",
                            "priority": "high",
                        },
                    ],
                    attention_points=[
                        f"å…± {len(verified)} ä¸ªæ¼æ´å·²ç¡®è®¤å­˜åœ¨",
                        f"å…± {len(false_pos)} ä¸ªè¯¯æŠ¥å·²æ’é™¤",
                        "å»ºè®®æŒ‰ä¸¥é‡ç¨‹åº¦ä¼˜å…ˆä¿®å¤ Critical å’Œ High çº§åˆ«æ¼æ´",
                    ],
                    context_data={
                        "verified_count": len(verified),
                        "false_positive_count": len(false_pos),
                        "total_analyzed": len(findings),
                        "verification_rate": len(verified) / len(findings) if findings else 0,
                    },
                )

                await self.emit_event(
                    "phase_complete",
                    f"âœ… éªŒè¯å®Œæˆ: {len(verified)} å·²ç¡®è®¤, {len(false_pos)} è¯¯æŠ¥"
                )

                return {
                    # ğŸ”¥ CRITICAL: è¿”å›æ›´æ–°åçš„ findingsï¼Œè¿™ä¼šæ›¿æ¢çŠ¶æ€ä¸­çš„ findings
                    # æ³¨æ„ï¼šç”±äº LangGraph ä½¿ç”¨ operator.addï¼Œæˆ‘ä»¬éœ€è¦åœ¨ runner ä¸­å¤„ç†åˆå¹¶
                    # è¿™é‡Œæˆ‘ä»¬è¿”å› _verified_findings_update ä½œä¸ºç‰¹æ®Šå­—æ®µ
                    "_verified_findings_update": updated_findings,
                    "verified_findings": verified,
                    "false_positives": false_pos,
                    "current_phase": "verification_complete",
                    # ğŸ”¥ ä¿å­˜äº¤æ¥ä¿¡æ¯
                    "verification_handoff": handoff.to_dict(),
                    "events": [{
                        "type": "verification_complete",
                        "data": {
                            "verified_count": len(verified),
                            "false_positive_count": len(false_pos),
                            "total_findings": len(updated_findings),
                            "handoff_summary": handoff.summary,
                        }
                    }],
                }
            else:
                return {
                    "verified_findings": [],
                    "false_positives": [],
                    "current_phase": "verification_complete",
                }
                
        except Exception as e:
            logger.error(f"Verification node failed: {e}", exc_info=True)
            return {
                "error": str(e),
                "current_phase": "error",
            }


class ReportNode(BaseNode):
    """
    æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
    
    è¾“å…¥: all state
    è¾“å‡º: summary, security_score
    """
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        await self.emit_event("phase_start", "ğŸ“Š ç”Ÿæˆå®¡è®¡æŠ¥å‘Š")

        try:
            # ğŸ”¥ CRITICAL FIX: ä¼˜å…ˆä½¿ç”¨éªŒè¯åçš„ findings æ›´æ–°
            findings = state.get("_verified_findings_update") or state.get("findings", [])
            verified = state.get("verified_findings", [])
            false_positives = state.get("false_positives", [])

            logger.info(f"[ReportNode] State contains {len(findings)} findings, {len(verified)} verified")
            
            # ç»Ÿè®¡æ¼æ´åˆ†å¸ƒ
            severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
            type_counts = {}
            
            for finding in findings:
                # è·³è¿‡éå­—å…¸ç±»å‹çš„ findingï¼ˆé˜²æ­¢æ•°æ®æ ¼å¼å¼‚å¸¸ï¼‰
                if not isinstance(finding, dict):
                    logger.warning(f"Skipping invalid finding (not a dict): {type(finding)}")
                    continue
                    
                sev = finding.get("severity", "medium")
                severity_counts[sev] = severity_counts.get(sev, 0) + 1
                
                vtype = finding.get("vulnerability_type", "other")
                type_counts[vtype] = type_counts.get(vtype, 0) + 1
            
            # è®¡ç®—å®‰å…¨è¯„åˆ†
            base_score = 100
            deductions = (
                severity_counts["critical"] * 25 +
                severity_counts["high"] * 15 +
                severity_counts["medium"] * 8 +
                severity_counts["low"] * 3
            )
            security_score = max(0, base_score - deductions)
            
            # ç”Ÿæˆæ‘˜è¦
            summary = {
                "total_findings": len(findings),
                "verified_count": len(verified),
                "false_positive_count": len(false_positives),
                "severity_distribution": severity_counts,
                "vulnerability_types": type_counts,
                "tech_stack": state.get("tech_stack", {}),
                "entry_points_analyzed": len(state.get("entry_points", [])),
                "high_risk_areas": state.get("high_risk_areas", []),
                "iterations": state.get("iteration", 1),
            }
            
            await self.emit_event(
                "phase_complete",
                f"æŠ¥å‘Šç”Ÿæˆå®Œæˆ: å®‰å…¨è¯„åˆ† {security_score}/100"
            )
            
            return {
                "summary": summary,
                "security_score": security_score,
                "current_phase": "complete",
                "events": [{
                    "type": "audit_complete",
                    "data": {
                        "security_score": security_score,
                        "total_findings": len(findings),
                        "verified_count": len(verified),
                    }
                }],
            }
            
        except Exception as e:
            logger.error(f"Report node failed: {e}", exc_info=True)
            return {
                "error": str(e),
                "current_phase": "error",
            }


class HumanReviewNode(BaseNode):
    """
    äººå·¥å®¡æ ¸èŠ‚ç‚¹
    
    åœ¨æ­¤èŠ‚ç‚¹æš‚åœï¼Œç­‰å¾…äººå·¥åé¦ˆ
    """
    
    async def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        äººå·¥å®¡æ ¸èŠ‚ç‚¹
        
        è¿™ä¸ªèŠ‚ç‚¹ä¼šè¢« interrupt_before æš‚åœ
        ç”¨æˆ·å¯ä»¥ï¼š
        1. ç¡®è®¤å‘ç°
        2. æ ‡è®°è¯¯æŠ¥
        3. è¯·æ±‚é‡æ–°åˆ†æ
        """
        await self.emit_event(
            "human_review",
            f"â¸ï¸ ç­‰å¾…äººå·¥å®¡æ ¸ ({len(state.get('verified_findings', []))} ä¸ªå¾…ç¡®è®¤)"
        )
        
        # è¿”å›å½“å‰çŠ¶æ€ï¼Œä¸åšä¿®æ”¹
        # äººå·¥åé¦ˆä¼šé€šè¿‡ update_state ä¼ å…¥
        return {
            "current_phase": "human_review",
            "messages": [{
                "role": "system",
                "content": "ç­‰å¾…äººå·¥å®¡æ ¸",
                "findings_for_review": state.get("verified_findings", []),
            }],
        }

