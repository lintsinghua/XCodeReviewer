"""
DeepAudit å®¡è®¡å·¥ä½œæµå›¾ - LLM é©±åŠ¨ç‰ˆ
ä½¿ç”¨ LangGraph æ„å»º LLM é©±åŠ¨çš„ Agent åä½œæµç¨‹

é‡è¦æ”¹å˜ï¼šè·¯ç”±å†³ç­–ç”± LLM å‚ä¸ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç æ¡ä»¶ï¼
"""

from typing import TypedDict, Annotated, List, Dict, Any, Optional, Literal
from datetime import datetime
import operator
import logging
import json

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode

logger = logging.getLogger(__name__)


# ============ çŠ¶æ€å®šä¹‰ ============

class Finding(TypedDict):
    """æ¼æ´å‘ç°"""
    id: str
    vulnerability_type: str
    severity: str
    title: str
    description: str
    file_path: Optional[str]
    line_start: Optional[int]
    code_snippet: Optional[str]
    is_verified: bool
    confidence: float
    source: str


class AuditState(TypedDict):
    """
    å®¡è®¡çŠ¶æ€
    åœ¨æ•´ä¸ªå·¥ä½œæµä¸­ä¼ é€’å’Œæ›´æ–°
    """
    # è¾“å…¥
    project_root: str
    project_info: Dict[str, Any]
    config: Dict[str, Any]
    task_id: str
    
    # Recon é˜¶æ®µè¾“å‡º
    tech_stack: Dict[str, Any]
    entry_points: List[Dict[str, Any]]
    high_risk_areas: List[str]
    dependencies: Dict[str, Any]
    
    # Analysis é˜¶æ®µè¾“å‡º
    findings: Annotated[List[Finding], operator.add]  # ä½¿ç”¨ add åˆå¹¶å¤šè½®å‘ç°
    
    # Verification é˜¶æ®µè¾“å‡º
    verified_findings: List[Finding]
    false_positives: List[str]
    
    # æ§åˆ¶æµ - ğŸ”¥ å…³é”®ï¼šLLM å¯ä»¥è®¾ç½®è¿™äº›æ¥å½±å“è·¯ç”±
    current_phase: str
    iteration: int
    max_iterations: int
    should_continue_analysis: bool
    
    # ğŸ”¥ æ–°å¢ï¼šLLM çš„è·¯ç”±å†³ç­–
    llm_next_action: Optional[str]  # LLM å»ºè®®çš„ä¸‹ä¸€æ­¥: "continue_analysis", "verify", "report", "end"
    llm_routing_reason: Optional[str]  # LLM çš„å†³ç­–ç†ç”±
    
    # ğŸ”¥ æ–°å¢ï¼šAgent é—´åä½œçš„ä»»åŠ¡äº¤æ¥ä¿¡æ¯
    recon_handoff: Optional[Dict[str, Any]]        # Recon -> Analysis çš„äº¤æ¥
    analysis_handoff: Optional[Dict[str, Any]]     # Analysis -> Verification çš„äº¤æ¥
    verification_handoff: Optional[Dict[str, Any]] # Verification -> Report çš„äº¤æ¥
    
    # æ¶ˆæ¯å’Œäº‹ä»¶
    messages: Annotated[List[Dict], operator.add]
    events: Annotated[List[Dict], operator.add]
    
    # æœ€ç»ˆè¾“å‡º
    summary: Optional[Dict[str, Any]]
    security_score: Optional[int]
    error: Optional[str]


# ============ LLM è·¯ç”±å†³ç­–å™¨ ============

class LLMRouter:
    """
    LLM è·¯ç”±å†³ç­–å™¨
    è®© LLM æ¥å†³å®šä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆ
    """
    
    def __init__(self, llm_service):
        self.llm_service = llm_service
    
    async def decide_after_recon(self, state: AuditState) -> Dict[str, Any]:
        """Recon åè®© LLM å†³å®šä¸‹ä¸€æ­¥"""
        entry_points = state.get("entry_points", [])
        high_risk_areas = state.get("high_risk_areas", [])
        tech_stack = state.get("tech_stack", {})
        initial_findings = state.get("findings", [])
        
        prompt = f"""ä½œä¸ºå®‰å…¨å®¡è®¡çš„å†³ç­–è€…ï¼ŒåŸºäºä»¥ä¸‹ä¿¡æ¯æ”¶é›†ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

## ä¿¡æ¯æ”¶é›†ç»“æœ
- å…¥å£ç‚¹æ•°é‡: {len(entry_points)}
- é«˜é£é™©åŒºåŸŸ: {high_risk_areas[:10]}
- æŠ€æœ¯æ ˆ: {tech_stack}
- åˆæ­¥å‘ç°: {len(initial_findings)} ä¸ª

## é€‰é¡¹
1. "analysis" - ç»§ç»­è¿›è¡Œæ¼æ´åˆ†æï¼ˆæ¨èï¼šæœ‰å…¥å£ç‚¹æˆ–é«˜é£é™©åŒºåŸŸæ—¶ï¼‰
2. "end" - ç»“æŸå®¡è®¡ï¼ˆä»…å½“æ²¡æœ‰ä»»ä½•å¯åˆ†æå†…å®¹æ—¶ï¼‰

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{"action": "analysisæˆ–end", "reason": "å†³ç­–ç†ç”±"}}"""

        try:
            response = await self.llm_service.chat_completion_raw(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å®‰å…¨å®¡è®¡æµç¨‹çš„å†³ç­–è€…ï¼Œè´Ÿè´£å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=200,
            )
            
            content = response.get("content", "")
            # æå– JSON
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result
        except Exception as e:
            logger.warning(f"LLM routing decision failed: {e}")
        
        # é»˜è®¤å†³ç­–
        if entry_points or high_risk_areas:
            return {"action": "analysis", "reason": "æœ‰å¯åˆ†æå†…å®¹"}
        return {"action": "end", "reason": "æ²¡æœ‰å‘ç°å…¥å£ç‚¹æˆ–é«˜é£é™©åŒºåŸŸ"}
    
    async def decide_after_analysis(self, state: AuditState) -> Dict[str, Any]:
        """Analysis åè®© LLM å†³å®šä¸‹ä¸€æ­¥"""
        findings = state.get("findings", [])
        iteration = state.get("iteration", 0)
        max_iterations = state.get("max_iterations", 3)
        
        # ç»Ÿè®¡å‘ç°
        severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        for f in findings:
            # è·³è¿‡éå­—å…¸ç±»å‹çš„ finding
            if not isinstance(f, dict):
                continue
            sev = f.get("severity", "medium")
            severity_counts[sev] = severity_counts.get(sev, 0) + 1
        
        prompt = f"""ä½œä¸ºå®‰å…¨å®¡è®¡çš„å†³ç­–è€…ï¼ŒåŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

## åˆ†æç»“æœ
- æ€»å‘ç°æ•°: {len(findings)}
- ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ: {severity_counts}
- å½“å‰è¿­ä»£: {iteration}/{max_iterations}

## é€‰é¡¹
1. "verification" - éªŒè¯å‘ç°çš„æ¼æ´ï¼ˆæ¨èï¼šæœ‰å‘ç°éœ€è¦éªŒè¯æ—¶ï¼‰
2. "analysis" - ç»§ç»­æ·±å…¥åˆ†æï¼ˆæ¨èï¼šå‘ç°è¾ƒå°‘ä½†è¿˜æœ‰è¿­ä»£æ¬¡æ•°æ—¶ï¼‰
3. "report" - ç”ŸæˆæŠ¥å‘Šï¼ˆæ¨èï¼šæ²¡æœ‰å‘ç°æˆ–å·²å……åˆ†åˆ†ææ—¶ï¼‰

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{"action": "verification/analysis/report", "reason": "å†³ç­–ç†ç”±"}}"""

        try:
            response = await self.llm_service.chat_completion_raw(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å®‰å…¨å®¡è®¡æµç¨‹çš„å†³ç­–è€…ï¼Œè´Ÿè´£å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=200,
            )
            
            content = response.get("content", "")
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result
        except Exception as e:
            logger.warning(f"LLM routing decision failed: {e}")
        
        # é»˜è®¤å†³ç­–
        if not findings:
            return {"action": "report", "reason": "æ²¡æœ‰å‘ç°æ¼æ´"}
        if len(findings) >= 3 or iteration >= max_iterations:
            return {"action": "verification", "reason": "æœ‰è¶³å¤Ÿçš„å‘ç°éœ€è¦éªŒè¯"}
        return {"action": "analysis", "reason": "å‘ç°è¾ƒå°‘ï¼Œç»§ç»­åˆ†æ"}
    
    async def decide_after_verification(self, state: AuditState) -> Dict[str, Any]:
        """Verification åè®© LLM å†³å®šä¸‹ä¸€æ­¥"""
        verified_findings = state.get("verified_findings", [])
        false_positives = state.get("false_positives", [])
        iteration = state.get("iteration", 0)
        max_iterations = state.get("max_iterations", 3)
        
        prompt = f"""ä½œä¸ºå®‰å…¨å®¡è®¡çš„å†³ç­–è€…ï¼ŒåŸºäºä»¥ä¸‹éªŒè¯ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚

## éªŒè¯ç»“æœ
- å·²ç¡®è®¤æ¼æ´: {len(verified_findings)}
- è¯¯æŠ¥æ•°é‡: {len(false_positives)}
- å½“å‰è¿­ä»£: {iteration}/{max_iterations}

## é€‰é¡¹
1. "analysis" - å›åˆ°åˆ†æé˜¶æ®µé‡æ–°åˆ†æï¼ˆæ¨èï¼šè¯¯æŠ¥ç‡å¤ªé«˜æ—¶ï¼‰
2. "report" - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆæ¨èï¼šéªŒè¯å®Œæˆæ—¶ï¼‰

è¯·è¿”å› JSON æ ¼å¼ï¼š
{{"action": "analysis/report", "reason": "å†³ç­–ç†ç”±"}}"""

        try:
            response = await self.llm_service.chat_completion_raw(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å®‰å…¨å®¡è®¡æµç¨‹çš„å†³ç­–è€…ï¼Œè´Ÿè´£å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚"},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=200,
            )
            
            content = response.get("content", "")
            import re
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result
        except Exception as e:
            logger.warning(f"LLM routing decision failed: {e}")
        
        # é»˜è®¤å†³ç­–
        if len(false_positives) > len(verified_findings) and iteration < max_iterations:
            return {"action": "analysis", "reason": "è¯¯æŠ¥ç‡è¾ƒé«˜ï¼Œéœ€è¦é‡æ–°åˆ†æ"}
        return {"action": "report", "reason": "éªŒè¯å®Œæˆï¼Œç”ŸæˆæŠ¥å‘Š"}


# ============ è·¯ç”±å‡½æ•° (ç»“åˆ LLM å†³ç­–) ============

def route_after_recon(state: AuditState) -> Literal["analysis", "end"]:
    """
    Recon åçš„è·¯ç”±å†³ç­–
    ä¼˜å…ˆä½¿ç”¨ LLM çš„å†³ç­–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤é€»è¾‘
    """
    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if state.get("error") or state.get("current_phase") == "error":
        logger.error(f"Recon phase has error, routing to end: {state.get('error')}")
        return "end"
    
    # æ£€æŸ¥ LLM æ˜¯å¦æœ‰å†³ç­–
    llm_action = state.get("llm_next_action")
    if llm_action:
        logger.info(f"Using LLM routing decision: {llm_action}, reason: {state.get('llm_routing_reason')}")
        if llm_action == "end":
            return "end"
        return "analysis"
    
    # é»˜è®¤é€»è¾‘ï¼ˆä½œä¸º fallbackï¼‰
    if not state.get("entry_points") and not state.get("high_risk_areas"):
        return "end"
    return "analysis"


def route_after_analysis(state: AuditState) -> Literal["verification", "analysis", "report"]:
    """
    Analysis åçš„è·¯ç”±å†³ç­–
    ä¼˜å…ˆä½¿ç”¨ LLM çš„å†³ç­–
    """
    # æ£€æŸ¥ LLM æ˜¯å¦æœ‰å†³ç­–
    llm_action = state.get("llm_next_action")
    if llm_action:
        logger.info(f"Using LLM routing decision: {llm_action}, reason: {state.get('llm_routing_reason')}")
        if llm_action == "verification":
            return "verification"
        elif llm_action == "analysis":
            return "analysis"
        elif llm_action == "report":
            return "report"
    
    # é»˜è®¤é€»è¾‘
    findings = state.get("findings", [])
    iteration = state.get("iteration", 0)
    max_iterations = state.get("max_iterations", 3)
    should_continue = state.get("should_continue_analysis", False)
    
    if not findings:
        return "report"
    
    if should_continue and iteration < max_iterations:
        return "analysis"
    
    return "verification"


def route_after_verification(state: AuditState) -> Literal["analysis", "report"]:
    """
    Verification åçš„è·¯ç”±å†³ç­–
    ä¼˜å…ˆä½¿ç”¨ LLM çš„å†³ç­–
    """
    # æ£€æŸ¥ LLM æ˜¯å¦æœ‰å†³ç­–
    llm_action = state.get("llm_next_action")
    if llm_action:
        logger.info(f"Using LLM routing decision: {llm_action}, reason: {state.get('llm_routing_reason')}")
        if llm_action == "analysis":
            return "analysis"
        return "report"
    
    # é»˜è®¤é€»è¾‘
    false_positives = state.get("false_positives", [])
    iteration = state.get("iteration", 0)
    max_iterations = state.get("max_iterations", 3)
    
    if len(false_positives) > len(state.get("verified_findings", [])) and iteration < max_iterations:
        return "analysis"
    
    return "report"


# ============ åˆ›å»ºå®¡è®¡å›¾ ============

def create_audit_graph(
    recon_node,
    analysis_node,
    verification_node,
    report_node,
    checkpointer: Optional[MemorySaver] = None,
    llm_service=None,  # ç”¨äº LLM è·¯ç”±å†³ç­–
) -> StateGraph:
    """
    åˆ›å»ºå®¡è®¡å·¥ä½œæµå›¾
    
    Args:
        recon_node: ä¿¡æ¯æ”¶é›†èŠ‚ç‚¹
        analysis_node: æ¼æ´åˆ†æèŠ‚ç‚¹
        verification_node: æ¼æ´éªŒè¯èŠ‚ç‚¹
        report_node: æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
        checkpointer: æ£€æŸ¥ç‚¹å­˜å‚¨å™¨
        llm_service: LLM æœåŠ¡ï¼ˆç”¨äºè·¯ç”±å†³ç­–ï¼‰
    
    Returns:
        ç¼–è¯‘åçš„ StateGraph
    
    å·¥ä½œæµç»“æ„:
    
        START
          â”‚
          â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”
       â”‚Recon â”‚  ä¿¡æ¯æ”¶é›† (LLM é©±åŠ¨)
       â””â”€â”€â”¬â”€â”€â”€â”˜
          â”‚ LLM å†³å®š
          â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Analysis â”‚â—„â”€â”€â”€â”€â”€â”  æ¼æ´åˆ†æ (LLM é©±åŠ¨ï¼Œå¯å¾ªç¯)
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”‚
            â”‚ LLM å†³å®š   â”‚
            â–¼            â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
       â”‚Verificationâ”‚â”€â”€â”€â”€â”˜  æ¼æ´éªŒè¯ (LLM é©±åŠ¨ï¼Œå¯å›æº¯)
       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â”‚ LLM å†³å®š
             â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Report  â”‚  æŠ¥å‘Šç”Ÿæˆ
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
           END
    """
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AuditState)
    
    # å¦‚æœæœ‰ LLM æœåŠ¡ï¼Œåˆ›å»ºè·¯ç”±å†³ç­–å™¨
    llm_router = LLMRouter(llm_service) if llm_service else None
    
    # åŒ…è£…èŠ‚ç‚¹ä»¥æ·»åŠ  LLM è·¯ç”±å†³ç­–
    async def recon_with_routing(state):
        result = await recon_node(state)
        
        # LLM å†³å®šä¸‹ä¸€æ­¥
        if llm_router:
            decision = await llm_router.decide_after_recon({**state, **result})
            result["llm_next_action"] = decision.get("action")
            result["llm_routing_reason"] = decision.get("reason")
        
        return result
    
    async def analysis_with_routing(state):
        result = await analysis_node(state)
        
        # LLM å†³å®šä¸‹ä¸€æ­¥
        if llm_router:
            decision = await llm_router.decide_after_analysis({**state, **result})
            result["llm_next_action"] = decision.get("action")
            result["llm_routing_reason"] = decision.get("reason")
        
        return result
    
    async def verification_with_routing(state):
        result = await verification_node(state)
        
        # LLM å†³å®šä¸‹ä¸€æ­¥
        if llm_router:
            decision = await llm_router.decide_after_verification({**state, **result})
            result["llm_next_action"] = decision.get("action")
            result["llm_routing_reason"] = decision.get("reason")
        
        return result
    
    # æ·»åŠ èŠ‚ç‚¹
    if llm_router:
        workflow.add_node("recon", recon_with_routing)
        workflow.add_node("analysis", analysis_with_routing)
        workflow.add_node("verification", verification_with_routing)
    else:
        workflow.add_node("recon", recon_node)
        workflow.add_node("analysis", analysis_node)
        workflow.add_node("verification", verification_node)
    
    workflow.add_node("report", report_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("recon")
    
    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "recon",
        route_after_recon,
        {
            "analysis": "analysis",
            "end": END,
        }
    )
    
    workflow.add_conditional_edges(
        "analysis",
        route_after_analysis,
        {
            "verification": "verification",
            "analysis": "analysis",
            "report": "report",
        }
    )
    
    workflow.add_conditional_edges(
        "verification",
        route_after_verification,
        {
            "analysis": "analysis",
            "report": "report",
        }
    )
    
    # Report -> END
    workflow.add_edge("report", END)
    
    # ç¼–è¯‘å›¾
    if checkpointer:
        return workflow.compile(checkpointer=checkpointer)
    else:
        return workflow.compile()


# ============ å¸¦äººæœºåä½œçš„å®¡è®¡å›¾ ============

def create_audit_graph_with_human(
    recon_node,
    analysis_node,
    verification_node,
    report_node,
    human_review_node,
    checkpointer: Optional[MemorySaver] = None,
    llm_service=None,
) -> StateGraph:
    """
    åˆ›å»ºå¸¦äººæœºåä½œçš„å®¡è®¡å·¥ä½œæµå›¾
    
    åœ¨éªŒè¯é˜¶æ®µåå¢åŠ äººå·¥å®¡æ ¸èŠ‚ç‚¹
    """
    
    workflow = StateGraph(AuditState)
    llm_router = LLMRouter(llm_service) if llm_service else None
    
    # åŒ…è£…èŠ‚ç‚¹
    async def recon_with_routing(state):
        result = await recon_node(state)
        if llm_router:
            decision = await llm_router.decide_after_recon({**state, **result})
            result["llm_next_action"] = decision.get("action")
            result["llm_routing_reason"] = decision.get("reason")
        return result
    
    async def analysis_with_routing(state):
        result = await analysis_node(state)
        if llm_router:
            decision = await llm_router.decide_after_analysis({**state, **result})
            result["llm_next_action"] = decision.get("action")
            result["llm_routing_reason"] = decision.get("reason")
        return result
    
    # æ·»åŠ èŠ‚ç‚¹
    if llm_router:
        workflow.add_node("recon", recon_with_routing)
        workflow.add_node("analysis", analysis_with_routing)
    else:
        workflow.add_node("recon", recon_node)
        workflow.add_node("analysis", analysis_node)
    
    workflow.add_node("verification", verification_node)
    workflow.add_node("human_review", human_review_node)
    workflow.add_node("report", report_node)
    
    workflow.set_entry_point("recon")
    
    workflow.add_conditional_edges(
        "recon",
        route_after_recon,
        {"analysis": "analysis", "end": END}
    )
    
    workflow.add_conditional_edges(
        "analysis",
        route_after_analysis,
        {
            "verification": "verification",
            "analysis": "analysis",
            "report": "report",
        }
    )
    
    # Verification -> Human Review
    workflow.add_edge("verification", "human_review")
    
    # Human Review åçš„è·¯ç”±
    def route_after_human(state: AuditState) -> Literal["analysis", "report"]:
        if state.get("should_continue_analysis"):
            return "analysis"
        return "report"
    
    workflow.add_conditional_edges(
        "human_review",
        route_after_human,
        {"analysis": "analysis", "report": "report"}
    )
    
    workflow.add_edge("report", END)
    
    if checkpointer:
        return workflow.compile(checkpointer=checkpointer, interrupt_before=["human_review"])
    else:
        return workflow.compile()


# ============ æ‰§è¡Œå™¨ ============

class AuditGraphRunner:
    """
    å®¡è®¡å›¾æ‰§è¡Œå™¨
    å°è£… LangGraph å·¥ä½œæµçš„æ‰§è¡Œ
    """
    
    def __init__(
        self,
        graph: StateGraph,
        event_emitter=None,
    ):
        self.graph = graph
        self.event_emitter = event_emitter
    
    async def run(
        self,
        project_root: str,
        project_info: Dict[str, Any],
        config: Dict[str, Any],
        task_id: str,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®¡è®¡å·¥ä½œæµ
        """
        # åˆå§‹çŠ¶æ€
        initial_state: AuditState = {
            "project_root": project_root,
            "project_info": project_info,
            "config": config,
            "task_id": task_id,
            "tech_stack": {},
            "entry_points": [],
            "high_risk_areas": [],
            "dependencies": {},
            "findings": [],
            "verified_findings": [],
            "false_positives": [],
            "current_phase": "start",
            "iteration": 0,
            "max_iterations": config.get("max_iterations", 3),
            "should_continue_analysis": False,
            "llm_next_action": None,
            "llm_routing_reason": None,
            "messages": [],
            "events": [],
            "summary": None,
            "security_score": None,
            "error": None,
        }
        
        run_config = {
            "configurable": {
                "thread_id": task_id,
            }
        }
        
        try:
            async for event in self.graph.astream(initial_state, config=run_config):
                if self.event_emitter:
                    for node_name, node_state in event.items():
                        await self.event_emitter.emit_info(
                            f"èŠ‚ç‚¹ {node_name} å®Œæˆ"
                        )
                        
                        # å‘å°„ LLM è·¯ç”±å†³ç­–äº‹ä»¶
                        if node_state.get("llm_routing_reason"):
                            await self.event_emitter.emit_info(
                                f"ğŸ§  LLM å†³ç­–: {node_state.get('llm_next_action')} - {node_state.get('llm_routing_reason')}"
                            )
                        
                        if node_name == "analysis" and node_state.get("findings"):
                            new_findings = node_state["findings"]
                            await self.event_emitter.emit_info(
                                f"å‘ç° {len(new_findings)} ä¸ªæ½œåœ¨æ¼æ´"
                            )
            
            final_state = self.graph.get_state(run_config)
            return final_state.values
            
        except Exception as e:
            logger.error(f"Graph execution failed: {e}", exc_info=True)
            raise
    
    async def run_with_human_review(
        self,
        initial_state: AuditState,
        human_feedback_callback,
    ) -> Dict[str, Any]:
        """å¸¦äººæœºåä½œçš„æ‰§è¡Œ"""
        run_config = {
            "configurable": {
                "thread_id": initial_state["task_id"],
            }
        }
        
        async for event in self.graph.astream(initial_state, config=run_config):
            pass
        
        current_state = self.graph.get_state(run_config)
        
        if current_state.next == ("human_review",):
            human_decision = await human_feedback_callback(current_state.values)
            
            updated_state = {
                **current_state.values,
                "should_continue_analysis": human_decision.get("continue_analysis", False),
            }
            
            async for event in self.graph.astream(updated_state, config=run_config):
                pass
        
        return self.graph.get_state(run_config).values
