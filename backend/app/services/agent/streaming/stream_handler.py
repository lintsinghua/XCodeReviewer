"""
æµå¼äº‹ä»¶å¤„ç†å™¨
å¤„ç† LangGraph çš„å„ç§æµå¼äº‹ä»¶å¹¶è½¬æ¢ä¸ºå‰ç«¯å¯æ¶ˆè´¹çš„æ ¼å¼
"""

import json
import logging
from enum import Enum
from typing import Any, Dict, Optional, AsyncGenerator, List
from dataclasses import dataclass, field
from datetime import datetime, timezone

logger = logging.getLogger(__name__)


class StreamEventType(str, Enum):
    """æµå¼äº‹ä»¶ç±»å‹"""
    # ğŸ”¥ LLM æ€è€ƒç›¸å…³ - è¿™äº›æ˜¯æœ€é‡è¦çš„ï¼å±•ç¤º LLM çš„å¤§è„‘æ´»åŠ¨
    LLM_START = "llm_start"                # LLM å¼€å§‹æ€è€ƒ
    LLM_THOUGHT = "llm_thought"            # LLM æ€è€ƒå†…å®¹ â­ æ ¸å¿ƒ
    LLM_DECISION = "llm_decision"          # LLM å†³ç­– â­ æ ¸å¿ƒ
    LLM_ACTION = "llm_action"              # LLM åŠ¨ä½œ
    LLM_OBSERVATION = "llm_observation"    # LLM è§‚å¯Ÿç»“æœ
    LLM_COMPLETE = "llm_complete"          # LLM å®Œæˆ
    
    # LLM Token æµ (å®æ—¶è¾“å‡º)
    THINKING_START = "thinking_start"      # å¼€å§‹æ€è€ƒ
    THINKING_TOKEN = "thinking_token"      # æ€è€ƒ Token (æµå¼)
    THINKING_END = "thinking_end"          # æ€è€ƒç»“æŸ
    
    # å·¥å…·è°ƒç”¨ç›¸å…³ - LLM å†³å®šè°ƒç”¨å·¥å…·
    TOOL_CALL_START = "tool_call_start"    # å·¥å…·è°ƒç”¨å¼€å§‹
    TOOL_CALL_INPUT = "tool_call_input"    # å·¥å…·è¾“å…¥å‚æ•°
    TOOL_CALL_OUTPUT = "tool_call_output"  # å·¥å…·è¾“å‡ºç»“æœ
    TOOL_CALL_END = "tool_call_end"        # å·¥å…·è°ƒç”¨ç»“æŸ
    TOOL_CALL_ERROR = "tool_call_error"    # å·¥å…·è°ƒç”¨é”™è¯¯
    
    # èŠ‚ç‚¹ç›¸å…³
    NODE_START = "node_start"              # èŠ‚ç‚¹å¼€å§‹
    NODE_END = "node_end"                  # èŠ‚ç‚¹ç»“æŸ
    
    # é˜¶æ®µç›¸å…³
    PHASE_START = "phase_start"
    PHASE_END = "phase_end"
    
    # å‘ç°ç›¸å…³
    FINDING_NEW = "finding_new"            # æ–°å‘ç°
    FINDING_VERIFIED = "finding_verified"  # éªŒè¯é€šè¿‡
    
    # çŠ¶æ€ç›¸å…³
    PROGRESS = "progress"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    
    # ä»»åŠ¡ç›¸å…³
    TASK_START = "task_start"
    TASK_COMPLETE = "task_complete"
    TASK_ERROR = "task_error"
    TASK_CANCEL = "task_cancel"
    
    # å¿ƒè·³
    HEARTBEAT = "heartbeat"


@dataclass
class StreamEvent:
    """æµå¼äº‹ä»¶"""
    event_type: StreamEventType
    data: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    sequence: int = 0
    
    # å¯é€‰å­—æ®µ
    node_name: Optional[str] = None
    phase: Optional[str] = None
    tool_name: Optional[str] = None
    
    def to_sse(self) -> str:
        """è½¬æ¢ä¸º SSE æ ¼å¼"""
        event_data = {
            "type": self.event_type.value,
            "data": self.data,
            "timestamp": self.timestamp,
            "sequence": self.sequence,
        }
        
        if self.node_name:
            event_data["node"] = self.node_name
        if self.phase:
            event_data["phase"] = self.phase
        if self.tool_name:
            event_data["tool"] = self.tool_name
        
        return f"event: {self.event_type.value}\ndata: {json.dumps(event_data, ensure_ascii=False)}\n\n"
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "event_type": self.event_type.value,
            "data": self.data,
            "timestamp": self.timestamp,
            "sequence": self.sequence,
            "node_name": self.node_name,
            "phase": self.phase,
            "tool_name": self.tool_name,
        }


class StreamHandler:
    """
    æµå¼äº‹ä»¶å¤„ç†å™¨
    
    æœ€ä½³å®è·µ:
    1. ä½¿ç”¨ astream_events æ•è·æ‰€æœ‰ LangGraph äº‹ä»¶
    2. å°†å†…éƒ¨äº‹ä»¶è½¬æ¢ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼
    3. æ”¯æŒå¤šç§äº‹ä»¶ç±»å‹çš„åˆ†å‘
    """
    
    def __init__(self, task_id: str):
        self.task_id = task_id
        self._sequence = 0
        self._current_phase = None
        self._current_node = None
        self._thinking_buffer = []
        self._tool_states: Dict[str, Dict] = {}
    
    def _next_sequence(self) -> int:
        """è·å–ä¸‹ä¸€ä¸ªåºåˆ—å·"""
        self._sequence += 1
        return self._sequence
    
    async def process_langgraph_event(self, event: Dict[str, Any]) -> Optional[StreamEvent]:
        """
        å¤„ç† LangGraph äº‹ä»¶
        
        æ”¯æŒçš„äº‹ä»¶ç±»å‹:
        - on_chain_start: é“¾/èŠ‚ç‚¹å¼€å§‹
        - on_chain_end: é“¾/èŠ‚ç‚¹ç»“æŸ
        - on_chain_stream: LLM Token æµ
        - on_chat_model_start: æ¨¡å‹å¼€å§‹
        - on_chat_model_stream: æ¨¡å‹ Token æµ
        - on_chat_model_end: æ¨¡å‹ç»“æŸ
        - on_tool_start: å·¥å…·å¼€å§‹
        - on_tool_end: å·¥å…·ç»“æŸ
        - on_custom_event: è‡ªå®šä¹‰äº‹ä»¶
        """
        event_kind = event.get("event", "")
        event_name = event.get("name", "")
        event_data = event.get("data", {})
        
        # LLM Token æµ
        if event_kind == "on_chat_model_stream":
            return await self._handle_llm_stream(event_data, event_name)
        
        # LLM å¼€å§‹
        elif event_kind == "on_chat_model_start":
            return await self._handle_llm_start(event_data, event_name)
        
        # LLM ç»“æŸ
        elif event_kind == "on_chat_model_end":
            return await self._handle_llm_end(event_data, event_name)
        
        # å·¥å…·å¼€å§‹
        elif event_kind == "on_tool_start":
            return await self._handle_tool_start(event_name, event_data)
        
        # å·¥å…·ç»“æŸ
        elif event_kind == "on_tool_end":
            return await self._handle_tool_end(event_name, event_data)
        
        # èŠ‚ç‚¹å¼€å§‹
        elif event_kind == "on_chain_start" and self._is_node_event(event_name):
            return await self._handle_node_start(event_name, event_data)
        
        # èŠ‚ç‚¹ç»“æŸ
        elif event_kind == "on_chain_end" and self._is_node_event(event_name):
            return await self._handle_node_end(event_name, event_data)
        
        # è‡ªå®šä¹‰äº‹ä»¶
        elif event_kind == "on_custom_event":
            return await self._handle_custom_event(event_name, event_data)
        
        return None
    
    def _is_node_event(self, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯èŠ‚ç‚¹äº‹ä»¶"""
        node_names = ["recon", "analysis", "verification", "report", "ReconNode", "AnalysisNode", "VerificationNode", "ReportNode"]
        return any(n.lower() in name.lower() for n in node_names)
    
    async def _handle_llm_start(self, data: Dict, name: str) -> StreamEvent:
        """å¤„ç† LLM å¼€å§‹äº‹ä»¶"""
        self._thinking_buffer = []
        
        return StreamEvent(
            event_type=StreamEventType.THINKING_START,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            data={
                "model": name,
                "message": "ğŸ¤” æ­£åœ¨æ€è€ƒ...",
            },
        )
    
    async def _handle_llm_stream(self, data: Dict, name: str) -> Optional[StreamEvent]:
        """å¤„ç† LLM Token æµäº‹ä»¶"""
        chunk = data.get("chunk")
        if not chunk:
            return None
        
        # æå– Token å†…å®¹
        content = ""
        if hasattr(chunk, "content"):
            content = chunk.content
        elif isinstance(chunk, dict):
            content = chunk.get("content", "")
        
        if not content:
            return None
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self._thinking_buffer.append(content)
        
        return StreamEvent(
            event_type=StreamEventType.THINKING_TOKEN,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            data={
                "token": content,
                "accumulated": "".join(self._thinking_buffer),
            },
        )
    
    async def _handle_llm_end(self, data: Dict, name: str) -> StreamEvent:
        """å¤„ç† LLM ç»“æŸäº‹ä»¶"""
        full_response = "".join(self._thinking_buffer)
        self._thinking_buffer = []
        
        # æå–ä½¿ç”¨çš„ Token æ•°
        usage = {}
        output = data.get("output")
        if output and hasattr(output, "usage_metadata"):
            usage = {
                "input_tokens": getattr(output.usage_metadata, "input_tokens", 0),
                "output_tokens": getattr(output.usage_metadata, "output_tokens", 0),
            }
        
        return StreamEvent(
            event_type=StreamEventType.THINKING_END,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            data={
                "response": full_response[:2000],  # æˆªæ–­é•¿å“åº”
                "usage": usage,
                "message": "ğŸ’¡ æ€è€ƒå®Œæˆ",
            },
        )
    
    async def _handle_tool_start(self, tool_name: str, data: Dict) -> StreamEvent:
        """å¤„ç†å·¥å…·å¼€å§‹äº‹ä»¶"""
        import time
        
        tool_input = data.get("input", {})
        
        # è®°å½•å·¥å…·çŠ¶æ€
        self._tool_states[tool_name] = {
            "start_time": time.time(),
            "input": tool_input,
        }
        
        return StreamEvent(
            event_type=StreamEventType.TOOL_CALL_START,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            tool_name=tool_name,
            data={
                "tool_name": tool_name,
                "input": self._truncate_data(tool_input),
                "message": f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}",
            },
        )
    
    async def _handle_tool_end(self, tool_name: str, data: Dict) -> StreamEvent:
        """å¤„ç†å·¥å…·ç»“æŸäº‹ä»¶"""
        import time
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        duration_ms = 0
        if tool_name in self._tool_states:
            start_time = self._tool_states[tool_name].get("start_time", time.time())
            duration_ms = int((time.time() - start_time) * 1000)
            del self._tool_states[tool_name]
        
        # æå–è¾“å‡º
        output = data.get("output", "")
        if hasattr(output, "content"):
            output = output.content
        
        return StreamEvent(
            event_type=StreamEventType.TOOL_CALL_END,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            tool_name=tool_name,
            data={
                "tool_name": tool_name,
                "output": self._truncate_data(output),
                "duration_ms": duration_ms,
                "message": f"âœ… å·¥å…· {tool_name} å®Œæˆ ({duration_ms}ms)",
            },
        )
    
    async def _handle_node_start(self, node_name: str, data: Dict) -> StreamEvent:
        """å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶"""
        self._current_node = node_name
        
        # æ˜ å°„èŠ‚ç‚¹åˆ°é˜¶æ®µ
        phase_map = {
            "recon": "reconnaissance",
            "analysis": "analysis",
            "verification": "verification",
            "report": "reporting",
        }
        
        for key, phase in phase_map.items():
            if key in node_name.lower():
                self._current_phase = phase
                break
        
        return StreamEvent(
            event_type=StreamEventType.NODE_START,
            sequence=self._next_sequence(),
            node_name=node_name,
            phase=self._current_phase,
            data={
                "node": node_name,
                "phase": self._current_phase,
                "message": f"â–¶ï¸ å¼€å§‹èŠ‚ç‚¹: {node_name}",
            },
        )
    
    async def _handle_node_end(self, node_name: str, data: Dict) -> StreamEvent:
        """å¤„ç†èŠ‚ç‚¹ç»“æŸäº‹ä»¶"""
        # æå–è¾“å‡ºä¿¡æ¯
        output = data.get("output", {})
        
        summary = {}
        if isinstance(output, dict):
            # æå–å…³é”®ä¿¡æ¯
            if "findings" in output:
                summary["findings_count"] = len(output["findings"])
            if "entry_points" in output:
                summary["entry_points_count"] = len(output["entry_points"])
            if "high_risk_areas" in output:
                summary["high_risk_areas_count"] = len(output["high_risk_areas"])
            if "verified_findings" in output:
                summary["verified_count"] = len(output["verified_findings"])
        
        return StreamEvent(
            event_type=StreamEventType.NODE_END,
            sequence=self._next_sequence(),
            node_name=node_name,
            phase=self._current_phase,
            data={
                "node": node_name,
                "phase": self._current_phase,
                "summary": summary,
                "message": f"â¹ï¸ èŠ‚ç‚¹å®Œæˆ: {node_name}",
            },
        )
    
    async def _handle_custom_event(self, event_name: str, data: Dict) -> StreamEvent:
        """å¤„ç†è‡ªå®šä¹‰äº‹ä»¶"""
        # æ˜ å°„è‡ªå®šä¹‰äº‹ä»¶ååˆ°äº‹ä»¶ç±»å‹
        event_type_map = {
            "finding": StreamEventType.FINDING_NEW,
            "finding_verified": StreamEventType.FINDING_VERIFIED,
            "progress": StreamEventType.PROGRESS,
            "warning": StreamEventType.WARNING,
            "error": StreamEventType.ERROR,
        }
        
        event_type = event_type_map.get(event_name, StreamEventType.INFO)
        
        return StreamEvent(
            event_type=event_type,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            data=data,
        )
    
    def _truncate_data(self, data: Any, max_length: int = 1000) -> Any:
        """æˆªæ–­æ•°æ®"""
        if isinstance(data, str):
            return data[:max_length] + "..." if len(data) > max_length else data
        elif isinstance(data, dict):
            return {k: self._truncate_data(v, max_length // 2) for k, v in list(data.items())[:10]}
        elif isinstance(data, list):
            return [self._truncate_data(item, max_length // len(data)) for item in data[:10]]
        else:
            return str(data)[:max_length]
    
    def create_progress_event(
        self,
        current: int,
        total: int,
        message: Optional[str] = None,
    ) -> StreamEvent:
        """åˆ›å»ºè¿›åº¦äº‹ä»¶"""
        percentage = (current / total * 100) if total > 0 else 0
        
        return StreamEvent(
            event_type=StreamEventType.PROGRESS,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            data={
                "current": current,
                "total": total,
                "percentage": round(percentage, 1),
                "message": message or f"è¿›åº¦: {current}/{total}",
            },
        )
    
    def create_finding_event(
        self,
        finding: Dict[str, Any],
        is_verified: bool = False,
    ) -> StreamEvent:
        """åˆ›å»ºå‘ç°äº‹ä»¶"""
        event_type = StreamEventType.FINDING_VERIFIED if is_verified else StreamEventType.FINDING_NEW
        
        return StreamEvent(
            event_type=event_type,
            sequence=self._next_sequence(),
            node_name=self._current_node,
            phase=self._current_phase,
            data={
                "title": finding.get("title", "Unknown"),
                "severity": finding.get("severity", "medium"),
                "vulnerability_type": finding.get("vulnerability_type", "other"),
                "file_path": finding.get("file_path"),
                "line_start": finding.get("line_start"),
                "is_verified": is_verified,
                "message": f"{'âœ… å·²éªŒè¯' if is_verified else 'ğŸ” æ–°å‘ç°'}: [{finding.get('severity', 'medium').upper()}] {finding.get('title', 'Unknown')}",
            },
        )
    
    def create_heartbeat(self) -> StreamEvent:
        """åˆ›å»ºå¿ƒè·³äº‹ä»¶"""
        return StreamEvent(
            event_type=StreamEventType.HEARTBEAT,
            sequence=self._sequence,  # å¿ƒè·³ä¸å¢åŠ åºåˆ—å·
            data={"message": "ping"},
        )

