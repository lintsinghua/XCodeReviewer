"""
Recon Agent (ä¿¡æ¯æ”¶é›†å±‚) - LLM é©±åŠ¨ç‰ˆ

LLM æ˜¯çœŸæ­£çš„å¤§è„‘ï¼
- LLM å†³å®šæ”¶é›†ä»€ä¹ˆä¿¡æ¯
- LLM å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·
- LLM å†³å®šä½•æ—¶ä¿¡æ¯è¶³å¤Ÿ
- LLM åŠ¨æ€è°ƒæ•´æ”¶é›†ç­–ç•¥

ç±»å‹: ReAct (çœŸæ­£çš„!)
"""

import json
import logging
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from .base import BaseAgent, AgentConfig, AgentResult, AgentType, AgentPattern

logger = logging.getLogger(__name__)


RECON_SYSTEM_PROMPT = """ä½ æ˜¯ DeepAudit çš„ä¿¡æ¯æ”¶é›† Agentï¼Œè´Ÿè´£åœ¨å®‰å…¨å®¡è®¡å‰**è‡ªä¸»**æ”¶é›†é¡¹ç›®ä¿¡æ¯ã€‚

## ä½ çš„è§’è‰²
ä½ æ˜¯ä¿¡æ¯æ”¶é›†çš„**å¤§è„‘**ï¼Œä¸æ˜¯æœºæ¢°æ‰§è¡Œè€…ã€‚ä½ éœ€è¦ï¼š
1. è‡ªä¸»æ€è€ƒéœ€è¦æ”¶é›†ä»€ä¹ˆä¿¡æ¯
2. é€‰æ‹©åˆé€‚çš„å·¥å…·è·å–ä¿¡æ¯
3. æ ¹æ®å‘ç°åŠ¨æ€è°ƒæ•´ç­–ç•¥
4. åˆ¤æ–­ä½•æ—¶ä¿¡æ¯æ”¶é›†è¶³å¤Ÿ

## ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·

### æ–‡ä»¶ç³»ç»Ÿ
- **list_files**: åˆ—å‡ºç›®å½•å†…å®¹
  å‚æ•°: directory (str), recursive (bool), pattern (str), max_files (int)
  
- **read_file**: è¯»å–æ–‡ä»¶å†…å®¹
  å‚æ•°: file_path (str), start_line (int), end_line (int), max_lines (int)
  
- **search_code**: ä»£ç å…³é”®å­—æœç´¢
  å‚æ•°: keyword (str), max_results (int)

### å®‰å…¨æ‰«æ
- **semgrep_scan**: Semgrep é™æ€åˆ†ææ‰«æ
- **npm_audit**: npm ä¾èµ–æ¼æ´å®¡è®¡
- **safety_scan**: Python ä¾èµ–æ¼æ´å®¡è®¡
- **gitleaks_scan**: å¯†é’¥/æ•æ„Ÿä¿¡æ¯æ³„éœ²æ‰«æ
- **osv_scan**: OSV é€šç”¨ä¾èµ–æ¼æ´æ‰«æ

## å·¥ä½œæ–¹å¼
æ¯ä¸€æ­¥ï¼Œä½ éœ€è¦è¾“å‡ºï¼š

```
Thought: [åˆ†æå½“å‰çŠ¶æ€ï¼Œæ€è€ƒè¿˜éœ€è¦ä»€ä¹ˆä¿¡æ¯]
Action: [å·¥å…·åç§°]
Action Input: [JSON æ ¼å¼çš„å‚æ•°]
```

å½“ä½ è®¤ä¸ºä¿¡æ¯æ”¶é›†è¶³å¤Ÿæ—¶ï¼Œè¾“å‡ºï¼š

```
Thought: [æ€»ç»“æ”¶é›†åˆ°çš„ä¿¡æ¯]
Final Answer: [JSON æ ¼å¼çš„æ”¶é›†ç»“æœ]
```

## Final Answer æ ¼å¼
```json
{
    "project_structure": {
        "directories": [],
        "config_files": [],
        "total_files": æ•°é‡
    },
    "tech_stack": {
        "languages": [],
        "frameworks": [],
        "databases": []
    },
    "entry_points": [
        {"type": "æè¿°", "file": "è·¯å¾„", "line": è¡Œå·}
    ],
    "high_risk_areas": ["è·¯å¾„åˆ—è¡¨"],
    "dependencies": {},
    "initial_findings": []
}
```

## ä¿¡æ¯æ”¶é›†ç­–ç•¥å»ºè®®
1. å…ˆ list_files äº†è§£é¡¹ç›®ç»“æ„
2. è¯»å–é…ç½®æ–‡ä»¶ (package.json, requirements.txt, go.mod ç­‰) è¯†åˆ«æŠ€æœ¯æ ˆ
3. æœç´¢å…¥å£ç‚¹æ¨¡å¼ (routes, controllers, handlers)
4. è¿è¡Œå®‰å…¨æ‰«æå‘ç°åˆæ­¥é—®é¢˜
5. æ ¹æ®å‘ç°ç»§ç»­æ·±å…¥

## é‡è¦åŸåˆ™
1. **ä½ æ˜¯å¤§è„‘** - æ¯ä¸€æ­¥éƒ½è¦æ€è€ƒï¼Œä¸è¦æœºæ¢°æ‰§è¡Œ
2. **åŠ¨æ€è°ƒæ•´** - æ ¹æ®å‘ç°è°ƒæ•´ç­–ç•¥
3. **æ•ˆç‡ä¼˜å…ˆ** - ä¸è¦é‡å¤æ”¶é›†å·²æœ‰ä¿¡æ¯
4. **ä¸»åŠ¨æ¢ç´¢** - å‘ç°æœ‰è¶£çš„ä¸œè¥¿è¦æ·±å…¥

ç°åœ¨å¼€å§‹æ”¶é›†é¡¹ç›®ä¿¡æ¯ï¼"""


@dataclass
class ReconStep:
    """ä¿¡æ¯æ”¶é›†æ­¥éª¤"""
    thought: str
    action: Optional[str] = None
    action_input: Optional[Dict] = None
    observation: Optional[str] = None
    is_final: bool = False
    final_answer: Optional[Dict] = None


class ReconAgent(BaseAgent):
    """
    ä¿¡æ¯æ”¶é›† Agent - LLM é©±åŠ¨ç‰ˆ
    
    LLM å…¨ç¨‹å‚ä¸ï¼Œè‡ªä¸»å†³å®šï¼š
    1. æ”¶é›†ä»€ä¹ˆä¿¡æ¯
    2. ä½¿ç”¨ä»€ä¹ˆå·¥å…·
    3. ä½•æ—¶è¶³å¤Ÿ
    """
    
    def __init__(
        self,
        llm_service,
        tools: Dict[str, Any],
        event_emitter=None,
    ):
        config = AgentConfig(
            name="Recon",
            agent_type=AgentType.RECON,
            pattern=AgentPattern.REACT,
            max_iterations=15,
            system_prompt=RECON_SYSTEM_PROMPT,
        )
        super().__init__(config, llm_service, tools, event_emitter)
        
        self._conversation_history: List[Dict[str, str]] = []
        self._steps: List[ReconStep] = []
    
    def _get_tools_description(self) -> str:
        """ç”Ÿæˆå·¥å…·æè¿°"""
        tools_info = []
        for name, tool in self.tools.items():
            if name.startswith("_"):
                continue
            desc = f"- {name}: {getattr(tool, 'description', 'No description')}"
            tools_info.append(desc)
        return "\n".join(tools_info)
    
    def _parse_llm_response(self, response: str) -> ReconStep:
        """è§£æ LLM å“åº”"""
        step = ReconStep(thought="")
        
        # æå– Thought
        thought_match = re.search(r'Thought:\s*(.*?)(?=Action:|Final Answer:|$)', response, re.DOTALL)
        if thought_match:
            step.thought = thought_match.group(1).strip()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç­”æ¡ˆ
        final_match = re.search(r'Final Answer:\s*(.*?)$', response, re.DOTALL)
        if final_match:
            step.is_final = True
            try:
                answer_text = final_match.group(1).strip()
                answer_text = re.sub(r'```json\s*', '', answer_text)
                answer_text = re.sub(r'```\s*', '', answer_text)
                step.final_answer = json.loads(answer_text)
            except json.JSONDecodeError:
                step.final_answer = {"raw_answer": final_match.group(1).strip()}
            return step
        
        # æå– Action
        action_match = re.search(r'Action:\s*(\w+)', response)
        if action_match:
            step.action = action_match.group(1).strip()
        
        # æå– Action Input
        input_match = re.search(r'Action Input:\s*(.*?)(?=Thought:|Action:|Observation:|$)', response, re.DOTALL)
        if input_match:
            input_text = input_match.group(1).strip()
            input_text = re.sub(r'```json\s*', '', input_text)
            input_text = re.sub(r'```\s*', '', input_text)
            try:
                step.action_input = json.loads(input_text)
            except json.JSONDecodeError:
                step.action_input = {"raw_input": input_text}
        
        return step
    
    async def _execute_tool(self, tool_name: str, tool_input: Dict) -> str:
        """æ‰§è¡Œå·¥å…·"""
        tool = self.tools.get(tool_name)
        
        if not tool:
            return f"é”™è¯¯: å·¥å…· '{tool_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨å·¥å…·: {list(self.tools.keys())}"
        
        try:
            self._tool_calls += 1
            await self.emit_tool_call(tool_name, tool_input)
            
            import time
            start = time.time()
            
            result = await tool.execute(**tool_input)
            
            duration_ms = int((time.time() - start) * 1000)
            await self.emit_tool_result(tool_name, str(result.data)[:200], duration_ms)
            
            if result.success:
                output = str(result.data)
                if len(output) > 4000:
                    output = output[:4000] + f"\n\n... [è¾“å‡ºå·²æˆªæ–­ï¼Œå…± {len(str(result.data))} å­—ç¬¦]"
                return output
            else:
                return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result.error}"
                
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            return f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
    
    async def run(self, input_data: Dict[str, Any]) -> AgentResult:
        """
        æ‰§è¡Œä¿¡æ¯æ”¶é›† - LLM å…¨ç¨‹å‚ä¸ï¼
        """
        import time
        start_time = time.time()
        
        project_info = input_data.get("project_info", {})
        config = input_data.get("config", {})
        task = input_data.get("task", "")
        task_context = input_data.get("task_context", "")
        
        # æ„å»ºåˆå§‹æ¶ˆæ¯
        initial_message = f"""è¯·å¼€å§‹æ”¶é›†é¡¹ç›®ä¿¡æ¯ã€‚

## é¡¹ç›®åŸºæœ¬ä¿¡æ¯
- åç§°: {project_info.get('name', 'unknown')}
- æ ¹ç›®å½•: {project_info.get('root', '.')}

## ä»»åŠ¡ä¸Šä¸‹æ–‡
{task_context or task or 'è¿›è¡Œå…¨é¢çš„ä¿¡æ¯æ”¶é›†ï¼Œä¸ºå®‰å…¨å®¡è®¡åšå‡†å¤‡ã€‚'}

## å¯ç”¨å·¥å…·
{self._get_tools_description()}

è¯·å¼€å§‹ä½ çš„ä¿¡æ¯æ”¶é›†å·¥ä½œã€‚é¦–å…ˆæ€è€ƒåº”è¯¥æ”¶é›†ä»€ä¹ˆä¿¡æ¯ï¼Œç„¶åé€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚"""

        # åˆå§‹åŒ–å¯¹è¯å†å²
        self._conversation_history = [
            {"role": "system", "content": self.config.system_prompt},
            {"role": "user", "content": initial_message},
        ]
        
        self._steps = []
        final_result = None
        
        await self.emit_thinking("ğŸ” Recon Agent å¯åŠ¨ï¼ŒLLM å¼€å§‹è‡ªä¸»æ”¶é›†ä¿¡æ¯...")
        
        try:
            for iteration in range(self.config.max_iterations):
                if self.is_cancelled:
                    break
                
                self._iteration = iteration + 1
                
                # ğŸ”¥ å‘å°„ LLM å¼€å§‹æ€è€ƒäº‹ä»¶
                await self.emit_llm_start(iteration + 1)
                
                # ğŸ”¥ è°ƒç”¨ LLM è¿›è¡Œæ€è€ƒå’Œå†³ç­–
                response = await self.llm_service.chat_completion_raw(
                    messages=self._conversation_history,
                    temperature=0.1,
                    max_tokens=2048,
                )
                
                llm_output = response.get("content", "")
                tokens_this_round = response.get("usage", {}).get("total_tokens", 0)
                self._total_tokens += tokens_this_round
                
                # è§£æ LLM å“åº”
                step = self._parse_llm_response(llm_output)
                self._steps.append(step)
                
                # ğŸ”¥ å‘å°„ LLM æ€è€ƒå†…å®¹äº‹ä»¶ - å±•ç¤º LLM åœ¨æƒ³ä»€ä¹ˆ
                if step.thought:
                    await self.emit_llm_thought(step.thought, iteration + 1)
                
                # æ·»åŠ  LLM å“åº”åˆ°å†å²
                self._conversation_history.append({
                    "role": "assistant",
                    "content": llm_output,
                })
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if step.is_final:
                    await self.emit_llm_decision("å®Œæˆä¿¡æ¯æ”¶é›†", "LLM åˆ¤æ–­å·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯")
                    await self.emit_llm_complete(
                        f"ä¿¡æ¯æ”¶é›†å®Œæˆï¼Œå…± {self._iteration} è½®æ€è€ƒ",
                        self._total_tokens
                    )
                    final_result = step.final_answer
                    break
                
                # æ‰§è¡Œå·¥å…·
                if step.action:
                    # ğŸ”¥ å‘å°„ LLM åŠ¨ä½œå†³ç­–äº‹ä»¶
                    await self.emit_llm_action(step.action, step.action_input or {})
                    
                    observation = await self._execute_tool(
                        step.action,
                        step.action_input or {}
                    )
                    
                    step.observation = observation
                    
                    # ğŸ”¥ å‘å°„ LLM è§‚å¯Ÿäº‹ä»¶
                    await self.emit_llm_observation(observation)
                    
                    # æ·»åŠ è§‚å¯Ÿç»“æœåˆ°å†å²
                    self._conversation_history.append({
                        "role": "user",
                        "content": f"Observation:\n{observation}",
                    })
                else:
                    # LLM æ²¡æœ‰é€‰æ‹©å·¥å…·ï¼Œæç¤ºå®ƒç»§ç»­
                    await self.emit_llm_decision("ç»§ç»­æ€è€ƒ", "LLM éœ€è¦æ›´å¤šä¿¡æ¯")
                    self._conversation_history.append({
                        "role": "user",
                        "content": "è¯·ç»§ç»­ï¼Œé€‰æ‹©ä¸€ä¸ªå·¥å…·æ‰§è¡Œï¼Œæˆ–è€…å¦‚æœä¿¡æ¯æ”¶é›†å®Œæˆï¼Œè¾“å‡º Final Answerã€‚",
                    })
            
            # å¤„ç†ç»“æœ
            duration_ms = int((time.time() - start_time) * 1000)
            
            # å¦‚æœæ²¡æœ‰æœ€ç»ˆç»“æœï¼Œä»å†å²ä¸­æ±‡æ€»
            if not final_result:
                final_result = self._summarize_from_steps()
            
            await self.emit_event(
                "info",
                f"ğŸ¯ Recon Agent å®Œæˆ: {self._iteration} è½®è¿­ä»£, {self._tool_calls} æ¬¡å·¥å…·è°ƒç”¨"
            )
            
            return AgentResult(
                success=True,
                data=final_result,
                iterations=self._iteration,
                tool_calls=self._tool_calls,
                tokens_used=self._total_tokens,
                duration_ms=duration_ms,
            )
            
        except Exception as e:
            logger.error(f"Recon Agent failed: {e}", exc_info=True)
            return AgentResult(success=False, error=str(e))
    
    def _summarize_from_steps(self) -> Dict[str, Any]:
        """ä»æ­¥éª¤ä¸­æ±‡æ€»ç»“æœ"""
        # é»˜è®¤ç»“æœç»“æ„
        result = {
            "project_structure": {},
            "tech_stack": {
                "languages": [],
                "frameworks": [],
                "databases": [],
            },
            "entry_points": [],
            "high_risk_areas": [],
            "dependencies": {},
            "initial_findings": [],
        }
        
        # ä»æ­¥éª¤çš„è§‚å¯Ÿç»“æœä¸­æå–ä¿¡æ¯
        for step in self._steps:
            if step.observation:
                # å°è¯•ä»è§‚å¯Ÿä¸­è¯†åˆ«æŠ€æœ¯æ ˆç­‰ä¿¡æ¯
                obs_lower = step.observation.lower()
                
                if "package.json" in obs_lower:
                    result["tech_stack"]["languages"].append("JavaScript/TypeScript")
                if "requirements.txt" in obs_lower or "setup.py" in obs_lower:
                    result["tech_stack"]["languages"].append("Python")
                if "go.mod" in obs_lower:
                    result["tech_stack"]["languages"].append("Go")
                
                # è¯†åˆ«æ¡†æ¶
                if "react" in obs_lower:
                    result["tech_stack"]["frameworks"].append("React")
                if "django" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Django")
                if "fastapi" in obs_lower:
                    result["tech_stack"]["frameworks"].append("FastAPI")
                if "express" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Express")
        
        # å»é‡
        result["tech_stack"]["languages"] = list(set(result["tech_stack"]["languages"]))
        result["tech_stack"]["frameworks"] = list(set(result["tech_stack"]["frameworks"]))
        
        return result
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self._conversation_history
    
    def get_steps(self) -> List[ReconStep]:
        """è·å–æ‰§è¡Œæ­¥éª¤"""
        return self._steps
