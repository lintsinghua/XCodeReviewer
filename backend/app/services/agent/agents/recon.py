"""
Recon Agent (ä¿¡æ¯æ”¶é›†å±‚) - LLM é©±åŠ¨ç‰ˆ

LLM æ˜¯çœŸæ­£çš„å¤§è„‘ï¼
- LLM å†³å®šæ”¶é›†ä»€ä¹ˆä¿¡æ¯
- LLM å†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·
- LLM å†³å®šä½•æ—¶ä¿¡æ¯è¶³å¤Ÿ
- LLM åŠ¨æ€è°ƒæ•´æ”¶é›†ç­–ç•¥

ç±»å‹: ReAct (çœŸæ­£çš„!)
"""

import asyncio
import json
import logging
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from .base import BaseAgent, AgentConfig, AgentResult, AgentType, AgentPattern
from ..json_parser import AgentJsonParser
from ..prompts import TOOL_USAGE_GUIDE

logger = logging.getLogger(__name__)


RECON_SYSTEM_PROMPT = """ä½ æ˜¯ DeepAudit çš„ä¾¦å¯Ÿ Agentï¼Œè´Ÿè´£æ”¶é›†å’Œåˆ†æé¡¹ç›®ä¿¡æ¯ã€‚

## ä½ çš„èŒè´£
ä½œä¸ºä¾¦å¯Ÿå±‚ï¼Œä½ è´Ÿè´£ï¼š
1. åˆ†æé¡¹ç›®ç»“æ„å’ŒæŠ€æœ¯æ ˆ
2. è¯†åˆ«å…³é”®å…¥å£ç‚¹
3. å‘ç°é…ç½®æ–‡ä»¶å’Œæ•æ„ŸåŒºåŸŸ
4. **æ¨èéœ€è¦ä½¿ç”¨çš„å¤–éƒ¨å®‰å…¨å·¥å…·**
5. æä¾›åˆæ­¥é£é™©è¯„ä¼°

## ä¾¦å¯Ÿç›®æ ‡

### 1. æŠ€æœ¯æ ˆè¯†åˆ«ï¼ˆç”¨äºé€‰æ‹©å¤–éƒ¨å·¥å…·ï¼‰
- ç¼–ç¨‹è¯­è¨€å’Œç‰ˆæœ¬
- Webæ¡†æ¶ï¼ˆDjango, Flask, FastAPI, Expressç­‰ï¼‰
- æ•°æ®åº“ç±»å‹
- å‰ç«¯æ¡†æ¶
- **æ ¹æ®æŠ€æœ¯æ ˆæ¨èå¤–éƒ¨å·¥å…·ï¼š**
  - Pythoné¡¹ç›® â†’ bandit_scan, safety_scan
  - Node.jsé¡¹ç›® â†’ npm_audit
  - æ‰€æœ‰é¡¹ç›® â†’ semgrep_scan, gitleaks_scan
  - å¤§å‹é¡¹ç›® â†’ kunlun_scan, osv_scan

### 2. å…¥å£ç‚¹å‘ç°
- HTTPè·¯ç”±å’ŒAPIç«¯ç‚¹
- Websocketå¤„ç†
- å®šæ—¶ä»»åŠ¡å’Œåå°ä½œä¸š
- æ¶ˆæ¯é˜Ÿåˆ—æ¶ˆè´¹è€…

### 3. æ•æ„ŸåŒºåŸŸå®šä½
- è®¤è¯å’Œæˆæƒä»£ç 
- æ•°æ®åº“æ“ä½œ
- æ–‡ä»¶å¤„ç†
- å¤–éƒ¨æœåŠ¡è°ƒç”¨

### 4. é…ç½®åˆ†æ
- å®‰å…¨é…ç½®
- è°ƒè¯•è®¾ç½®
- å¯†é’¥ç®¡ç†

## å·¥ä½œæ–¹å¼
æ¯ä¸€æ­¥ï¼Œä½ éœ€è¦è¾“å‡ºï¼š

```
Thought: [åˆ†æå½“å‰æƒ…å†µï¼Œæ€è€ƒéœ€è¦æ”¶é›†ä»€ä¹ˆä¿¡æ¯]
Action: [å·¥å…·åç§°]
Action Input: {"å‚æ•°1": "å€¼1"}
```

å½“ä½ å®Œæˆä¿¡æ¯æ”¶é›†åï¼Œè¾“å‡ºï¼š

```
Thought: [æ€»ç»“æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯]
Final Answer: [JSON æ ¼å¼çš„ç»“æœ]
```

## âš ï¸ è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆä¸¥æ ¼éµå®ˆï¼‰

**ç¦æ­¢ä½¿ç”¨ Markdown æ ¼å¼æ ‡è®°ï¼** ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯çº¯æ–‡æœ¬æ ¼å¼ï¼š

âœ… æ­£ç¡®æ ¼å¼ï¼š
```
Thought: æˆ‘éœ€è¦æŸ¥çœ‹é¡¹ç›®ç»“æ„æ¥äº†è§£é¡¹ç›®ç»„æˆ
Action: list_files
Action Input: {"directory": "."}
```

âŒ é”™è¯¯æ ¼å¼ï¼ˆç¦æ­¢ä½¿ç”¨ï¼‰ï¼š
```
**Thought:** æˆ‘éœ€è¦æŸ¥çœ‹é¡¹ç›®ç»“æ„
**Action:** list_files
**Action Input:** {"directory": "."}
```

è§„åˆ™ï¼š
1. ä¸è¦åœ¨ Thought:ã€Action:ã€Action Input:ã€Final Answer: å‰åæ·»åŠ  `**`
2. ä¸è¦ä½¿ç”¨å…¶ä»– Markdown æ ¼å¼ï¼ˆå¦‚ `###`ã€`*æ–œä½“*` ç­‰ï¼‰
3. Action Input å¿…é¡»æ˜¯å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œä¸èƒ½ä¸ºç©ºæˆ–æˆªæ–­

## è¾“å‡ºæ ¼å¼

```
Final Answer: {
    "project_structure": {...},
    "tech_stack": {
        "languages": [...],
        "frameworks": [...],
        "databases": [...]
    },
    "recommended_tools": {
        "must_use": ["semgrep_scan", "gitleaks_scan", ...],
        "recommended": ["kunlun_scan", ...],
        "reason": "åŸºäºé¡¹ç›®æŠ€æœ¯æ ˆçš„æ¨èç†ç”±"
    },
    "entry_points": [
        {"type": "...", "file": "...", "line": ..., "method": "..."}
    ],
    "high_risk_areas": [
        "æ–‡ä»¶è·¯å¾„:è¡Œå· - é£é™©æè¿°"
    ],
    "initial_findings": [
        {"title": "...", "file_path": "...", "line_start": ..., "description": "..."}
    ],
    "summary": "é¡¹ç›®ä¾¦å¯Ÿæ€»ç»“"
}
```

## âš ï¸ é‡è¦è¾“å‡ºè¦æ±‚

### recommended_tools æ ¼å¼è¦æ±‚
**å¿…é¡»**æ ¹æ®é¡¹ç›®æŠ€æœ¯æ ˆæ¨èå¤–éƒ¨å·¥å…·ï¼š
- `must_use`: å¿…é¡»ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨
- `recommended`: æ¨èä½¿ç”¨çš„å·¥å…·åˆ—è¡¨
- `reason`: æ¨èç†ç”±

### high_risk_areas æ ¼å¼è¦æ±‚
æ¯ä¸ªé«˜é£é™©åŒºåŸŸ**å¿…é¡»**åŒ…å«å…·ä½“çš„æ–‡ä»¶è·¯å¾„ï¼Œæ ¼å¼ä¸ºï¼š
- `"app.py:36 - SECRET_KEY ç¡¬ç¼–ç "`
- `"utils/file.py:120 - ä½¿ç”¨ç”¨æˆ·è¾“å…¥æ„é€ æ–‡ä»¶è·¯å¾„"`
- `"api/views.py:45 - SQL æŸ¥è¯¢ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥"`

**ç¦æ­¢**è¾“å‡ºçº¯æè¿°æ€§æ–‡æœ¬å¦‚ "File write operations with user-controlled paths"ï¼Œå¿…é¡»æŒ‡æ˜å…·ä½“æ–‡ä»¶ã€‚

### initial_findings æ ¼å¼è¦æ±‚
æ¯ä¸ªå‘ç°**å¿…é¡»**åŒ…å«ï¼š
- `title`: æ¼æ´æ ‡é¢˜
- `file_path`: å…·ä½“æ–‡ä»¶è·¯å¾„
- `line_start`: è¡Œå·
- `description`: è¯¦ç»†æè¿°

## ğŸš¨ é˜²æ­¢å¹»è§‰ï¼ˆå…³é”®ï¼ï¼‰

**åªæŠ¥å‘Šä½ å®é™…è¯»å–è¿‡çš„æ–‡ä»¶ï¼**

1. **file_path å¿…é¡»æ¥è‡ªå®é™…å·¥å…·è°ƒç”¨ç»“æœ**
   - åªä½¿ç”¨ list_files è¿”å›çš„æ–‡ä»¶åˆ—è¡¨ä¸­çš„è·¯å¾„
   - åªä½¿ç”¨ read_file æˆåŠŸè¯»å–çš„æ–‡ä»¶è·¯å¾„
   - ä¸è¦"çŒœæµ‹"å…¸å‹çš„é¡¹ç›®ç»“æ„ï¼ˆå¦‚ app.py, config.pyï¼‰

2. **è¡Œå·å¿…é¡»æ¥è‡ªå®é™…ä»£ç **
   - åªä½¿ç”¨ read_file è¿”å›å†…å®¹ä¸­çš„çœŸå®è¡Œå·
   - ä¸è¦ç¼–é€ è¡Œå·

3. **ç¦æ­¢å¥—ç”¨æ¨¡æ¿**
   - ä¸è¦å› ä¸ºæ˜¯ "Python é¡¹ç›®" å°±å‡è®¾å­˜åœ¨ requirements.txt
   - ä¸è¦å› ä¸ºæ˜¯ "Web é¡¹ç›®" å°±å‡è®¾å­˜åœ¨ routes.py æˆ– views.py

âŒ é”™è¯¯åšæ³•ï¼š
```
list_files è¿”å›: ["main.rs", "lib.rs", "Cargo.toml"]
high_risk_areas: ["app.py:36 - å­˜åœ¨å®‰å…¨é—®é¢˜"]  <- è¿™æ˜¯å¹»è§‰ï¼é¡¹ç›®æ ¹æœ¬æ²¡æœ‰ app.py
```

âœ… æ­£ç¡®åšæ³•ï¼š
```
list_files è¿”å›: ["main.rs", "lib.rs", "Cargo.toml"]
high_risk_areas: ["main.rs:xx - å¯èƒ½å­˜åœ¨é—®é¢˜"]  <- å¿…é¡»ä½¿ç”¨å®é™…å­˜åœ¨çš„æ–‡ä»¶
```

## âš ï¸ å…³é”®çº¦æŸ - å¿…é¡»éµå®ˆï¼
1. **ç¦æ­¢ç›´æ¥è¾“å‡º Final Answer** - ä½ å¿…é¡»å…ˆè°ƒç”¨å·¥å…·æ¥æ”¶é›†é¡¹ç›®ä¿¡æ¯
2. **è‡³å°‘è°ƒç”¨ä¸‰ä¸ªå·¥å…·** - ä½¿ç”¨ rag_query è¯­ä¹‰æœç´¢å…³é”®å…¥å£ï¼Œread_file è¯»å–æ–‡ä»¶ï¼Œlist_files ä»…æŸ¥çœ‹æ ¹ç›®å½•
3. **æ²¡æœ‰å·¥å…·è°ƒç”¨çš„ä¾¦å¯Ÿæ— æ•ˆ** - ä¸å…è®¸ä»…å‡­é¡¹ç›®åç§°ç›´æ¥æ¨æµ‹
4. **å…ˆ Action å Final Answer** - å¿…é¡»å…ˆæ‰§è¡Œå·¥å…·ï¼Œè·å– Observationï¼Œå†è¾“å‡ºæœ€ç»ˆç»“è®º

é”™è¯¯ç¤ºä¾‹ï¼ˆç¦æ­¢ï¼‰ï¼š
```
Thought: è¿™æ˜¯ä¸€ä¸ª PHP é¡¹ç›®ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨é—®é¢˜
Final Answer: {...}  âŒ æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ï¼
```

æ­£ç¡®ç¤ºä¾‹ï¼ˆå¿…é¡»ï¼‰ï¼š
```
Thought: æˆ‘éœ€è¦å…ˆæŸ¥çœ‹é¡¹ç›®ç»“æ„æ¥äº†è§£é¡¹ç›®ç»„æˆ
Action: rag_query
Action Input: {"query": "é¡¹ç›®çš„å…¥å£ç‚¹å’Œè·¯ç”±å®šä¹‰åœ¨å“ªé‡Œï¼Ÿ", "top_k": 5}
```
**æˆ–è€…**ä»…æŸ¥çœ‹æ ¹ç›®å½•ç»“æ„ï¼š
```
Thought: æˆ‘éœ€è¦å…ˆæŸ¥çœ‹é¡¹ç›®æ ¹ç›®å½•ç»“æ„
Action: list_files
Action Input: {"directory": "."}
```
ç„¶åç­‰å¾… Observationï¼Œå†ç»§ç»­æ”¶é›†ä¿¡æ¯æˆ–è¾“å‡º Final Answerã€‚
"""


# ... (ä¸Šæ–‡å¯¼å…¥)
# ...

@dataclass
class ReconStep:
    """ä¿¡æ¯æ”¶é›†æ­¥éª¤"""
    thought: str
    action: Optional[str] = None
    action_input: Optional[Dict] = None
    observation: Optional[str] = None
    is_final: bool = False
    final_answer: Optional[Dict] = None


class ReconAgent(BaseAgent):
    """
    ä¿¡æ¯æ”¶é›† Agent - LLM é©±åŠ¨ç‰ˆ
    
    LLM å…¨ç¨‹å‚ä¸ï¼Œè‡ªä¸»å†³å®šï¼š
    1. æ”¶é›†ä»€ä¹ˆä¿¡æ¯
    2. ä½¿ç”¨ä»€ä¹ˆå·¥å…·
    3. ä½•æ—¶è¶³å¤Ÿ
    """
    
    def __init__(
        self,
        llm_service,
        tools: Dict[str, Any],
        event_emitter=None,
    ):
        # ç»„åˆå¢å¼ºçš„ç³»ç»Ÿæç¤ºè¯
        full_system_prompt = f"{RECON_SYSTEM_PROMPT}\n\n{TOOL_USAGE_GUIDE}"
        
        config = AgentConfig(
            name="Recon",
            agent_type=AgentType.RECON,
            pattern=AgentPattern.REACT,
            max_iterations=15,
            system_prompt=full_system_prompt,
        )
        super().__init__(config, llm_service, tools, event_emitter)
        
        self._conversation_history: List[Dict[str, str]] = []
        self._steps: List[ReconStep] = []
    
    def _parse_llm_response(self, response: str) -> ReconStep:
        """è§£æ LLM å“åº” - å¢å¼ºç‰ˆï¼Œæ›´å¥å£®åœ°æå–æ€è€ƒå†…å®¹"""
        step = ReconStep(thought="")

        # ğŸ”¥ v2.1: é¢„å¤„ç† - ç§»é™¤ Markdown æ ¼å¼æ ‡è®°ï¼ˆLLM æœ‰æ—¶ä¼šè¾“å‡º **Action:** è€Œé Action:ï¼‰
        cleaned_response = response
        cleaned_response = re.sub(r'\*\*Action:\*\*', 'Action:', cleaned_response)
        cleaned_response = re.sub(r'\*\*Action Input:\*\*', 'Action Input:', cleaned_response)
        cleaned_response = re.sub(r'\*\*Thought:\*\*', 'Thought:', cleaned_response)
        cleaned_response = re.sub(r'\*\*Final Answer:\*\*', 'Final Answer:', cleaned_response)
        cleaned_response = re.sub(r'\*\*Observation:\*\*', 'Observation:', cleaned_response)

        # ğŸ”¥ é¦–å…ˆå°è¯•æå–æ˜ç¡®çš„ Thought æ ‡è®°
        thought_match = re.search(r'Thought:\s*(.*?)(?=Action:|Final Answer:|$)', cleaned_response, re.DOTALL)
        if thought_match:
            step.thought = thought_match.group(1).strip()

        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç­”æ¡ˆ
        final_match = re.search(r'Final Answer:\s*(.*?)$', cleaned_response, re.DOTALL)
        if final_match:
            step.is_final = True
            answer_text = final_match.group(1).strip()
            answer_text = re.sub(r'```json\s*', '', answer_text)
            answer_text = re.sub(r'```\s*', '', answer_text)
            # ä½¿ç”¨å¢å¼ºçš„ JSON è§£æå™¨
            step.final_answer = AgentJsonParser.parse(
                answer_text,
                default={"raw_answer": answer_text}
            )
            # ç¡®ä¿ findings æ ¼å¼æ­£ç¡®
            if "initial_findings" in step.final_answer:
                step.final_answer["initial_findings"] = [
                    f for f in step.final_answer["initial_findings"]
                    if isinstance(f, dict)
                ]

            # ğŸ”¥ å¦‚æœæ²¡æœ‰æå–åˆ° thoughtï¼Œä½¿ç”¨ Final Answer å‰çš„å†…å®¹ä½œä¸ºæ€è€ƒ
            if not step.thought:
                before_final = cleaned_response[:cleaned_response.find('Final Answer:')].strip()
                if before_final:
                    # ç§»é™¤å¯èƒ½çš„ Thought: å‰ç¼€
                    before_final = re.sub(r'^Thought:\s*', '', before_final)
                    step.thought = before_final[:500] if len(before_final) > 500 else before_final

            return step

        # ğŸ”¥ æå– Action
        action_match = re.search(r'Action:\s*(\w+)', cleaned_response)
        if action_match:
            step.action = action_match.group(1).strip()

            # ğŸ”¥ å¦‚æœæ²¡æœ‰æå–åˆ° thoughtï¼Œæå– Action ä¹‹å‰çš„å†…å®¹ä½œä¸ºæ€è€ƒ
            if not step.thought:
                action_pos = cleaned_response.find('Action:')
                if action_pos > 0:
                    before_action = cleaned_response[:action_pos].strip()
                    # ç§»é™¤å¯èƒ½çš„ Thought: å‰ç¼€
                    before_action = re.sub(r'^Thought:\s*', '', before_action)
                    if before_action:
                        step.thought = before_action[:500] if len(before_action) > 500 else before_action

        # ğŸ”¥ æå– Action Input
        input_match = re.search(r'Action Input:\s*(.*?)(?=Thought:|Action:|Observation:|$)', cleaned_response, re.DOTALL)
        if input_match:
            input_text = input_match.group(1).strip()
            input_text = re.sub(r'```json\s*', '', input_text)
            input_text = re.sub(r'```\s*', '', input_text)
            # ä½¿ç”¨å¢å¼ºçš„ JSON è§£æå™¨
            step.action_input = AgentJsonParser.parse(
                input_text,
                default={"raw_input": input_text}
            )

        # ğŸ”¥ æœ€åçš„ fallbackï¼šå¦‚æœæ•´ä¸ªå“åº”æ²¡æœ‰ä»»ä½•æ ‡è®°ï¼Œæ•´ä½“ä½œä¸ºæ€è€ƒ
        if not step.thought and not step.action and not step.is_final:
            if response.strip():
                step.thought = response.strip()[:500]

        return step
    

    
    async def run(self, input_data: Dict[str, Any]) -> AgentResult:
        """
        æ‰§è¡Œä¿¡æ¯æ”¶é›† - LLM å…¨ç¨‹å‚ä¸ï¼
        """
        import time
        start_time = time.time()
        
        project_info = input_data.get("project_info", {})
        config = input_data.get("config", {})
        task = input_data.get("task", "")
        task_context = input_data.get("task_context", "")
        
        # ğŸ”¥ è·å–ç›®æ ‡æ–‡ä»¶åˆ—è¡¨
        target_files = config.get("target_files", [])
        exclude_patterns = config.get("exclude_patterns", [])
        
        # æ„å»ºåˆå§‹æ¶ˆæ¯
        initial_message = f"""è¯·å¼€å§‹æ”¶é›†é¡¹ç›®ä¿¡æ¯ã€‚

## é¡¹ç›®åŸºæœ¬ä¿¡æ¯
- åç§°: {project_info.get('name', 'unknown')}
- æ ¹ç›®å½•: {project_info.get('root', '.')}
- æ–‡ä»¶æ•°é‡: {project_info.get('file_count', 'unknown')}

## å®¡è®¡èŒƒå›´
"""
        # ğŸ”¥ å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ–‡ä»¶ï¼Œæ˜ç¡®å‘ŠçŸ¥ Agent
        if target_files:
            initial_message += f"""âš ï¸ **é‡è¦**: ç”¨æˆ·æŒ‡å®šäº† {len(target_files)} ä¸ªç›®æ ‡æ–‡ä»¶è¿›è¡Œå®¡è®¡ï¼š
"""
            for tf in target_files[:10]:
                initial_message += f"- {tf}\n"
            if len(target_files) > 10:
                initial_message += f"- ... è¿˜æœ‰ {len(target_files) - 10} ä¸ªæ–‡ä»¶\n"
            initial_message += """
è¯·ç›´æ¥è¯»å–å’Œåˆ†æè¿™äº›æŒ‡å®šçš„æ–‡ä»¶ï¼Œä¸è¦æµªè´¹æ—¶é—´éå†å…¶ä»–ç›®å½•ã€‚
"""
        else:
            initial_message += "å…¨é¡¹ç›®å®¡è®¡ï¼ˆæ— ç‰¹å®šæ–‡ä»¶é™åˆ¶ï¼‰\n"
        
        if exclude_patterns:
            initial_message += f"\næ’é™¤æ¨¡å¼: {', '.join(exclude_patterns[:5])}\n"
        
        initial_message += f"""
## ä»»åŠ¡ä¸Šä¸‹æ–‡
{task_context or task or 'è¿›è¡Œå…¨é¢çš„ä¿¡æ¯æ”¶é›†ï¼Œä¸ºå®‰å…¨å®¡è®¡åšå‡†å¤‡ã€‚'}

## å¯ç”¨å·¥å…·
{self.get_tools_description()}

è¯·å¼€å§‹ä½ çš„ä¿¡æ¯æ”¶é›†å·¥ä½œã€‚é¦–å…ˆæ€è€ƒåº”è¯¥æ”¶é›†ä»€ä¹ˆä¿¡æ¯ï¼Œç„¶å**ç«‹å³**é€‰æ‹©åˆé€‚çš„å·¥å…·æ‰§è¡Œï¼ˆè¾“å‡º Actionï¼‰ã€‚ä¸è¦åªè¾“å‡º Thoughtï¼Œå¿…é¡»ç´§æ¥ç€è¾“å‡º Actionã€‚"""

        # åˆå§‹åŒ–å¯¹è¯å†å²
        self._conversation_history = [
            {"role": "system", "content": self.config.system_prompt},
            {"role": "user", "content": initial_message},
        ]
        
        self._steps = []
        final_result = None
        error_message = None  # ğŸ”¥ è·Ÿè¸ªé”™è¯¯ä¿¡æ¯
        
        await self.emit_thinking("Recon Agent å¯åŠ¨ï¼ŒLLM å¼€å§‹è‡ªä¸»æ”¶é›†ä¿¡æ¯...")
        
        try:
            for iteration in range(self.config.max_iterations):
                if self.is_cancelled:
                    break
                
                self._iteration = iteration + 1
                
                # ğŸ”¥ å†æ¬¡æ£€æŸ¥å–æ¶ˆæ ‡å¿—ï¼ˆåœ¨LLMè°ƒç”¨ä¹‹å‰ï¼‰
                if self.is_cancelled:
                    await self.emit_thinking("ğŸ›‘ ä»»åŠ¡å·²å–æ¶ˆï¼Œåœæ­¢æ‰§è¡Œ")
                    break
                
                # è°ƒç”¨ LLM è¿›è¡Œæ€è€ƒå’Œå†³ç­–ï¼ˆä½¿ç”¨åŸºç±»ç»Ÿä¸€æ–¹æ³•ï¼‰
                try:
                    llm_output, tokens_this_round = await self.stream_llm_call(
                        self._conversation_history,
                        # ğŸ”¥ ä¸ä¼ é€’ temperature å’Œ max_tokensï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®
                    )
                except asyncio.CancelledError:
                    logger.info(f"[{self.name}] LLM call cancelled")
                    break
                
                self._total_tokens += tokens_this_round
                
                # ğŸ”¥ Enhanced: Handle empty LLM response with better diagnostics
                if not llm_output or not llm_output.strip():
                    empty_retry_count = getattr(self, '_empty_retry_count', 0) + 1
                    self._empty_retry_count = empty_retry_count
                    
                    # ğŸ”¥ è®°å½•æ›´è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
                    logger.warning(
                        f"[{self.name}] Empty LLM response in iteration {self._iteration} "
                        f"(retry {empty_retry_count}/3, tokens_this_round={tokens_this_round})"
                    )
                    
                    if empty_retry_count >= 3:
                        logger.error(f"[{self.name}] Too many empty responses, generating fallback result")
                        error_message = "è¿ç»­æ”¶åˆ°ç©ºå“åº”ï¼Œä½¿ç”¨å›é€€ç»“æœ"
                        await self.emit_event("warning", error_message)
                        # ğŸ”¥ ä¸æ˜¯ç›´æ¥ breakï¼Œè€Œæ˜¯å°è¯•ç”Ÿæˆä¸€ä¸ªå›é€€ç»“æœ
                        break
                    
                    # ğŸ”¥ æ›´æœ‰é’ˆå¯¹æ€§çš„é‡è¯•æç¤º
                    retry_prompt = f"""æ”¶åˆ°ç©ºå“åº”ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„æ€è€ƒå’Œè¡ŒåŠ¨ï¼š

Thought: [ä½ å¯¹å½“å‰æƒ…å†µçš„åˆ†æ]
Action: [å·¥å…·åç§°ï¼Œå¦‚ list_files, read_file, search_code]
Action Input: {{"å‚æ•°å": "å‚æ•°å€¼"}}

å¯ç”¨å·¥å…·: {', '.join(self.tools.keys())}

å¦‚æœä½ è®¤ä¸ºä¿¡æ¯æ”¶é›†å·²ç»å®Œæˆï¼Œè¯·è¾“å‡ºï¼š
Thought: [æ€»ç»“æ”¶é›†åˆ°çš„ä¿¡æ¯]
Final Answer: [JSONæ ¼å¼çš„ç»“æœ]"""
                    
                    self._conversation_history.append({
                        "role": "user",
                        "content": retry_prompt,
                    })
                    continue
                
                # é‡ç½®ç©ºå“åº”è®¡æ•°å™¨
                self._empty_retry_count = 0

                # è§£æ LLM å“åº”
                step = self._parse_llm_response(llm_output)
                self._steps.append(step)
                
                # ğŸ”¥ å‘å°„ LLM æ€è€ƒå†…å®¹äº‹ä»¶ - å±•ç¤º LLM åœ¨æƒ³ä»€ä¹ˆ
                if step.thought:
                    await self.emit_llm_thought(step.thought, iteration + 1)
                
                # æ·»åŠ  LLM å“åº”åˆ°å†å²
                self._conversation_history.append({
                    "role": "assistant",
                    "content": llm_output,
                })
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if step.is_final:
                    await self.emit_llm_decision("å®Œæˆä¿¡æ¯æ”¶é›†", "LLM åˆ¤æ–­å·²æ”¶é›†è¶³å¤Ÿä¿¡æ¯")
                    await self.emit_llm_complete(
                        f"ä¿¡æ¯æ”¶é›†å®Œæˆï¼Œå…± {self._iteration} è½®æ€è€ƒ",
                        self._total_tokens
                    )
                    final_result = step.final_answer
                    break
                
                # æ‰§è¡Œå·¥å…·
                if step.action:
                    # ğŸ”¥ å‘å°„ LLM åŠ¨ä½œå†³ç­–äº‹ä»¶
                    await self.emit_llm_action(step.action, step.action_input or {})
                    
                    # ğŸ”¥ å¾ªç¯æ£€æµ‹ï¼šè¿½è¸ªå·¥å…·è°ƒç”¨å¤±è´¥å†å²
                    tool_call_key = f"{step.action}:{json.dumps(step.action_input or {}, sort_keys=True)}"
                    if not hasattr(self, '_failed_tool_calls'):
                        self._failed_tool_calls = {}
                    
                    observation = await self.execute_tool(
                        step.action,
                        step.action_input or {}
                    )
                    
                    # ğŸ”¥ æ£€æµ‹å·¥å…·è°ƒç”¨å¤±è´¥å¹¶è¿½è¸ª
                    is_tool_error = (
                        "å¤±è´¥" in observation or 
                        "é”™è¯¯" in observation or 
                        "ä¸å­˜åœ¨" in observation or
                        "æ–‡ä»¶è¿‡å¤§" in observation or
                        "Error" in observation
                    )
                    
                    if is_tool_error:
                        self._failed_tool_calls[tool_call_key] = self._failed_tool_calls.get(tool_call_key, 0) + 1
                        fail_count = self._failed_tool_calls[tool_call_key]
                        
                        # ğŸ”¥ å¦‚æœåŒä¸€è°ƒç”¨è¿ç»­å¤±è´¥3æ¬¡ï¼Œæ·»åŠ å¼ºåˆ¶è·³è¿‡æç¤º
                        if fail_count >= 3:
                            logger.warning(f"[{self.name}] Tool call failed {fail_count} times: {tool_call_key}")
                            observation += f"\n\nâš ï¸ **ç³»ç»Ÿæç¤º**: æ­¤å·¥å…·è°ƒç”¨å·²è¿ç»­å¤±è´¥ {fail_count} æ¬¡ã€‚è¯·ï¼š\n"
                            observation += "1. å°è¯•ä½¿ç”¨ä¸åŒçš„å‚æ•°ï¼ˆå¦‚æŒ‡å®šè¾ƒå°çš„è¡ŒèŒƒå›´ï¼‰\n"
                            observation += "2. ä½¿ç”¨ search_code å·¥å…·å®šä½å…³é”®ä»£ç ç‰‡æ®µ\n"
                            observation += "3. è·³è¿‡æ­¤æ–‡ä»¶ï¼Œç»§ç»­åˆ†æå…¶ä»–æ–‡ä»¶\n"
                            observation += "4. å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œç›´æ¥è¾“å‡º Final Answer"
                            
                            # é‡ç½®è®¡æ•°å™¨ä½†ä¿ç•™è®°å½•
                            self._failed_tool_calls[tool_call_key] = 0
                    else:
                        # æˆåŠŸè°ƒç”¨ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                        if tool_call_key in self._failed_tool_calls:
                            del self._failed_tool_calls[tool_call_key]
                    
                    # ğŸ”¥ å·¥å…·æ‰§è¡Œåæ£€æŸ¥å–æ¶ˆçŠ¶æ€
                    if self.is_cancelled:
                        logger.info(f"[{self.name}] Cancelled after tool execution")
                        break
                    
                    step.observation = observation
                    
                    # ğŸ”¥ å‘å°„ LLM è§‚å¯Ÿäº‹ä»¶
                    await self.emit_llm_observation(observation)
                    
                    # æ·»åŠ è§‚å¯Ÿç»“æœåˆ°å†å²
                    self._conversation_history.append({
                        "role": "user",
                        "content": f"Observation:\n{observation}",
                    })
                else:
                    # LLM æ²¡æœ‰é€‰æ‹©å·¥å…·ï¼Œæç¤ºå®ƒç»§ç»­
                    await self.emit_llm_decision("ç»§ç»­æ€è€ƒ", "LLM éœ€è¦æ›´å¤šä¿¡æ¯")
                    self._conversation_history.append({
                        "role": "user",
                        "content": "è¯·ç»§ç»­ã€‚ä½ è¾“å‡ºäº† Thought ä½†æ²¡æœ‰è¾“å‡º Actionã€‚è¯·**ç«‹å³**é€‰æ‹©ä¸€ä¸ªå·¥å…·æ‰§è¡Œï¼ˆAction: ...ï¼‰ï¼Œæˆ–è€…å¦‚æœä¿¡æ¯æ”¶é›†å®Œæˆï¼Œè¾“å‡º Final Answerã€‚",
                    })
            
            # ğŸ”¥ å¦‚æœå¾ªç¯ç»“æŸä½†æ²¡æœ‰ final_resultï¼Œå¼ºåˆ¶ LLM æ€»ç»“
            if not final_result and not self.is_cancelled and not error_message:
                await self.emit_thinking("ğŸ“ ä¿¡æ¯æ”¶é›†é˜¶æ®µç»“æŸï¼Œæ­£åœ¨ç”Ÿæˆæ€»ç»“...")
                
                # æ·»åŠ å¼ºåˆ¶æ€»ç»“çš„æç¤º
                self._conversation_history.append({
                    "role": "user",
                    "content": """ä¿¡æ¯æ”¶é›†é˜¶æ®µå·²ç»“æŸã€‚è¯·ç«‹å³è¾“å‡º Final Answerï¼Œæ€»ç»“ä½ æ”¶é›†åˆ°çš„æ‰€æœ‰ä¿¡æ¯ã€‚

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
```json
{
    "project_structure": {"directories": [...], "key_files": [...]},
    "tech_stack": {"languages": [...], "frameworks": [...], "databases": [...]},
    "entry_points": [{"type": "...", "file": "...", "description": "..."}],
    "high_risk_areas": ["file1.py", "file2.js"],
    "initial_findings": [{"title": "...", "description": "...", "file_path": "..."}],
    "summary": "é¡¹ç›®æ€»ç»“æè¿°"
}
```

Final Answer:""",
                })
                
                try:
                    summary_output, _ = await self.stream_llm_call(
                        self._conversation_history,
                        # ğŸ”¥ ä¸ä¼ é€’ temperature å’Œ max_tokensï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®
                    )
                    
                    if summary_output and summary_output.strip():
                        # è§£ææ€»ç»“è¾“å‡º
                        summary_text = summary_output.strip()
                        summary_text = re.sub(r'```json\s*', '', summary_text)
                        summary_text = re.sub(r'```\s*', '', summary_text)
                        final_result = AgentJsonParser.parse(
                            summary_text,
                            default=self._summarize_from_steps()
                        )
                except Exception as e:
                    logger.warning(f"[{self.name}] Failed to generate summary: {e}")
            
            # å¤„ç†ç»“æœ
            duration_ms = int((time.time() - start_time) * 1000)
            
            # ğŸ”¥ å¦‚æœè¢«å–æ¶ˆï¼Œè¿”å›å–æ¶ˆç»“æœ
            if self.is_cancelled:
                await self.emit_event(
                    "info",
                    f"ğŸ›‘ Recon Agent å·²å–æ¶ˆ: {self._iteration} è½®è¿­ä»£"
                )
                return AgentResult(
                    success=False,
                    error="ä»»åŠ¡å·²å–æ¶ˆ",
                    data=self._summarize_from_steps(),
                    iterations=self._iteration,
                    tool_calls=self._tool_calls,
                    tokens_used=self._total_tokens,
                    duration_ms=duration_ms,
                )
            
            # ğŸ”¥ å¦‚æœæœ‰é”™è¯¯ï¼Œè¿”å›å¤±è´¥ç»“æœ
            if error_message:
                await self.emit_event(
                    "error",
                    f"âŒ Recon Agent å¤±è´¥: {error_message}"
                )
                return AgentResult(
                    success=False,
                    error=error_message,
                    data=self._summarize_from_steps(),
                    iterations=self._iteration,
                    tool_calls=self._tool_calls,
                    tokens_used=self._total_tokens,
                    duration_ms=duration_ms,
                )
            
            # å¦‚æœæ²¡æœ‰æœ€ç»ˆç»“æœï¼Œä»å†å²ä¸­æ±‡æ€»
            if not final_result:
                final_result = self._summarize_from_steps()
            
            # ğŸ”¥ è®°å½•å·¥ä½œå’Œæ´å¯Ÿ
            self.record_work(f"å®Œæˆé¡¹ç›®ä¿¡æ¯æ”¶é›†ï¼Œå‘ç° {len(final_result.get('entry_points', []))} ä¸ªå…¥å£ç‚¹")
            self.record_work(f"è¯†åˆ«æŠ€æœ¯æ ˆ: {final_result.get('tech_stack', {})}")
            
            if final_result.get("high_risk_areas"):
                self.add_insight(f"å‘ç° {len(final_result['high_risk_areas'])} ä¸ªé«˜é£é™©åŒºåŸŸéœ€è¦é‡ç‚¹åˆ†æ")
            if final_result.get("initial_findings"):
                self.add_insight(f"åˆæ­¥å‘ç° {len(final_result['initial_findings'])} ä¸ªæ½œåœ¨é—®é¢˜")
            
            await self.emit_event(
                "info",
                f"Recon Agent å®Œæˆ: {self._iteration} è½®è¿­ä»£, {self._tool_calls} æ¬¡å·¥å…·è°ƒç”¨"
            )
            
            return AgentResult(
                success=True,
                data=final_result,
                iterations=self._iteration,
                tool_calls=self._tool_calls,
                tokens_used=self._total_tokens,
                duration_ms=duration_ms,
            )
            
        except Exception as e:
            logger.error(f"Recon Agent failed: {e}", exc_info=True)
            return AgentResult(success=False, error=str(e))
    
    def _summarize_from_steps(self) -> Dict[str, Any]:
        """ä»æ­¥éª¤ä¸­æ±‡æ€»ç»“æœ - å¢å¼ºç‰ˆï¼Œä» LLM æ€è€ƒè¿‡ç¨‹ä¸­æå–æ›´å¤šä¿¡æ¯"""
        # é»˜è®¤ç»“æœç»“æ„
        result = {
            "project_structure": {},
            "tech_stack": {
                "languages": [],
                "frameworks": [],
                "databases": [],
            },
            "entry_points": [],
            "high_risk_areas": [],
            "dependencies": {},
            "initial_findings": [],
            "summary": "",  # ğŸ”¥ æ–°å¢ï¼šæ±‡æ€» LLM çš„æ€è€ƒ
        }
        
        # ğŸ”¥ æ”¶é›†æ‰€æœ‰ LLM çš„æ€è€ƒå†…å®¹
        thoughts = []
        
        # ä»æ­¥éª¤çš„è§‚å¯Ÿç»“æœå’Œæ€è€ƒä¸­æå–ä¿¡æ¯
        for step in self._steps:
            # æ”¶é›†æ€è€ƒå†…å®¹
            if step.thought:
                thoughts.append(step.thought)
            
            if step.observation:
                # å°è¯•ä»è§‚å¯Ÿä¸­è¯†åˆ«æŠ€æœ¯æ ˆç­‰ä¿¡æ¯
                obs_lower = step.observation.lower()
                
                # è¯†åˆ«è¯­è¨€
                if "package.json" in obs_lower or ".js" in obs_lower or ".ts" in obs_lower:
                    result["tech_stack"]["languages"].append("JavaScript/TypeScript")
                if "requirements.txt" in obs_lower or "setup.py" in obs_lower or ".py" in obs_lower:
                    result["tech_stack"]["languages"].append("Python")
                if "go.mod" in obs_lower or ".go" in obs_lower:
                    result["tech_stack"]["languages"].append("Go")
                if "pom.xml" in obs_lower or ".java" in obs_lower:
                    result["tech_stack"]["languages"].append("Java")
                if ".php" in obs_lower:
                    result["tech_stack"]["languages"].append("PHP")
                if ".rb" in obs_lower or "gemfile" in obs_lower:
                    result["tech_stack"]["languages"].append("Ruby")
                
                # è¯†åˆ«æ¡†æ¶
                if "react" in obs_lower:
                    result["tech_stack"]["frameworks"].append("React")
                if "vue" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Vue")
                if "angular" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Angular")
                if "django" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Django")
                if "flask" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Flask")
                if "fastapi" in obs_lower:
                    result["tech_stack"]["frameworks"].append("FastAPI")
                if "express" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Express")
                if "spring" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Spring")
                if "streamlit" in obs_lower:
                    result["tech_stack"]["frameworks"].append("Streamlit")
                
                # è¯†åˆ«æ•°æ®åº“
                if "mysql" in obs_lower or "pymysql" in obs_lower:
                    result["tech_stack"]["databases"].append("MySQL")
                if "postgres" in obs_lower or "asyncpg" in obs_lower:
                    result["tech_stack"]["databases"].append("PostgreSQL")
                if "mongodb" in obs_lower or "pymongo" in obs_lower:
                    result["tech_stack"]["databases"].append("MongoDB")
                if "redis" in obs_lower:
                    result["tech_stack"]["databases"].append("Redis")
                if "sqlite" in obs_lower:
                    result["tech_stack"]["databases"].append("SQLite")
                
                # ğŸ”¥ è¯†åˆ«é«˜é£é™©åŒºåŸŸï¼ˆä»è§‚å¯Ÿä¸­æå–ï¼‰
                risk_keywords = ["api", "auth", "login", "password", "secret", "key", "token", 
                               "admin", "upload", "download", "exec", "eval", "sql", "query"]
                for keyword in risk_keywords:
                    if keyword in obs_lower:
                        # å°è¯•ä»è§‚å¯Ÿä¸­æå–æ–‡ä»¶è·¯å¾„
                        import re
                        file_matches = re.findall(r'[\w/]+\.(?:py|js|ts|java|php|go|rb)', step.observation)
                        for file_path in file_matches[:3]:  # é™åˆ¶æ•°é‡
                            if file_path not in result["high_risk_areas"]:
                                result["high_risk_areas"].append(file_path)
        
        # å»é‡
        result["tech_stack"]["languages"] = list(set(result["tech_stack"]["languages"]))
        result["tech_stack"]["frameworks"] = list(set(result["tech_stack"]["frameworks"]))
        result["tech_stack"]["databases"] = list(set(result["tech_stack"]["databases"]))
        result["high_risk_areas"] = list(set(result["high_risk_areas"]))[:20]  # é™åˆ¶æ•°é‡
        
        # ğŸ”¥ æ±‡æ€» LLM çš„æ€è€ƒä½œä¸º summary
        if thoughts:
            # å–æœ€åå‡ ä¸ªæ€è€ƒä½œä¸ºæ€»ç»“
            result["summary"] = "\n".join(thoughts[-3:])
        
        return result
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self._conversation_history
    
    def get_steps(self) -> List[ReconStep]:
        """è·å–æ‰§è¡Œæ­¥éª¤"""
        return self._steps
