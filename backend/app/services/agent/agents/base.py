"""
Agent åŸºç±»
å®šä¹‰ Agent çš„åŸºæœ¬æ¥å£å’Œé€šç”¨åŠŸèƒ½

æ ¸å¿ƒåŸåˆ™ï¼š
1. LLM æ˜¯ Agent çš„å¤§è„‘ï¼Œå…¨ç¨‹å‚ä¸å†³ç­–
2. Agent ä¹‹é—´é€šè¿‡ TaskHandoff ä¼ é€’ç»“æ„åŒ–ä¸Šä¸‹æ–‡
3. äº‹ä»¶åˆ†ä¸ºæµå¼äº‹ä»¶ï¼ˆå‰ç«¯å±•ç¤ºï¼‰å’ŒæŒä¹…åŒ–äº‹ä»¶ï¼ˆæ•°æ®åº“è®°å½•ï¼‰
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timezone
import asyncio
import logging
import uuid

logger = logging.getLogger(__name__)


class AgentType(Enum):
    """Agent ç±»å‹"""
    ORCHESTRATOR = "orchestrator"
    RECON = "recon"
    ANALYSIS = "analysis"
    VERIFICATION = "verification"


class AgentPattern(Enum):
    """Agent è¿è¡Œæ¨¡å¼"""
    REACT = "react"                    # ååº”å¼ï¼šæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
    PLAN_AND_EXECUTE = "plan_execute"  # è®¡åˆ’æ‰§è¡Œï¼šå…ˆè§„åˆ’åæ‰§è¡Œ


@dataclass
class AgentConfig:
    """Agent é…ç½®"""
    name: str
    agent_type: AgentType
    pattern: AgentPattern = AgentPattern.REACT
    
    # LLM é…ç½®
    model: Optional[str] = None
    temperature: float = 0.1
    max_tokens: int = 4096
    
    # æ‰§è¡Œé™åˆ¶
    max_iterations: int = 20
    timeout_seconds: int = 600
    
    # å·¥å…·é…ç½®
    tools: List[str] = field(default_factory=list)
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt: Optional[str] = None
    
    # å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentResult:
    """Agent æ‰§è¡Œç»“æœ"""
    success: bool
    data: Any = None
    error: Optional[str] = None
    
    # æ‰§è¡Œç»Ÿè®¡
    iterations: int = 0
    tool_calls: int = 0
    tokens_used: int = 0
    duration_ms: int = 0
    
    # ä¸­é—´ç»“æœ
    intermediate_steps: List[Dict[str, Any]] = field(default_factory=list)
    
    # å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # ğŸ”¥ åä½œä¿¡æ¯ - Agent ä¼ é€’ç»™ä¸‹ä¸€ä¸ª Agent çš„ç»“æ„åŒ–ä¿¡æ¯
    handoff: Optional["TaskHandoff"] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "success": self.success,
            "data": self.data,
            "error": self.error,
            "iterations": self.iterations,
            "tool_calls": self.tool_calls,
            "tokens_used": self.tokens_used,
            "duration_ms": self.duration_ms,
            "metadata": self.metadata,
            "handoff": self.handoff.to_dict() if self.handoff else None,
        }


@dataclass
class TaskHandoff:
    """
    ä»»åŠ¡äº¤æ¥åè®® - Agent ä¹‹é—´ä¼ é€’çš„ç»“æ„åŒ–ä¿¡æ¯
    
    è®¾è®¡åŸåˆ™ï¼š
    1. åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡è®©ä¸‹ä¸€ä¸ª Agent ç†è§£å‰åºå·¥ä½œ
    2. æä¾›æ˜ç¡®çš„å»ºè®®å’Œå…³æ³¨ç‚¹
    3. å¯ç›´æ¥è½¬æ¢ä¸º LLM å¯ç†è§£çš„ prompt
    """
    # åŸºæœ¬ä¿¡æ¯
    from_agent: str
    to_agent: str
    
    # å·¥ä½œæ‘˜è¦
    summary: str
    work_completed: List[str] = field(default_factory=list)
    
    # å…³é”®å‘ç°å’Œæ´å¯Ÿ
    key_findings: List[Dict[str, Any]] = field(default_factory=list)
    insights: List[str] = field(default_factory=list)
    
    # å»ºè®®å’Œå…³æ³¨ç‚¹
    suggested_actions: List[Dict[str, Any]] = field(default_factory=list)
    attention_points: List[str] = field(default_factory=list)
    priority_areas: List[str] = field(default_factory=list)
    
    # ä¸Šä¸‹æ–‡æ•°æ®
    context_data: Dict[str, Any] = field(default_factory=dict)
    
    # ç½®ä¿¡åº¦
    confidence: float = 0.8
    
    # æ—¶é—´æˆ³
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "from_agent": self.from_agent,
            "to_agent": self.to_agent,
            "summary": self.summary,
            "work_completed": self.work_completed,
            "key_findings": self.key_findings,
            "insights": self.insights,
            "suggested_actions": self.suggested_actions,
            "attention_points": self.attention_points,
            "priority_areas": self.priority_areas,
            "context_data": self.context_data,
            "confidence": self.confidence,
            "timestamp": self.timestamp.isoformat(),
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TaskHandoff":
        return cls(
            from_agent=data.get("from_agent", ""),
            to_agent=data.get("to_agent", ""),
            summary=data.get("summary", ""),
            work_completed=data.get("work_completed", []),
            key_findings=data.get("key_findings", []),
            insights=data.get("insights", []),
            suggested_actions=data.get("suggested_actions", []),
            attention_points=data.get("attention_points", []),
            priority_areas=data.get("priority_areas", []),
            context_data=data.get("context_data", {}),
            confidence=data.get("confidence", 0.8),
        )
    
    def to_prompt_context(self) -> str:
        """
        è½¬æ¢ä¸º LLM å¯ç†è§£çš„ä¸Šä¸‹æ–‡æ ¼å¼
        è¿™æ˜¯å…³é”®ï¼è®© LLM èƒ½å¤Ÿç†è§£å‰åº Agent çš„å·¥ä½œ
        """
        lines = [
            f"## æ¥è‡ª {self.from_agent} Agent çš„ä»»åŠ¡äº¤æ¥",
            "",
            f"### å·¥ä½œæ‘˜è¦",
            self.summary,
            "",
        ]
        
        if self.work_completed:
            lines.append("### å·²å®Œæˆçš„å·¥ä½œ")
            for work in self.work_completed:
                lines.append(f"- {work}")
            lines.append("")
        
        if self.key_findings:
            lines.append("### å…³é”®å‘ç°")
            for i, finding in enumerate(self.key_findings[:15], 1):
                severity = finding.get("severity", "medium")
                title = finding.get("title", "Unknown")
                file_path = finding.get("file_path", "")
                lines.append(f"{i}. [{severity.upper()}] {title}")
                if file_path:
                    lines.append(f"   ä½ç½®: {file_path}:{finding.get('line_start', '')}")
                if finding.get("description"):
                    lines.append(f"   æè¿°: {finding['description'][:100]}")
            lines.append("")
        
        if self.insights:
            lines.append("### æ´å¯Ÿå’Œåˆ†æ")
            for insight in self.insights:
                lines.append(f"- {insight}")
            lines.append("")
        
        if self.suggested_actions:
            lines.append("### å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
            for action in self.suggested_actions:
                action_type = action.get("type", "general")
                description = action.get("description", "")
                priority = action.get("priority", "medium")
                lines.append(f"- [{priority.upper()}] {action_type}: {description}")
            lines.append("")
        
        if self.attention_points:
            lines.append("### âš ï¸ éœ€è¦ç‰¹åˆ«å…³æ³¨")
            for point in self.attention_points:
                lines.append(f"- {point}")
            lines.append("")
        
        if self.priority_areas:
            lines.append("### ä¼˜å…ˆåˆ†æåŒºåŸŸ")
            for area in self.priority_areas:
                lines.append(f"- {area}")
        
        return "\n".join(lines)


class BaseAgent(ABC):
    """
    Agent åŸºç±»
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. LLM æ˜¯ Agent çš„å¤§è„‘ï¼Œå…¨ç¨‹å‚ä¸å†³ç­–
    2. æ‰€æœ‰æ—¥å¿—åº”è¯¥åæ˜  LLM çš„æ€è€ƒè¿‡ç¨‹
    3. å·¥å…·è°ƒç”¨æ˜¯ LLM çš„å†³ç­–ç»“æœ
    
    åä½œåŸåˆ™ï¼š
    1. é€šè¿‡ TaskHandoff æ¥æ”¶å‰åº Agent çš„ä¸Šä¸‹æ–‡
    2. æ‰§è¡Œå®Œæˆåç”Ÿæˆ TaskHandoff ä¼ é€’ç»™ä¸‹ä¸€ä¸ª Agent
    3. æ´å¯Ÿå’Œå‘ç°åº”è¯¥ç»“æ„åŒ–è®°å½•
    """
    
    def __init__(
        self,
        config: AgentConfig,
        llm_service,
        tools: Dict[str, Any],
        event_emitter=None,
    ):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            config: Agent é…ç½®
            llm_service: LLM æœåŠ¡
            tools: å¯ç”¨å·¥å…·å­—å…¸
            event_emitter: äº‹ä»¶å‘å°„å™¨
        """
        self.config = config
        self.llm_service = llm_service
        self.tools = tools
        self.event_emitter = event_emitter
        
        # è¿è¡ŒçŠ¶æ€
        self._iteration = 0
        self._total_tokens = 0
        self._tool_calls = 0
        self._cancelled = False
        
        # ğŸ”¥ åä½œçŠ¶æ€
        self._incoming_handoff: Optional[TaskHandoff] = None
        self._insights: List[str] = []  # æ”¶é›†çš„æ´å¯Ÿ
        self._work_completed: List[str] = []  # å®Œæˆçš„å·¥ä½œè®°å½•
    
    @property
    def name(self) -> str:
        return self.config.name
    
    @property
    def agent_type(self) -> AgentType:
        return self.config.agent_type
    
    @abstractmethod
    async def run(self, input_data: Dict[str, Any]) -> AgentResult:
        """
        æ‰§è¡Œ Agent ä»»åŠ¡
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            Agent æ‰§è¡Œç»“æœ
        """
        pass
    
    def cancel(self):
        """å–æ¶ˆæ‰§è¡Œ"""
        self._cancelled = True
    
    @property
    def is_cancelled(self) -> bool:
        return self._cancelled
    
    # ============ åä½œæ–¹æ³• ============
    
    def receive_handoff(self, handoff: TaskHandoff):
        """
        æ¥æ”¶æ¥è‡ªå‰åº Agent çš„ä»»åŠ¡äº¤æ¥
        
        Args:
            handoff: ä»»åŠ¡äº¤æ¥å¯¹è±¡
        """
        self._incoming_handoff = handoff
        logger.info(
            f"[{self.name}] Received handoff from {handoff.from_agent}: "
            f"{handoff.summary[:50]}..."
        )
    
    def get_handoff_context(self) -> str:
        """
        è·å–äº¤æ¥ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ„å»º LLM promptï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not self._incoming_handoff:
            return ""
        return self._incoming_handoff.to_prompt_context()
    
    def add_insight(self, insight: str):
        """è®°å½•æ´å¯Ÿ"""
        self._insights.append(insight)
    
    def record_work(self, work: str):
        """è®°å½•å®Œæˆçš„å·¥ä½œ"""
        self._work_completed.append(work)
    
    def create_handoff(
        self,
        to_agent: str,
        summary: str,
        key_findings: List[Dict[str, Any]] = None,
        suggested_actions: List[Dict[str, Any]] = None,
        attention_points: List[str] = None,
        priority_areas: List[str] = None,
        context_data: Dict[str, Any] = None,
    ) -> TaskHandoff:
        """
        åˆ›å»ºä»»åŠ¡äº¤æ¥
        
        Args:
            to_agent: ç›®æ ‡ Agent
            summary: å·¥ä½œæ‘˜è¦
            key_findings: å…³é”®å‘ç°
            suggested_actions: å»ºè®®çš„è¡ŒåŠ¨
            attention_points: éœ€è¦å…³æ³¨çš„ç‚¹
            priority_areas: ä¼˜å…ˆåˆ†æåŒºåŸŸ
            context_data: ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            TaskHandoff å¯¹è±¡
        """
        return TaskHandoff(
            from_agent=self.name,
            to_agent=to_agent,
            summary=summary,
            work_completed=self._work_completed.copy(),
            key_findings=key_findings or [],
            insights=self._insights.copy(),
            suggested_actions=suggested_actions or [],
            attention_points=attention_points or [],
            priority_areas=priority_areas or [],
            context_data=context_data or {},
        )
    
    def build_prompt_with_handoff(self, base_prompt: str) -> str:
        """
        æ„å»ºåŒ…å«äº¤æ¥ä¸Šä¸‹æ–‡çš„ prompt
        
        Args:
            base_prompt: åŸºç¡€ prompt
            
        Returns:
            å¢å¼ºåçš„ prompt
        """
        handoff_context = self.get_handoff_context()
        if not handoff_context:
            return base_prompt
        
        return f"""{base_prompt}

---
## å‰åº Agent äº¤æ¥ä¿¡æ¯

{handoff_context}

---
è¯·åŸºäºä»¥ä¸Šæ¥è‡ªå‰åº Agent çš„ä¿¡æ¯ï¼Œç»“åˆä½ çš„ä¸“ä¸šèƒ½åŠ›å¼€å±•å·¥ä½œã€‚
"""
    
    # ============ æ ¸å¿ƒäº‹ä»¶å‘å°„æ–¹æ³• ============
    
    async def emit_event(
        self,
        event_type: str,
        message: str,
        **kwargs
    ):
        """å‘å°„äº‹ä»¶"""
        if self.event_emitter:
            from ..event_manager import AgentEventData
            await self.event_emitter.emit(AgentEventData(
                event_type=event_type,
                message=message,
                **kwargs
            ))
    
    # ============ LLM æ€è€ƒç›¸å…³äº‹ä»¶ ============
    
    async def emit_thinking(self, message: str):
        """å‘å°„ LLM æ€è€ƒäº‹ä»¶"""
        await self.emit_event("thinking", f"[{self.name}] {message}")
    
    async def emit_llm_start(self, iteration: int):
        """å‘å°„ LLM å¼€å§‹æ€è€ƒäº‹ä»¶"""
        await self.emit_event(
            "llm_start",
            f"[{self.name}] ç¬¬ {iteration} è½®è¿­ä»£å¼€å§‹",
            metadata={"iteration": iteration}
        )
    
    async def emit_llm_thought(self, thought: str, iteration: int):
        """å‘å°„ LLM æ€è€ƒå†…å®¹äº‹ä»¶ - è¿™æ˜¯æ ¸å¿ƒï¼å±•ç¤º LLM åœ¨æƒ³ä»€ä¹ˆ"""
        # æˆªæ–­è¿‡é•¿çš„æ€è€ƒå†…å®¹
        display_thought = thought[:500] + "..." if len(thought) > 500 else thought
        await self.emit_event(
            "llm_thought",
            f"[{self.name}] æ€è€ƒ: {display_thought}",
            metadata={
                "thought": thought,
                "iteration": iteration,
            }
        )
    
    async def emit_thinking_start(self):
        """å‘å°„å¼€å§‹æ€è€ƒäº‹ä»¶ï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰"""
        await self.emit_event("thinking_start", f"[{self.name}] å¼€å§‹æ€è€ƒ...")
    
    async def emit_thinking_token(self, token: str, accumulated: str):
        """å‘å°„æ€è€ƒ token äº‹ä»¶ï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰"""
        await self.emit_event(
            "thinking_token",
            "",  # ä¸éœ€è¦ messageï¼Œå‰ç«¯ä» metadata è·å–
            metadata={
                "token": token,
                "accumulated": accumulated,
            }
        )
    
    async def emit_thinking_end(self, full_response: str):
        """å‘å°„æ€è€ƒç»“æŸäº‹ä»¶ï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰"""
        await self.emit_event(
            "thinking_end",
            f"[{self.name}] æ€è€ƒå®Œæˆ",
            metadata={"accumulated": full_response}
        )
    
    async def emit_llm_decision(self, decision: str, reason: str = ""):
        """å‘å°„ LLM å†³ç­–äº‹ä»¶ - å±•ç¤º LLM åšäº†ä»€ä¹ˆå†³å®š"""
        await self.emit_event(
            "llm_decision",
            f"[{self.name}] å†³ç­–: {decision}" + (f" ({reason})" if reason else ""),
            metadata={
                "decision": decision,
                "reason": reason,
            }
        )
    
    async def emit_llm_complete(self, result_summary: str, tokens_used: int):
        """å‘å°„ LLM å®Œæˆäº‹ä»¶"""
        await self.emit_event(
            "llm_complete",
            f"[{self.name}] å®Œæˆ: {result_summary} (æ¶ˆè€— {tokens_used} tokens)",
            metadata={
                "tokens_used": tokens_used,
            }
        )
    
    async def emit_llm_action(self, action: str, action_input: Dict):
        """å‘å°„ LLM åŠ¨ä½œå†³ç­–äº‹ä»¶"""
        await self.emit_event(
            "llm_action",
            f"[{self.name}] æ‰§è¡ŒåŠ¨ä½œ: {action}",
            metadata={
                "action": action,
                "action_input": action_input,
            }
        )
    
    async def emit_llm_observation(self, observation: str):
        """å‘å°„ LLM è§‚å¯Ÿäº‹ä»¶"""
        # æˆªæ–­è¿‡é•¿çš„è§‚å¯Ÿç»“æœ
        display_obs = observation[:300] + "..." if len(observation) > 300 else observation
        await self.emit_event(
            "llm_observation",
            f"[{self.name}] è§‚å¯Ÿç»“æœ: {display_obs}",
            metadata={
                "observation": observation[:2000],  # é™åˆ¶å­˜å‚¨é•¿åº¦
            }
        )
    
    # ============ å·¥å…·è°ƒç”¨ç›¸å…³äº‹ä»¶ ============
    
    async def emit_tool_call(self, tool_name: str, tool_input: Dict):
        """å‘å°„å·¥å…·è°ƒç”¨äº‹ä»¶"""
        await self.emit_event(
            "tool_call",
            f"[{self.name}] è°ƒç”¨å·¥å…·: {tool_name}",
            tool_name=tool_name,
            tool_input=tool_input,
        )
    
    async def emit_tool_result(self, tool_name: str, result: str, duration_ms: int):
        """å‘å°„å·¥å…·ç»“æœäº‹ä»¶"""
        await self.emit_event(
            "tool_result",
            f"[{self.name}] å·¥å…· {tool_name} å®Œæˆ ({duration_ms}ms)",
            tool_name=tool_name,
            tool_duration_ms=duration_ms,
        )
    
    # ============ å‘ç°ç›¸å…³äº‹ä»¶ ============
    
    async def emit_finding(self, title: str, severity: str, vuln_type: str, file_path: str = ""):
        """å‘å°„æ¼æ´å‘ç°äº‹ä»¶"""
        severity_emoji = {
            "critical": "ğŸ”´",
            "high": "ğŸŸ ",
            "medium": "ğŸŸ¡",
            "low": "ğŸŸ¢",
        }.get(severity.lower(), "âšª")
        
        await self.emit_event(
            "finding",
            f"{severity_emoji} [{self.name}] å‘ç°æ¼æ´: [{severity.upper()}] {title}\n   ç±»å‹: {vuln_type}\n   ä½ç½®: {file_path}",
            metadata={
                "title": title,
                "severity": severity,
                "vulnerability_type": vuln_type,
                "file_path": file_path,
            }
        )
    
    # ============ é€šç”¨å·¥å…·æ–¹æ³• ============
    
    async def call_tool(self, tool_name: str, **kwargs) -> Any:
        """
        è°ƒç”¨å·¥å…·
        
        Args:
            tool_name: å·¥å…·åç§°
            **kwargs: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        tool = self.tools.get(tool_name)
        if not tool:
            logger.warning(f"Tool not found: {tool_name}")
            return None
        
        self._tool_calls += 1
        await self.emit_tool_call(tool_name, kwargs)
        
        import time
        start = time.time()
        
        result = await tool.execute(**kwargs)
        
        duration_ms = int((time.time() - start) * 1000)
        await self.emit_tool_result(tool_name, str(result.data)[:500], duration_ms)
        
        return result
    
    async def call_llm(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å¯ç”¨å·¥å…·æè¿°
            
        Returns:
            LLM å“åº”
        """
        self._iteration += 1
        
        try:
            response = await self.llm_service.chat_completion(
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                tools=tools,
            )
            
            if response.get("usage"):
                self._total_tokens += response["usage"].get("total_tokens", 0)
            
            return response
            
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise
    
    def get_tool_descriptions(self) -> List[Dict[str, Any]]:
        """è·å–å·¥å…·æè¿°ï¼ˆç”¨äº LLMï¼‰"""
        descriptions = []
        
        for name, tool in self.tools.items():
            if name.startswith("_"):
                continue
            
            desc = {
                "type": "function",
                "function": {
                    "name": name,
                    "description": tool.description,
                }
            }
            
            # æ·»åŠ å‚æ•° schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                desc["function"]["parameters"] = tool.args_schema.schema()
            
            descriptions.append(desc)
        
        return descriptions
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡"""
        return {
            "agent": self.name,
            "type": self.agent_type.value,
            "iterations": self._iteration,
            "tool_calls": self._tool_calls,
            "tokens_used": self._total_tokens,
        }
    
    # ============ ç»Ÿä¸€çš„æµå¼ LLM è°ƒç”¨ ============
    
    async def stream_llm_call(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.1,
        max_tokens: int = 2048,
    ) -> Tuple[str, int]:
        """
        ç»Ÿä¸€çš„æµå¼ LLM è°ƒç”¨æ–¹æ³•
        
        æ‰€æœ‰ Agent å…±ç”¨æ­¤æ–¹æ³•ï¼Œé¿å…é‡å¤ä»£ç 
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ token æ•°
            
        Returns:
            (å®Œæ•´å“åº”å†…å®¹, tokenæ•°é‡)
        """
        accumulated = ""
        total_tokens = 0
        
        await self.emit_thinking_start()
        
        try:
            async for chunk in self.llm_service.chat_completion_stream(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            ):
                # æ£€æŸ¥å–æ¶ˆ
                if self.is_cancelled:
                    break
                
                if chunk["type"] == "token":
                    token = chunk["content"]
                    accumulated = chunk["accumulated"]
                    await self.emit_thinking_token(token, accumulated)
                    
                elif chunk["type"] == "done":
                    accumulated = chunk["content"]
                    if chunk.get("usage"):
                        total_tokens = chunk["usage"].get("total_tokens", 0)
                    break
                    
                elif chunk["type"] == "error":
                    accumulated = chunk.get("accumulated", "")
                    logger.error(f"Stream error: {chunk.get('error')}")
                    break
                    
        except asyncio.CancelledError:
            logger.info(f"[{self.name}] LLM call cancelled")
            raise
        finally:
            await self.emit_thinking_end(accumulated)
        
        return accumulated, total_tokens
    
    async def execute_tool(self, tool_name: str, tool_input: Dict) -> str:
        """
        ç»Ÿä¸€çš„å·¥å…·æ‰§è¡Œæ–¹æ³•
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœå­—ç¬¦ä¸²
        """
        tool = self.tools.get(tool_name)
        
        if not tool:
            return f"é”™è¯¯: å·¥å…· '{tool_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨å·¥å…·: {list(self.tools.keys())}"
        
        try:
            self._tool_calls += 1
            await self.emit_tool_call(tool_name, tool_input)
            
            import time
            start = time.time()
            
            result = await tool.execute(**tool_input)
            
            duration_ms = int((time.time() - start) * 1000)
            await self.emit_tool_result(tool_name, str(result.data)[:200], duration_ms)
            
            if result.success:
                output = str(result.data)
                
                # åŒ…å« metadata ä¸­çš„é¢å¤–ä¿¡æ¯
                if result.metadata:
                    if "issues" in result.metadata:
                        import json
                        output += f"\n\nå‘ç°çš„é—®é¢˜:\n{json.dumps(result.metadata['issues'], ensure_ascii=False, indent=2)}"
                    if "findings" in result.metadata:
                        import json
                        output += f"\n\nå‘ç°:\n{json.dumps(result.metadata['findings'][:10], ensure_ascii=False, indent=2)}"
                
                # æˆªæ–­è¿‡é•¿è¾“å‡º
                if len(output) > 6000:
                    output = output[:6000] + f"\n\n... [è¾“å‡ºå·²æˆªæ–­ï¼Œå…± {len(str(result.data))} å­—ç¬¦]"
                return output
            else:
                return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result.error}"
                
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            return f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
    
    def get_tools_description(self) -> str:
        """ç”Ÿæˆå·¥å…·æè¿°æ–‡æœ¬ï¼ˆç”¨äº promptï¼‰"""
        tools_info = []
        for name, tool in self.tools.items():
            if name.startswith("_"):
                continue
            desc = f"- {name}: {getattr(tool, 'description', 'No description')}"
            tools_info.append(desc)
        return "\n".join(tools_info)
