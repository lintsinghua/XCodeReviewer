"""
Agent åŸºç±»
å®šä¹‰ Agent çš„åŸºæœ¬æ¥å£å’Œé€šç”¨åŠŸèƒ½

æ ¸å¿ƒåŸåˆ™ï¼š
1. LLM æ˜¯ Agent çš„å¤§è„‘ï¼Œå…¨ç¨‹å‚ä¸å†³ç­–
2. Agent ä¹‹é—´é€šè¿‡ TaskHandoff ä¼ é€’ç»“æ„åŒ–ä¸Šä¸‹æ–‡
3. äº‹ä»¶åˆ†ä¸ºæµå¼äº‹ä»¶ï¼ˆå‰ç«¯å±•ç¤ºï¼‰å’ŒæŒä¹…åŒ–äº‹ä»¶ï¼ˆæ•°æ®åº“è®°å½•ï¼‰
4. æ”¯æŒåŠ¨æ€Agentæ ‘å’Œä¸“ä¸šçŸ¥è¯†æ¨¡å—
5. å®Œæ•´çš„çŠ¶æ€ç®¡ç†å’ŒAgenté—´é€šä¿¡
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timezone
import asyncio
import logging
import uuid

from ..core.state import AgentState, AgentStatus
from ..core.registry import agent_registry
from ..core.message import message_bus, MessageType, AgentMessage

logger = logging.getLogger(__name__)


class AgentType(Enum):
    """Agent ç±»å‹"""
    ORCHESTRATOR = "orchestrator"
    RECON = "recon"
    ANALYSIS = "analysis"
    VERIFICATION = "verification"


class AgentPattern(Enum):
    """Agent è¿è¡Œæ¨¡å¼"""
    REACT = "react"                    # ååº”å¼ï¼šæ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿå¾ªç¯
    PLAN_AND_EXECUTE = "plan_execute"  # è®¡åˆ’æ‰§è¡Œï¼šå…ˆè§„åˆ’åæ‰§è¡Œ


@dataclass
class AgentConfig:
    """Agent é…ç½®"""
    name: str
    agent_type: AgentType
    pattern: AgentPattern = AgentPattern.REACT
    
    # LLM é…ç½®
    model: Optional[str] = None
    temperature: float = 0.1
    max_tokens: int = 4096
    
    # æ‰§è¡Œé™åˆ¶
    max_iterations: int = 20
    timeout_seconds: int = 600
    
    # å·¥å…·é…ç½®
    tools: List[str] = field(default_factory=list)
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt: Optional[str] = None
    
    # å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentResult:
    """Agent æ‰§è¡Œç»“æœ"""
    success: bool
    data: Any = None
    error: Optional[str] = None
    
    # æ‰§è¡Œç»Ÿè®¡
    iterations: int = 0
    tool_calls: int = 0
    tokens_used: int = 0
    duration_ms: int = 0
    
    # ä¸­é—´ç»“æœ
    intermediate_steps: List[Dict[str, Any]] = field(default_factory=list)
    
    # å…ƒæ•°æ®
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # ğŸ”¥ åä½œä¿¡æ¯ - Agent ä¼ é€’ç»™ä¸‹ä¸€ä¸ª Agent çš„ç»“æ„åŒ–ä¿¡æ¯
    handoff: Optional["TaskHandoff"] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "success": self.success,
            "data": self.data,
            "error": self.error,
            "iterations": self.iterations,
            "tool_calls": self.tool_calls,
            "tokens_used": self.tokens_used,
            "duration_ms": self.duration_ms,
            "metadata": self.metadata,
            "handoff": self.handoff.to_dict() if self.handoff else None,
        }


@dataclass
class TaskHandoff:
    """
    ä»»åŠ¡äº¤æ¥åè®® - Agent ä¹‹é—´ä¼ é€’çš„ç»“æ„åŒ–ä¿¡æ¯
    
    è®¾è®¡åŸåˆ™ï¼š
    1. åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡è®©ä¸‹ä¸€ä¸ª Agent ç†è§£å‰åºå·¥ä½œ
    2. æä¾›æ˜ç¡®çš„å»ºè®®å’Œå…³æ³¨ç‚¹
    3. å¯ç›´æ¥è½¬æ¢ä¸º LLM å¯ç†è§£çš„ prompt
    """
    # åŸºæœ¬ä¿¡æ¯
    from_agent: str
    to_agent: str
    
    # å·¥ä½œæ‘˜è¦
    summary: str
    work_completed: List[str] = field(default_factory=list)
    
    # å…³é”®å‘ç°å’Œæ´å¯Ÿ
    key_findings: List[Dict[str, Any]] = field(default_factory=list)
    insights: List[str] = field(default_factory=list)
    
    # å»ºè®®å’Œå…³æ³¨ç‚¹
    suggested_actions: List[Dict[str, Any]] = field(default_factory=list)
    attention_points: List[str] = field(default_factory=list)
    priority_areas: List[str] = field(default_factory=list)
    
    # ä¸Šä¸‹æ–‡æ•°æ®
    context_data: Dict[str, Any] = field(default_factory=dict)
    
    # ç½®ä¿¡åº¦
    confidence: float = 0.8
    
    # æ—¶é—´æˆ³
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "from_agent": self.from_agent,
            "to_agent": self.to_agent,
            "summary": self.summary,
            "work_completed": self.work_completed,
            "key_findings": self.key_findings,
            "insights": self.insights,
            "suggested_actions": self.suggested_actions,
            "attention_points": self.attention_points,
            "priority_areas": self.priority_areas,
            "context_data": self.context_data,
            "confidence": self.confidence,
            "timestamp": self.timestamp.isoformat(),
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "TaskHandoff":
        return cls(
            from_agent=data.get("from_agent", ""),
            to_agent=data.get("to_agent", ""),
            summary=data.get("summary", ""),
            work_completed=data.get("work_completed", []),
            key_findings=data.get("key_findings", []),
            insights=data.get("insights", []),
            suggested_actions=data.get("suggested_actions", []),
            attention_points=data.get("attention_points", []),
            priority_areas=data.get("priority_areas", []),
            context_data=data.get("context_data", {}),
            confidence=data.get("confidence", 0.8),
        )
    
    def to_prompt_context(self) -> str:
        """
        è½¬æ¢ä¸º LLM å¯ç†è§£çš„ä¸Šä¸‹æ–‡æ ¼å¼
        è¿™æ˜¯å…³é”®ï¼è®© LLM èƒ½å¤Ÿç†è§£å‰åº Agent çš„å·¥ä½œ
        """
        lines = [
            f"## æ¥è‡ª {self.from_agent} Agent çš„ä»»åŠ¡äº¤æ¥",
            "",
            f"### å·¥ä½œæ‘˜è¦",
            self.summary,
            "",
        ]
        
        if self.work_completed:
            lines.append("### å·²å®Œæˆçš„å·¥ä½œ")
            for work in self.work_completed:
                lines.append(f"- {work}")
            lines.append("")
        
        if self.key_findings:
            lines.append("### å…³é”®å‘ç°")
            for i, finding in enumerate(self.key_findings[:15], 1):
                severity = finding.get("severity", "medium")
                title = finding.get("title", "Unknown")
                file_path = finding.get("file_path", "")
                lines.append(f"{i}. [{severity.upper()}] {title}")
                if file_path:
                    lines.append(f"   ä½ç½®: {file_path}:{finding.get('line_start', '')}")
                if finding.get("description"):
                    lines.append(f"   æè¿°: {finding['description'][:100]}")
            lines.append("")
        
        if self.insights:
            lines.append("### æ´å¯Ÿå’Œåˆ†æ")
            for insight in self.insights:
                lines.append(f"- {insight}")
            lines.append("")
        
        if self.suggested_actions:
            lines.append("### å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
            for action in self.suggested_actions:
                action_type = action.get("type", "general")
                description = action.get("description", "")
                priority = action.get("priority", "medium")
                lines.append(f"- [{priority.upper()}] {action_type}: {description}")
            lines.append("")
        
        if self.attention_points:
            lines.append("### âš ï¸ éœ€è¦ç‰¹åˆ«å…³æ³¨")
            for point in self.attention_points:
                lines.append(f"- {point}")
            lines.append("")
        
        if self.priority_areas:
            lines.append("### ä¼˜å…ˆåˆ†æåŒºåŸŸ")
            for area in self.priority_areas:
                lines.append(f"- {area}")
        
        return "\n".join(lines)


class BaseAgent(ABC):
    """
    Agent åŸºç±»
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. LLM æ˜¯ Agent çš„å¤§è„‘ï¼Œå…¨ç¨‹å‚ä¸å†³ç­–
    2. æ‰€æœ‰æ—¥å¿—åº”è¯¥åæ˜  LLM çš„æ€è€ƒè¿‡ç¨‹
    3. å·¥å…·è°ƒç”¨æ˜¯ LLM çš„å†³ç­–ç»“æœ
    
    åä½œåŸåˆ™ï¼š
    1. é€šè¿‡ TaskHandoff æ¥æ”¶å‰åº Agent çš„ä¸Šä¸‹æ–‡
    2. æ‰§è¡Œå®Œæˆåç”Ÿæˆ TaskHandoff ä¼ é€’ç»™ä¸‹ä¸€ä¸ª Agent
    3. æ´å¯Ÿå’Œå‘ç°åº”è¯¥ç»“æ„åŒ–è®°å½•
    
    åŠ¨æ€Agentæ ‘ï¼š
    1. æ”¯æŒåŠ¨æ€åˆ›å»ºå­Agent
    2. Agenté—´é€šè¿‡æ¶ˆæ¯æ€»çº¿é€šä¿¡
    3. å®Œæ•´çš„çŠ¶æ€ç®¡ç†å’Œç”Ÿå‘½å‘¨æœŸ
    """
    
    def __init__(
        self,
        config: AgentConfig,
        llm_service,
        tools: Dict[str, Any],
        event_emitter=None,
        parent_id: Optional[str] = None,
        knowledge_modules: Optional[List[str]] = None,
    ):
        """
        åˆå§‹åŒ– Agent
        
        Args:
            config: Agent é…ç½®
            llm_service: LLM æœåŠ¡
            tools: å¯ç”¨å·¥å…·å­—å…¸
            event_emitter: äº‹ä»¶å‘å°„å™¨
            parent_id: çˆ¶Agent IDï¼ˆç”¨äºåŠ¨æ€Agentæ ‘ï¼‰
            knowledge_modules: è¦åŠ è½½çš„çŸ¥è¯†æ¨¡å—
        """
        self.config = config
        self.llm_service = llm_service
        self.tools = tools
        self.event_emitter = event_emitter
        self.parent_id = parent_id
        self.knowledge_modules = knowledge_modules or []
        
        # ğŸ”¥ ç”Ÿæˆå”¯ä¸€ID
        self._agent_id = f"agent_{uuid.uuid4().hex[:8]}"
        
        # ğŸ”¥ å¢å¼ºçš„çŠ¶æ€ç®¡ç†
        self._state = AgentState(
            agent_id=self._agent_id,
            agent_name=config.name,
            agent_type=config.agent_type.value,
            parent_id=parent_id,
            max_iterations=config.max_iterations,
            knowledge_modules=self.knowledge_modules,
        )
        
        # è¿è¡ŒçŠ¶æ€ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        self._iteration = 0
        self._total_tokens = 0
        self._tool_calls = 0
        self._cancelled = False
        
        # ğŸ”¥ åä½œçŠ¶æ€
        self._incoming_handoff: Optional[TaskHandoff] = None
        self._insights: List[str] = []  # æ”¶é›†çš„æ´å¯Ÿ
        self._work_completed: List[str] = []  # å®Œæˆçš„å·¥ä½œè®°å½•
        
        # ğŸ”¥ æ˜¯å¦å·²æ³¨å†Œåˆ°æ³¨å†Œè¡¨
        self._registered = False
        
        # ğŸ”¥ åŠ è½½çŸ¥è¯†æ¨¡å—åˆ°ç³»ç»Ÿæç¤ºè¯
        if self.knowledge_modules:
            self._load_knowledge_modules()
    
    def _register_to_registry(self, task: Optional[str] = None) -> None:
        """æ³¨å†Œåˆ°Agentæ³¨å†Œè¡¨ï¼ˆå»¶è¿Ÿæ³¨å†Œï¼Œåœ¨runæ—¶è°ƒç”¨ï¼‰"""
        logger.debug(f"[AgentTree] _register_to_registry è¢«è°ƒç”¨: {self.config.name} (id={self._agent_id}, parent={self.parent_id}, _registered={self._registered})")
        
        if self._registered:
            logger.debug(f"[AgentTree] {self.config.name} å·²æ³¨å†Œï¼Œè·³è¿‡ (id={self._agent_id})")
            return
        
        logger.debug(f"[AgentTree] æ­£åœ¨æ³¨å†Œ Agent: {self.config.name} (id={self._agent_id}, parent={self.parent_id})")
        
        agent_registry.register_agent(
            agent_id=self._agent_id,
            agent_name=self.config.name,
            agent_type=self.config.agent_type.value,
            task=task or self._state.task or "Initializing",
            parent_id=self.parent_id,
            agent_instance=self,
            state=self._state,
            knowledge_modules=self.knowledge_modules,
        )
        
        # åˆ›å»ºæ¶ˆæ¯é˜Ÿåˆ—
        message_bus.create_queue(self._agent_id)
        self._registered = True
        
        tree = agent_registry.get_agent_tree()
        logger.debug(f"[AgentTree] Agent æ³¨å†Œå®Œæˆ: {self.config.name}, å½“å‰æ ‘èŠ‚ç‚¹æ•°: {len(tree['nodes'])}")
    
    def set_parent_id(self, parent_id: str) -> None:
        """è®¾ç½®çˆ¶Agent IDï¼ˆåœ¨è°ƒåº¦æ—¶è°ƒç”¨ï¼‰"""
        self.parent_id = parent_id
        self._state.parent_id = parent_id
    
    def _load_knowledge_modules(self) -> None:
        """åŠ è½½çŸ¥è¯†æ¨¡å—åˆ°ç³»ç»Ÿæç¤ºè¯"""
        if not self.knowledge_modules:
            return
        
        try:
            from ..knowledge import knowledge_loader
            
            enhanced_prompt = knowledge_loader.build_system_prompt_with_modules(
                self.config.system_prompt or "",
                self.knowledge_modules,
            )
            self.config.system_prompt = enhanced_prompt
            
            logger.info(f"[{self.name}] Loaded knowledge modules: {self.knowledge_modules}")
        except Exception as e:
            logger.warning(f"Failed to load knowledge modules: {e}")
    
    @property
    def name(self) -> str:
        return self.config.name
    
    @property
    def agent_id(self) -> str:
        return self._agent_id
    
    @property
    def state(self) -> AgentState:
        return self._state
    
    @property
    def agent_type(self) -> AgentType:
        return self.config.agent_type
    
    # ============ Agenté—´æ¶ˆæ¯å¤„ç† ============
    
    def check_messages(self) -> List[AgentMessage]:
        """
        æ£€æŸ¥å¹¶å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯
        
        Returns:
            æœªè¯»æ¶ˆæ¯åˆ—è¡¨
        """
        messages = message_bus.get_messages(
            self._agent_id,
            unread_only=True,
            mark_as_read=True,
        )
        
        for msg in messages:
            # å¤„ç†æ¶ˆæ¯
            if msg.from_agent == "user":
                # ç”¨æˆ·æ¶ˆæ¯ç›´æ¥æ·»åŠ åˆ°å¯¹è¯å†å²
                self._state.add_message("user", msg.content)
            else:
                # Agenté—´æ¶ˆæ¯ä½¿ç”¨XMLæ ¼å¼
                self._state.add_message("user", msg.to_xml())
            
            # å¦‚æœåœ¨ç­‰å¾…çŠ¶æ€ï¼Œæ¢å¤æ‰§è¡Œ
            if self._state.is_waiting_for_input():
                self._state.resume_from_waiting()
                agent_registry.update_agent_status(self._agent_id, "running")
        
        return messages
    
    def has_pending_messages(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„æ¶ˆæ¯"""
        return message_bus.has_unread_messages(self._agent_id)
    
    def send_message_to_parent(
        self,
        content: str,
        message_type: MessageType = MessageType.INFORMATION,
    ) -> None:
        """å‘çˆ¶Agentå‘é€æ¶ˆæ¯"""
        if self.parent_id:
            message_bus.send_message(
                from_agent=self._agent_id,
                to_agent=self.parent_id,
                content=content,
                message_type=message_type,
            )
    
    def send_message_to_agent(
        self,
        target_id: str,
        content: str,
        message_type: MessageType = MessageType.INFORMATION,
    ) -> None:
        """å‘æŒ‡å®šAgentå‘é€æ¶ˆæ¯"""
        message_bus.send_message(
            from_agent=self._agent_id,
            to_agent=target_id,
            content=content,
            message_type=message_type,
        )
    
    # ============ ç”Ÿå‘½å‘¨æœŸç®¡ç† ============
    
    def on_start(self) -> None:
        """Agentå¼€å§‹æ‰§è¡Œæ—¶è°ƒç”¨"""
        self._state.start()
        agent_registry.update_agent_status(self._agent_id, "running")
    
    def on_complete(self, result: Dict[str, Any]) -> None:
        """Agentå®Œæˆæ—¶è°ƒç”¨"""
        self._state.set_completed(result)
        agent_registry.update_agent_status(self._agent_id, "completed", result)
        
        # å‘çˆ¶AgentæŠ¥å‘Šå®Œæˆ
        if self.parent_id:
            message_bus.send_completion_report(
                from_agent=self._agent_id,
                to_agent=self.parent_id,
                summary=result.get("summary", "Task completed"),
                findings=result.get("findings", []),
                success=True,
            )
    
    def on_error(self, error: str) -> None:
        """Agentå‡ºé”™æ—¶è°ƒç”¨"""
        self._state.set_failed(error)
        agent_registry.update_agent_status(self._agent_id, "failed", {"error": error})
    
    @abstractmethod
    async def run(self, input_data: Dict[str, Any]) -> AgentResult:
        """
        æ‰§è¡Œ Agent ä»»åŠ¡
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            Agent æ‰§è¡Œç»“æœ
        """
        pass
    
    def cancel(self):
        """å–æ¶ˆæ‰§è¡Œ"""
        self._cancelled = True
        logger.info(f"[{self.name}] Cancel requested")
    
    @property
    def is_cancelled(self) -> bool:
        return self._cancelled
    
    # ============ åä½œæ–¹æ³• ============
    
    def receive_handoff(self, handoff: TaskHandoff):
        """
        æ¥æ”¶æ¥è‡ªå‰åº Agent çš„ä»»åŠ¡äº¤æ¥
        
        Args:
            handoff: ä»»åŠ¡äº¤æ¥å¯¹è±¡
        """
        self._incoming_handoff = handoff
        logger.info(
            f"[{self.name}] Received handoff from {handoff.from_agent}: "
            f"{handoff.summary[:50]}..."
        )
    
    def get_handoff_context(self) -> str:
        """
        è·å–äº¤æ¥ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ„å»º LLM promptï¼‰
        
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not self._incoming_handoff:
            return ""
        return self._incoming_handoff.to_prompt_context()
    
    def add_insight(self, insight: str):
        """è®°å½•æ´å¯Ÿ"""
        self._insights.append(insight)
    
    def record_work(self, work: str):
        """è®°å½•å®Œæˆçš„å·¥ä½œ"""
        self._work_completed.append(work)
    
    def create_handoff(
        self,
        to_agent: str,
        summary: str,
        key_findings: List[Dict[str, Any]] = None,
        suggested_actions: List[Dict[str, Any]] = None,
        attention_points: List[str] = None,
        priority_areas: List[str] = None,
        context_data: Dict[str, Any] = None,
    ) -> TaskHandoff:
        """
        åˆ›å»ºä»»åŠ¡äº¤æ¥
        
        Args:
            to_agent: ç›®æ ‡ Agent
            summary: å·¥ä½œæ‘˜è¦
            key_findings: å…³é”®å‘ç°
            suggested_actions: å»ºè®®çš„è¡ŒåŠ¨
            attention_points: éœ€è¦å…³æ³¨çš„ç‚¹
            priority_areas: ä¼˜å…ˆåˆ†æåŒºåŸŸ
            context_data: ä¸Šä¸‹æ–‡æ•°æ®
            
        Returns:
            TaskHandoff å¯¹è±¡
        """
        return TaskHandoff(
            from_agent=self.name,
            to_agent=to_agent,
            summary=summary,
            work_completed=self._work_completed.copy(),
            key_findings=key_findings or [],
            insights=self._insights.copy(),
            suggested_actions=suggested_actions or [],
            attention_points=attention_points or [],
            priority_areas=priority_areas or [],
            context_data=context_data or {},
        )
    
    def build_prompt_with_handoff(self, base_prompt: str) -> str:
        """
        æ„å»ºåŒ…å«äº¤æ¥ä¸Šä¸‹æ–‡çš„ prompt
        
        Args:
            base_prompt: åŸºç¡€ prompt
            
        Returns:
            å¢å¼ºåçš„ prompt
        """
        handoff_context = self.get_handoff_context()
        if not handoff_context:
            return base_prompt
        
        return f"""{base_prompt}

---
## å‰åº Agent äº¤æ¥ä¿¡æ¯

{handoff_context}

---
è¯·åŸºäºä»¥ä¸Šæ¥è‡ªå‰åº Agent çš„ä¿¡æ¯ï¼Œç»“åˆä½ çš„ä¸“ä¸šèƒ½åŠ›å¼€å±•å·¥ä½œã€‚
"""
    
    # ============ æ ¸å¿ƒäº‹ä»¶å‘å°„æ–¹æ³• ============
    
    async def emit_event(
        self,
        event_type: str,
        message: str,
        **kwargs
    ):
        """å‘å°„äº‹ä»¶"""
        if self.event_emitter:
            from ..event_manager import AgentEventData
            
            # å‡†å¤‡ metadata
            metadata = kwargs.get("metadata", {}) or {}
            if "agent_name" not in metadata:
                metadata["agent_name"] = self.name
            
            # åˆ†ç¦»å·²çŸ¥å­—æ®µå’ŒæœªçŸ¥å­—æ®µ
            known_fields = {
                "phase", "tool_name", "tool_input", "tool_output", 
                "tool_duration_ms", "finding_id", "tokens_used"
            }
            
            event_kwargs = {}
            for k, v in kwargs.items():
                if k in known_fields:
                    event_kwargs[k] = v
                elif k != "metadata":
                    # å°†æœªçŸ¥å­—æ®µæ”¾å…¥ metadata
                    metadata[k] = v
            
            await self.event_emitter.emit(AgentEventData(
                event_type=event_type,
                message=message,
                metadata=metadata,
                **event_kwargs
            ))
    
    # ============ LLM æ€è€ƒç›¸å…³äº‹ä»¶ ============
    
    async def emit_thinking(self, message: str):
        """å‘å°„ LLM æ€è€ƒäº‹ä»¶"""
        await self.emit_event("thinking", message)
    
    async def emit_llm_start(self, iteration: int):
        """å‘å°„ LLM å¼€å§‹æ€è€ƒäº‹ä»¶"""
        await self.emit_event(
            "llm_start",
            f"[{self.name}] ç¬¬ {iteration} è½®è¿­ä»£å¼€å§‹",
            metadata={"iteration": iteration}
        )
    
    async def emit_llm_thought(self, thought: str, iteration: int):
        """å‘å°„ LLM æ€è€ƒå†…å®¹äº‹ä»¶ - è¿™æ˜¯æ ¸å¿ƒï¼å±•ç¤º LLM åœ¨æƒ³ä»€ä¹ˆ"""
        # æˆªæ–­è¿‡é•¿çš„æ€è€ƒå†…å®¹
        display_thought = thought[:500] + "..." if len(thought) > 500 else thought
        await self.emit_event(
            "llm_thought",
            f"[{self.name}] æ€è€ƒ: {display_thought}",
            metadata={
                "thought": thought,
                "iteration": iteration,
            }
        )
    
    async def emit_thinking_start(self):
        """å‘å°„å¼€å§‹æ€è€ƒäº‹ä»¶ï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰"""
        await self.emit_event("thinking_start", "å¼€å§‹æ€è€ƒ...")
    
    async def emit_thinking_token(self, token: str, accumulated: str):
        """å‘å°„æ€è€ƒ token äº‹ä»¶ï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰"""
        await self.emit_event(
            "thinking_token",
            "",  # ä¸éœ€è¦ messageï¼Œå‰ç«¯ä» metadata è·å–
            metadata={
                "token": token,
                "accumulated": accumulated,
            }
        )
    
    async def emit_thinking_end(self, full_response: str):
        """å‘å°„æ€è€ƒç»“æŸäº‹ä»¶ï¼ˆæµå¼è¾“å‡ºç”¨ï¼‰"""
        await self.emit_event(
            "thinking_end",
            "æ€è€ƒå®Œæˆ",
            metadata={"accumulated": full_response}
        )
    
    async def emit_llm_decision(self, decision: str, reason: str = ""):
        """å‘å°„ LLM å†³ç­–äº‹ä»¶ - å±•ç¤º LLM åšäº†ä»€ä¹ˆå†³å®š"""
        await self.emit_event(
            "llm_decision",
            f"[{self.name}] å†³ç­–: {decision}" + (f" ({reason})" if reason else ""),
            metadata={
                "decision": decision,
                "reason": reason,
            }
        )
    
    async def emit_llm_complete(self, result_summary: str, tokens_used: int):
        """å‘å°„ LLM å®Œæˆäº‹ä»¶"""
        await self.emit_event(
            "llm_complete",
            f"[{self.name}] å®Œæˆ: {result_summary} (æ¶ˆè€— {tokens_used} tokens)",
            metadata={
                "tokens_used": tokens_used,
            }
        )
    
    async def emit_llm_action(self, action: str, action_input: Dict):
        """å‘å°„ LLM åŠ¨ä½œå†³ç­–äº‹ä»¶"""
        await self.emit_event(
            "llm_action",
            f"[{self.name}] æ‰§è¡ŒåŠ¨ä½œ: {action}",
            metadata={
                "action": action,
                "action_input": action_input,
            }
        )
    
    async def emit_llm_observation(self, observation: str):
        """å‘å°„ LLM è§‚å¯Ÿäº‹ä»¶"""
        # æˆªæ–­è¿‡é•¿çš„è§‚å¯Ÿç»“æœ
        display_obs = observation[:300] + "..." if len(observation) > 300 else observation
        await self.emit_event(
            "llm_observation",
            f"[{self.name}] è§‚å¯Ÿç»“æœ: {display_obs}",
            metadata={
                "observation": observation[:2000],  # é™åˆ¶å­˜å‚¨é•¿åº¦
            }
        )
    
    # ============ å·¥å…·è°ƒç”¨ç›¸å…³äº‹ä»¶ ============
    
    async def emit_tool_call(self, tool_name: str, tool_input: Dict):
        """å‘å°„å·¥å…·è°ƒç”¨äº‹ä»¶"""
        await self.emit_event(
            "tool_call",
            f"[{self.name}] è°ƒç”¨å·¥å…·: {tool_name}",
            tool_name=tool_name,
            tool_input=tool_input,
        )
    
    async def emit_tool_result(self, tool_name: str, result: str, duration_ms: int):
        """å‘å°„å·¥å…·ç»“æœäº‹ä»¶"""
        await self.emit_event(
            "tool_result",
            f"[{self.name}] å·¥å…· {tool_name} å®Œæˆ ({duration_ms}ms)",
            tool_name=tool_name,
            tool_duration_ms=duration_ms,
        )
    
    # ============ å‘ç°ç›¸å…³äº‹ä»¶ ============
    
    async def emit_finding(self, title: str, severity: str, vuln_type: str, file_path: str = ""):
        """å‘å°„æ¼æ´å‘ç°äº‹ä»¶"""
        severity_emoji = {
            "critical": "ğŸ”´",
            "high": "ğŸŸ ",
            "medium": "ğŸŸ¡",
            "low": "ğŸŸ¢",
        }.get(severity.lower(), "âšª")
        
        await self.emit_event(
            "finding",
            f"{severity_emoji} [{self.name}] å‘ç°æ¼æ´: [{severity.upper()}] {title}\n   ç±»å‹: {vuln_type}\n   ä½ç½®: {file_path}",
            metadata={
                "title": title,
                "severity": severity,
                "vulnerability_type": vuln_type,
                "file_path": file_path,
            }
        )
    
    # ============ é€šç”¨å·¥å…·æ–¹æ³• ============
    
    async def call_tool(self, tool_name: str, **kwargs) -> Any:
        """
        è°ƒç”¨å·¥å…·
        
        Args:
            tool_name: å·¥å…·åç§°
            **kwargs: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        tool = self.tools.get(tool_name)
        if not tool:
            logger.warning(f"Tool not found: {tool_name}")
            return None
        
        self._tool_calls += 1
        await self.emit_tool_call(tool_name, kwargs)
        
        import time
        start = time.time()
        
        result = await tool.execute(**kwargs)
        
        duration_ms = int((time.time() - start) * 1000)
        await self.emit_tool_result(tool_name, str(result.data)[:500], duration_ms)
        
        return result
    
    async def call_llm(
        self,
        messages: List[Dict[str, str]],
        tools: Optional[List[Dict]] = None,
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å¯ç”¨å·¥å…·æè¿°
            
        Returns:
            LLM å“åº”
        """
        self._iteration += 1
        
        try:
            response = await self.llm_service.chat_completion(
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                tools=tools,
            )
            
            if response.get("usage"):
                self._total_tokens += response["usage"].get("total_tokens", 0)
            
            return response
            
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise
    
    def get_tool_descriptions(self) -> List[Dict[str, Any]]:
        """è·å–å·¥å…·æè¿°ï¼ˆç”¨äº LLMï¼‰"""
        descriptions = []
        
        for name, tool in self.tools.items():
            if name.startswith("_"):
                continue
            
            desc = {
                "type": "function",
                "function": {
                    "name": name,
                    "description": tool.description,
                }
            }
            
            # æ·»åŠ å‚æ•° schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                desc["function"]["parameters"] = tool.args_schema.schema()
            
            descriptions.append(desc)
        
        return descriptions
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡"""
        return {
            "agent": self.name,
            "type": self.agent_type.value,
            "iterations": self._iteration,
            "tool_calls": self._tool_calls,
            "tokens_used": self._total_tokens,
        }
    
    # ============ Memory Compression ============
    
    def compress_messages_if_needed(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 100000,
    ) -> List[Dict[str, str]]:
        """
        å¦‚æœæ¶ˆæ¯å†å²è¿‡é•¿ï¼Œè‡ªåŠ¨å‹ç¼©
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            å‹ç¼©åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        from ...llm.memory_compressor import MemoryCompressor
        
        compressor = MemoryCompressor(max_total_tokens=max_tokens)
        
        if compressor.should_compress(messages):
            logger.info(f"[{self.name}] Compressing conversation history...")
            compressed = compressor.compress_history(messages)
            logger.info(f"[{self.name}] Compressed {len(messages)} -> {len(compressed)} messages")
            return compressed
        
        return messages
    
    # ============ ç»Ÿä¸€çš„æµå¼ LLM è°ƒç”¨ ============
    
    async def stream_llm_call(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.1,
        max_tokens: int = 2048,
        auto_compress: bool = True,
    ) -> Tuple[str, int]:
        """
        ç»Ÿä¸€çš„æµå¼ LLM è°ƒç”¨æ–¹æ³•
        
        æ‰€æœ‰ Agent å…±ç”¨æ­¤æ–¹æ³•ï¼Œé¿å…é‡å¤ä»£ç 
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ token æ•°
            auto_compress: æ˜¯å¦è‡ªåŠ¨å‹ç¼©è¿‡é•¿çš„æ¶ˆæ¯å†å²
            
        Returns:
            (å®Œæ•´å“åº”å†…å®¹, tokenæ•°é‡)
        """
        # ğŸ”¥ è‡ªåŠ¨å‹ç¼©è¿‡é•¿çš„æ¶ˆæ¯å†å²
        if auto_compress:
            messages = self.compress_messages_if_needed(messages)
        
        accumulated = ""
        total_tokens = 0
        
        # ğŸ”¥ åœ¨å¼€å§‹ LLM è°ƒç”¨å‰æ£€æŸ¥å–æ¶ˆ
        if self.is_cancelled:
            logger.info(f"[{self.name}] Cancelled before LLM call")
            return "", 0
        
        await self.emit_thinking_start()
        
        try:
            async for chunk in self.llm_service.chat_completion_stream(
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            ):
                # æ£€æŸ¥å–æ¶ˆ
                if self.is_cancelled:
                    logger.info(f"[{self.name}] Cancelled during LLM streaming")
                    break
                
                if chunk["type"] == "token":
                    token = chunk["content"]
                    accumulated = chunk["accumulated"]
                    await self.emit_thinking_token(token, accumulated)
                    # ğŸ”¥ CRITICAL: è®©å‡ºæ§åˆ¶æƒç»™äº‹ä»¶å¾ªç¯ï¼Œè®© SSE æœ‰æœºä¼šå‘é€äº‹ä»¶
                    # å¦‚æœä¸è¿™æ ·åšï¼Œæ‰€æœ‰ token ä¼šåœ¨å¾ªç¯ç»“æŸåä¸€èµ·å‘é€
                    await asyncio.sleep(0)
                    
                elif chunk["type"] == "done":
                    accumulated = chunk["content"]
                    if chunk.get("usage"):
                        total_tokens = chunk["usage"].get("total_tokens", 0)
                    break
                    
                elif chunk["type"] == "error":
                    accumulated = chunk.get("accumulated", "")
                    logger.error(f"Stream error: {chunk.get('error')}")
                    break
                    
        except asyncio.CancelledError:
            logger.info(f"[{self.name}] LLM call cancelled")
            raise
        finally:
            await self.emit_thinking_end(accumulated)
        
        return accumulated, total_tokens
    
    async def execute_tool(self, tool_name: str, tool_input: Dict) -> str:
        """
        ç»Ÿä¸€çš„å·¥å…·æ‰§è¡Œæ–¹æ³•
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœå­—ç¬¦ä¸²
        """
        # ğŸ”¥ åœ¨æ‰§è¡Œå·¥å…·å‰æ£€æŸ¥å–æ¶ˆ
        if self.is_cancelled:
            return "ä»»åŠ¡å·²å–æ¶ˆ"
        
        tool = self.tools.get(tool_name)
        
        if not tool:
            return f"é”™è¯¯: å·¥å…· '{tool_name}' ä¸å­˜åœ¨ã€‚å¯ç”¨å·¥å…·: {list(self.tools.keys())}"
        
        try:
            self._tool_calls += 1
            await self.emit_tool_call(tool_name, tool_input)
            
            import time
            start = time.time()
            
            result = await tool.execute(**tool_input)
            
            duration_ms = int((time.time() - start) * 1000)
            await self.emit_tool_result(tool_name, str(result.data)[:200], duration_ms)
            
            if result.success:
                output = str(result.data)
                
                # åŒ…å« metadata ä¸­çš„é¢å¤–ä¿¡æ¯
                if result.metadata:
                    if "issues" in result.metadata:
                        import json
                        output += f"\n\nå‘ç°çš„é—®é¢˜:\n{json.dumps(result.metadata['issues'], ensure_ascii=False, indent=2)}"
                    if "findings" in result.metadata:
                        import json
                        output += f"\n\nå‘ç°:\n{json.dumps(result.metadata['findings'][:10], ensure_ascii=False, indent=2)}"
                
                # æˆªæ–­è¿‡é•¿è¾“å‡º
                if len(output) > 6000:
                    output = output[:6000] + f"\n\n... [è¾“å‡ºå·²æˆªæ–­ï¼Œå…± {len(str(result.data))} å­—ç¬¦]"
                return output
            else:
                return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result.error}"
                
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            return f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
    
    def get_tools_description(self) -> str:
        """ç”Ÿæˆå·¥å…·æè¿°æ–‡æœ¬ï¼ˆç”¨äº promptï¼‰"""
        tools_info = []
        for name, tool in self.tools.items():
            if name.startswith("_"):
                continue
            desc = f"- {name}: {getattr(tool, 'description', 'No description')}"
            tools_info.append(desc)
        return "\n".join(tools_info)
