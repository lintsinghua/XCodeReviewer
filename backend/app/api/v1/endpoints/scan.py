from fastapi import APIRouter, UploadFile, File, Form, Depends, BackgroundTasks, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from typing import Any, List, Optional
from pydantic import BaseModel
from datetime import datetime, timezone
import uuid
import shutil
import os
import json
from pathlib import Path
import zipfile
import asyncio

from app.api import deps
from app.db.session import get_db, AsyncSessionLocal
from app.models.audit import AuditTask, AuditIssue
from app.models.user import User
from app.models.project import Project
from app.models.analysis import InstantAnalysis
from app.models.user_config import UserConfig
from app.services.llm.service import LLMService
from app.services.scanner import task_control, is_text_file, should_exclude, get_language_from_path, get_analysis_config
from app.services.zip_storage import load_project_zip, save_project_zip, has_project_zip
from app.core.config import settings

router = APIRouter()


def normalize_path(path: str) -> str:
    """
    ç»Ÿä¸€è·¯å¾„åˆ†éš”ç¬¦ä¸ºæ­£æ–œæ ï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§
    Windows ä½¿ç”¨åæ–œæ  (\)ï¼ŒUnix/Mac ä½¿ç”¨æ­£æ–œæ  (/)
    ç»Ÿä¸€è½¬æ¢ä¸ºæ­£æ–œæ ä»¥ä¿è¯ä¸€è‡´æ€§
    """
    return path.replace("\\", "/")


# æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
TEXT_EXTENSIONS = [
    ".js", ".ts", ".tsx", ".jsx", ".py", ".java", ".go", ".rs",
    ".cpp", ".c", ".h", ".cc", ".hh", ".cs", ".php", ".rb",
    ".kt", ".swift", ".sql", ".sh", ".json", ".yml", ".yaml"
]


async def process_zip_task(task_id: str, file_path: str, db_session_factory, user_config: dict = None):
    """åå°ZIPæ–‡ä»¶å¤„ç†ä»»åŠ¡"""
    async with db_session_factory() as db:
        task = await db.get(AuditTask, task_id)
        if not task:
            return

        try:
            task.status = "running"
            task.started_at = datetime.now(timezone.utc)
            await db.commit()
            
            # åˆ›å»ºä½¿ç”¨ç”¨æˆ·é…ç½®çš„LLMæœåŠ¡å®ä¾‹
            llm_service = LLMService(user_config=user_config or {})

            # Extract ZIP
            extract_dir = Path(f"/tmp/{task_id}")
            extract_dir.mkdir(parents=True, exist_ok=True)
            
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)

            # è·å–ç”¨æˆ·è‡ªå®šä¹‰æ’é™¤æ¨¡å¼
            scan_config = (user_config or {}).get('scan_config', {})
            custom_exclude_patterns = scan_config.get('exclude_patterns', [])
            
            # Find files
            files_to_scan = []
            for root, dirs, files in os.walk(extract_dir):
                # æ’é™¤å¸¸è§éä»£ç ç›®å½•
                dirs[:] = [d for d in dirs if d not in ['node_modules', '__pycache__', '.git', 'dist', 'build', 'vendor']]
                
                for file in files:
                    full_path = Path(root) / file
                    # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ ï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§
                    rel_path = normalize_path(str(full_path.relative_to(extract_dir)))
                    
                    # æ£€æŸ¥æ–‡ä»¶ç±»å‹å’Œæ’é™¤è§„åˆ™ï¼ˆåŒ…å«ç”¨æˆ·è‡ªå®šä¹‰æ’é™¤æ¨¡å¼ï¼‰
                    if is_text_file(rel_path) and not should_exclude(rel_path, custom_exclude_patterns):
                        try:
                            content = full_path.read_text(errors='ignore')
                            if len(content) <= settings.MAX_FILE_SIZE_BYTES:
                                files_to_scan.append({
                                    "path": rel_path,
                                    "content": content
                                })
                        except:
                            pass

            # è·å–åˆ†æé…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®ï¼‰
            analysis_config = get_analysis_config(user_config)
            max_analyze_files = analysis_config['max_analyze_files']
            llm_gap_ms = analysis_config['llm_gap_ms']

            # é™åˆ¶æ–‡ä»¶æ•°é‡
            # å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ–‡ä»¶ï¼Œåˆ™åªåˆ†æè¿™äº›æ–‡ä»¶
            target_files = scan_config.get('file_paths', [])
            if target_files:
                # ç»Ÿä¸€ç›®æ ‡æ–‡ä»¶è·¯å¾„çš„åˆ†éš”ç¬¦ï¼Œç¡®ä¿åŒ¹é…ä¸€è‡´æ€§
                normalized_targets = {normalize_path(p) for p in target_files}
                print(f"ğŸ¯ ZIPä»»åŠ¡: æŒ‡å®šåˆ†æ {len(normalized_targets)} ä¸ªæ–‡ä»¶")
                files_to_scan = [f for f in files_to_scan if f['path'] in normalized_targets]
            elif max_analyze_files > 0:
                files_to_scan = files_to_scan[:max_analyze_files]

            task.total_files = len(files_to_scan)
            await db.commit()

            print(f"ğŸ“Š ZIPä»»åŠ¡ {task_id}: æ‰¾åˆ° {len(files_to_scan)} ä¸ªæ–‡ä»¶ (æœ€å¤§æ–‡ä»¶æ•°: {max_analyze_files}, è¯·æ±‚é—´éš”: {llm_gap_ms}ms)")

            total_issues = 0
            total_lines = 0
            quality_scores = []
            scanned_files = 0
            failed_files = 0

            for file_info in files_to_scan:
                # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
                if task_control.is_cancelled(task_id):
                    print(f"ğŸ›‘ ZIPä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆ")
                    task.status = "cancelled"
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
                    task_control.cleanup_task(task_id)
                    return

                try:
                    content = file_info['content']
                    total_lines += content.count('\n') + 1
                    language = get_language_from_path(file_info['path'])
                    
                    # è·å–è§„åˆ™é›†å’Œæç¤ºè¯æ¨¡æ¿ID
                    scan_config = (user_config or {}).get('scan_config', {})
                    rule_set_id = scan_config.get('rule_set_id')
                    prompt_template_id = scan_config.get('prompt_template_id')
                    
                    # ä½¿ç”¨è§„åˆ™é›†å’Œæç¤ºè¯æ¨¡æ¿è¿›è¡Œåˆ†æ
                    if rule_set_id or prompt_template_id:
                        result = await llm_service.analyze_code_with_rules(
                            content, language, 
                            rule_set_id=rule_set_id,
                            prompt_template_id=prompt_template_id,
                            db_session=db
                        )
                    else:
                        result = await llm_service.analyze_code(content, language)
                    
                    issues = result.get("issues", [])
                    for i in issues:
                        issue = AuditIssue(
                            task_id=task.id,
                            file_path=file_info['path'],
                            line_number=i.get('line', 1),
                            column_number=i.get('column'),
                            issue_type=i.get('type', 'maintainability'),
                            severity=i.get('severity', 'low'),
                            title=i.get('title', 'Issue'),
                            message=i.get('title', 'Issue'),
                            description=i.get('description'),
                            suggestion=i.get('suggestion'),
                            code_snippet=i.get('code_snippet'),
                            ai_explanation=json.dumps(i.get('xai')) if i.get('xai') else None,
                            status="open"
                        )
                        db.add(issue)
                        total_issues += 1
                    
                    if "quality_score" in result:
                        quality_scores.append(result["quality_score"])
                    
                    scanned_files += 1
                    task.scanned_files = scanned_files
                    task.total_lines = total_lines
                    task.issues_count = total_issues
                    await db.commit()
                    
                    print(f"ğŸ“ˆ ZIPä»»åŠ¡ {task_id}: è¿›åº¦ {scanned_files}/{len(files_to_scan)}")
                    
                    # è¯·æ±‚é—´éš”
                    await asyncio.sleep(llm_gap_ms / 1000)

                except Exception as file_error:
                    failed_files += 1
                    print(f"âŒ ZIPä»»åŠ¡åˆ†ææ–‡ä»¶å¤±è´¥ ({file_info['path']}): {file_error}")
                    await asyncio.sleep(llm_gap_ms / 1000)

            # å®Œæˆä»»åŠ¡
            avg_quality_score = sum(quality_scores) / len(quality_scores) if quality_scores else 100.0
            
            # å¦‚æœæœ‰æ–‡ä»¶éœ€è¦åˆ†æä½†å…¨éƒ¨å¤±è´¥ï¼Œæ ‡è®°ä¸ºå¤±è´¥
            if len(files_to_scan) > 0 and scanned_files == 0:
                task.status = "failed"
                task.completed_at = datetime.now(timezone.utc)
                task.scanned_files = 0
                task.total_lines = total_lines
                task.issues_count = 0
                task.quality_score = 0
                await db.commit()
                print(f"âŒ ZIPä»»åŠ¡ {task_id} å¤±è´¥: æ‰€æœ‰ {len(files_to_scan)} ä¸ªæ–‡ä»¶åˆ†æå‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ LLM API é…ç½®")
            else:
                task.status = "completed"
                task.completed_at = datetime.now(timezone.utc)
                task.scanned_files = scanned_files
                task.total_lines = total_lines
                task.issues_count = total_issues
                task.quality_score = avg_quality_score
                await db.commit()
                print(f"âœ… ZIPä»»åŠ¡ {task_id} å®Œæˆ: æ‰«æ {scanned_files} ä¸ªæ–‡ä»¶, å‘ç° {total_issues} ä¸ªé—®é¢˜")
            task_control.cleanup_task(task_id)
            
        except Exception as e:
            print(f"âŒ ZIPæ‰«æå¤±è´¥: {e}")
            task.status = "failed"
            task.completed_at = datetime.now(timezone.utc)
            await db.commit()
            task_control.cleanup_task(task_id)
        finally:
            # Cleanup - åªæ¸…ç†è§£å‹ç›®å½•ï¼Œä¸åˆ é™¤æºZIPæ–‡ä»¶ï¼ˆå·²æŒä¹…åŒ–å­˜å‚¨ï¼‰
            if extract_dir.exists():
                shutil.rmtree(extract_dir)


@router.post("/upload-zip")
async def scan_zip(
    background_tasks: BackgroundTasks,
    project_id: str = Form(...),
    file: UploadFile = File(...),
    scan_config: Optional[str] = Form(None),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Upload and scan a ZIP file.
    ä¸Šä¼ ZIPæ–‡ä»¶å¹¶å¯åŠ¨æ‰«æï¼ŒåŒæ—¶å°†ZIPæ–‡ä»¶ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
    """
    # Verify project exists
    project = await db.get(Project, project_id)
    if not project:
        raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æƒé™ï¼šåªæœ‰é¡¹ç›®æ‰€æœ‰è€…å¯ä»¥ä¸Šä¼ 
    if project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤é¡¹ç›®")
    
    # Validate file
    if not file.filename.lower().endswith('.zip'):
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ ZIPæ ¼å¼æ–‡ä»¶")
        
    # Save Uploaded File to temp
    file_id = str(uuid.uuid4())
    file_path = f"/tmp/{file_id}.zip"
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # Check file size
    file_size = os.path.getsize(file_path)
    if file_size > 500 * 1024 * 1024:  # 500MB limit
        os.remove(file_path)
        raise HTTPException(status_code=400, detail="æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡500MB")
    
    # ä¿å­˜ZIPæ–‡ä»¶åˆ°æŒä¹…åŒ–å­˜å‚¨
    await save_project_zip(project_id, file_path, file.filename)
    
    # Parse scan_config if provided
    parsed_scan_config = {}
    if scan_config:
        try:
            parsed_scan_config = json.loads(scan_config)
        except json.JSONDecodeError:
            pass

    # Create Task
    task = AuditTask(
        project_id=project_id,
        created_by=current_user.id,
        task_type="zip_upload",
        status="pending",
        scan_config=scan_config if scan_config else "{}"
    )
    db.add(task)
    await db.commit()
    await db.refresh(task)

    # è·å–ç”¨æˆ·é…ç½®
    user_config = await get_user_config_dict(db, current_user.id)
    
    # å°†æ‰«æé…ç½®æ³¨å…¥åˆ° user_config ä¸­ï¼ˆåŒ…æ‹¬è§„åˆ™é›†ã€æç¤ºè¯æ¨¡æ¿å’Œæ’é™¤æ¨¡å¼ï¼‰
    if parsed_scan_config:
        user_config['scan_config'] = {
            'file_paths': parsed_scan_config.get('file_paths', []),
            'exclude_patterns': parsed_scan_config.get('exclude_patterns', []),
            'rule_set_id': parsed_scan_config.get('rule_set_id'),
            'prompt_template_id': parsed_scan_config.get('prompt_template_id'),
        }

    # Trigger Background Task - ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨çš„æ–‡ä»¶è·¯å¾„
    stored_zip_path = await load_project_zip(project_id)
    background_tasks.add_task(process_zip_task, task.id, stored_zip_path or file_path, AsyncSessionLocal, user_config)

    return {"task_id": task.id, "status": "queued"}


class ScanRequest(BaseModel):
    file_paths: Optional[List[str]] = None
    full_scan: bool = True
    exclude_patterns: Optional[List[str]] = None
    rule_set_id: Optional[str] = None
    prompt_template_id: Optional[str] = None


@router.post("/scan-stored-zip")
async def scan_stored_zip(
    project_id: str,
    background_tasks: BackgroundTasks,
    scan_request: Optional[ScanRequest] = None,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    ä½¿ç”¨å·²å­˜å‚¨çš„ZIPæ–‡ä»¶å¯åŠ¨æ‰«æï¼ˆæ— éœ€é‡æ–°ä¸Šä¼ ï¼‰
    """
    # Verify project exists
    project = await db.get(Project, project_id)
    if not project:
        raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æƒé™ï¼šåªæœ‰é¡¹ç›®æ‰€æœ‰è€…å¯ä»¥æ‰«æ
    if project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤é¡¹ç›®")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å­˜å‚¨çš„ZIPæ–‡ä»¶
    stored_zip_path = await load_project_zip(project_id)
    if not stored_zip_path:
        raise HTTPException(status_code=400, detail="é¡¹ç›®æ²¡æœ‰å·²å­˜å‚¨çš„ZIPæ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ ")
    
    # Create Task
    task = AuditTask(
        project_id=project_id,
        created_by=current_user.id,
        task_type="zip_upload",
        status="pending",
        scan_config=json.dumps(scan_request.dict()) if scan_request else "{}"
    )
    db.add(task)
    await db.commit()
    await db.refresh(task)

    # è·å–ç”¨æˆ·é…ç½®
    user_config = await get_user_config_dict(db, current_user.id)
    
    # å°†æ‰«æé…ç½®æ³¨å…¥åˆ° user_config ä¸­ï¼ˆåŒ…æ‹¬è§„åˆ™é›†ã€æç¤ºè¯æ¨¡æ¿å’Œæ’é™¤æ¨¡å¼ï¼‰
    if scan_request:
        user_config['scan_config'] = {
            'file_paths': scan_request.file_paths or [],
            'exclude_patterns': scan_request.exclude_patterns or [],
            'rule_set_id': scan_request.rule_set_id,
            'prompt_template_id': scan_request.prompt_template_id,
        }

    # Trigger Background Task
    background_tasks.add_task(process_zip_task, task.id, stored_zip_path, AsyncSessionLocal, user_config)

    return {"task_id": task.id, "status": "queued"}


class InstantAnalysisRequest(BaseModel):
    code: str
    language: str
    prompt_template_id: Optional[str] = None


class InstantAnalysisResponse(BaseModel):
    id: str
    user_id: str
    language: str
    issues_count: int
    quality_score: float
    analysis_time: float
    analysis_result: str  # JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å®Œæ•´çš„åˆ†æç»“æœ
    created_at: datetime

    class Config:
        from_attributes = True


async def get_user_config_dict(db: AsyncSession, user_id: str) -> dict:
    """è·å–ç”¨æˆ·é…ç½®å­—å…¸ï¼ˆåŒ…å«è§£å¯†æ•æ„Ÿå­—æ®µï¼‰"""
    from app.core.encryption import decrypt_sensitive_data
    
    # éœ€è¦è§£å¯†çš„æ•æ„Ÿå­—æ®µåˆ—è¡¨ï¼ˆä¸ config.py ä¿æŒä¸€è‡´ï¼‰
    SENSITIVE_LLM_FIELDS = [
        'llmApiKey', 'geminiApiKey', 'openaiApiKey', 'claudeApiKey',
        'qwenApiKey', 'deepseekApiKey', 'zhipuApiKey', 'moonshotApiKey',
        'baiduApiKey', 'minimaxApiKey', 'doubaoApiKey'
    ]
    SENSITIVE_OTHER_FIELDS = ['githubToken', 'gitlabToken']
    
    def decrypt_config(config: dict, sensitive_fields: list) -> dict:
        """è§£å¯†é…ç½®ä¸­çš„æ•æ„Ÿå­—æ®µ"""
        decrypted = config.copy()
        for field in sensitive_fields:
            if field in decrypted and decrypted[field]:
                decrypted[field] = decrypt_sensitive_data(decrypted[field])
        return decrypted
    
    result = await db.execute(
        select(UserConfig).where(UserConfig.user_id == user_id)
    )
    config = result.scalar_one_or_none()
    if not config:
        return {}
    
    # è§£æé…ç½®
    llm_config = json.loads(config.llm_config) if config.llm_config else {}
    other_config = json.loads(config.other_config) if config.other_config else {}
    
    # è§£å¯†æ•æ„Ÿå­—æ®µ
    llm_config = decrypt_config(llm_config, SENSITIVE_LLM_FIELDS)
    other_config = decrypt_config(other_config, SENSITIVE_OTHER_FIELDS)
    
    return {
        'llmConfig': llm_config,
        'otherConfig': other_config,
    }


@router.post("/instant")
async def instant_analysis(
    req: InstantAnalysisRequest,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user), 
) -> Any:
    """
    Perform instant code analysis.
    """
    # è·å–ç”¨æˆ·é…ç½®
    user_config = await get_user_config_dict(db, current_user.id)
    
    # åˆ›å»ºä½¿ç”¨ç”¨æˆ·é…ç½®çš„LLMæœåŠ¡å®ä¾‹
    llm_service = LLMService(user_config=user_config)
    
    start_time = datetime.now(timezone.utc)
    
    try:
        # å¦‚æœæŒ‡å®šäº†æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨è‡ªå®šä¹‰åˆ†æ
        # ç»Ÿä¸€ä½¿ç”¨ analyze_code_with_rulesï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        result = await llm_service.analyze_code_with_rules(
            req.code, req.language,
            prompt_template_id=req.prompt_template_id,
            db_session=db,
            use_default_template=True  # æ²¡æœ‰æŒ‡å®šæ¨¡æ¿æ—¶ä½¿ç”¨æ•°æ®åº“ä¸­çš„é»˜è®¤æ¨¡æ¿
        )
    except Exception as e:
        # åˆ†æå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        print(f"âŒ å³æ—¶åˆ†æå¤±è´¥: {error_msg}")
        raise HTTPException(
            status_code=500, 
            detail=f"ä»£ç åˆ†æå¤±è´¥: {error_msg}"
        )
    
    end_time = datetime.now(timezone.utc)
    duration = (end_time - start_time).total_seconds()

    # Save record
    analysis = InstantAnalysis(
        user_id=current_user.id,
        language=req.language,
        code_content="",  # Do not persist code for privacy
        analysis_result=json.dumps(result),
        issues_count=len(result.get("issues", [])),
        quality_score=result.get("quality_score", 0),
        analysis_time=duration
    )
    db.add(analysis)
    await db.commit()
    await db.refresh(analysis)
    
    # Return result with analysis ID for export functionality
    return {
        **result,
        "analysis_id": analysis.id,
        "analysis_time": duration
    }


@router.get("/instant/history", response_model=List[InstantAnalysisResponse])
async def get_instant_analysis_history(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
    limit: int = 20,
) -> Any:
    """
    Get user's instant analysis history.
    """
    result = await db.execute(
        select(InstantAnalysis)
        .where(InstantAnalysis.user_id == current_user.id)
        .order_by(InstantAnalysis.created_at.desc())
        .limit(limit)
    )
    return result.scalars().all()


@router.delete("/instant/history/{analysis_id}")
async def delete_instant_analysis(
    analysis_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Delete a specific instant analysis record.
    """
    result = await db.execute(
        select(InstantAnalysis)
        .where(InstantAnalysis.id == analysis_id)
        .where(InstantAnalysis.user_id == current_user.id)
    )
    analysis = result.scalar_one_or_none()
    
    if not analysis:
        raise HTTPException(status_code=404, detail="åˆ†æè®°å½•ä¸å­˜åœ¨")
    
    await db.delete(analysis)
    await db.commit()
    
    return {"message": "åˆ é™¤æˆåŠŸ"}


@router.delete("/instant/history")
async def delete_all_instant_analyses(
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Delete all instant analysis records for current user.
    """
    from sqlalchemy import delete
    
    await db.execute(
        delete(InstantAnalysis).where(InstantAnalysis.user_id == current_user.id)
    )
    await db.commit()
    
    return {"message": "å·²æ¸…ç©ºæ‰€æœ‰å†å²è®°å½•"}


@router.get("/instant/history/{analysis_id}/report/pdf")
async def export_instant_report_pdf(
    analysis_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Export instant analysis report as PDF by analysis ID.
    """
    from fastapi.responses import Response
    from app.services.report_generator import ReportGenerator
    
    # è·å–å³æ—¶åˆ†æè®°å½•
    result = await db.execute(
        select(InstantAnalysis)
        .where(InstantAnalysis.id == analysis_id)
        .where(InstantAnalysis.user_id == current_user.id)
    )
    analysis = result.scalar_one_or_none()
    
    if not analysis:
        raise HTTPException(status_code=404, detail="åˆ†æè®°å½•ä¸å­˜åœ¨")
    
    # è§£æåˆ†æç»“æœ
    try:
        analysis_result = json.loads(analysis.analysis_result) if analysis.analysis_result else {}
    except json.JSONDecodeError:
        analysis_result = {}
    
    # ç”Ÿæˆ PDF
    pdf_bytes = ReportGenerator.generate_instant_report(
        analysis_result,
        analysis.language,
        analysis.analysis_time
    )
    
    # è¿”å› PDF æ–‡ä»¶
    filename = f"instant-analysis-{analysis.language}-{analysis.id[:8]}.pdf"
    return Response(
        content=pdf_bytes,
        media_type="application/pdf",
        headers={
            "Content-Disposition": f'attachment; filename="{filename}"'
        }
    )
