"""
DeepAudit Agent å®¡è®¡ä»»åŠ¡ API
åŸºäº LangGraph çš„ Agent å®¡è®¡
"""

import asyncio
import json
import logging
import os
import shutil
from typing import Any, List, Optional, Dict, Set
from datetime import datetime, timezone
from uuid import uuid4

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import case
from sqlalchemy.future import select
from sqlalchemy.orm import selectinload
from pydantic import BaseModel, Field

from app.api import deps
from app.db.session import get_db, async_session_factory
from app.models.agent_task import (
    AgentTask, AgentEvent, AgentFinding,
    AgentTaskStatus, AgentTaskPhase, AgentEventType,
    VulnerabilitySeverity, FindingStatus,
)
from app.models.project import Project
from app.models.user import User
from app.models.user_config import UserConfig
from app.services.agent.event_manager import EventManager
from app.services.agent.streaming import StreamHandler, StreamEvent, StreamEventType

logger = logging.getLogger(__name__)
router = APIRouter()

# è¿è¡Œä¸­çš„ä»»åŠ¡ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
_running_tasks: Dict[str, Any] = {}

# ğŸ”¥ è¿è¡Œä¸­çš„ asyncio Tasksï¼ˆç”¨äºå¼ºåˆ¶å–æ¶ˆï¼‰
_running_asyncio_tasks: Dict[str, asyncio.Task] = {}


# ============ Schemas ============

class AgentTaskCreate(BaseModel):
    """åˆ›å»º Agent ä»»åŠ¡è¯·æ±‚"""
    project_id: str = Field(..., description="é¡¹ç›® ID")
    name: Optional[str] = Field(None, description="ä»»åŠ¡åç§°")
    description: Optional[str] = Field(None, description="ä»»åŠ¡æè¿°")
    
    # å®¡è®¡é…ç½®
    audit_scope: Optional[dict] = Field(None, description="å®¡è®¡èŒƒå›´")
    target_vulnerabilities: Optional[List[str]] = Field(
        default=["sql_injection", "xss", "command_injection", "path_traversal", "ssrf"],
        description="ç›®æ ‡æ¼æ´ç±»å‹"
    )
    verification_level: str = Field(
        "sandbox", 
        description="éªŒè¯çº§åˆ«: analysis_only, sandbox, generate_poc"
    )
    
    # åˆ†æ”¯
    branch_name: Optional[str] = Field(None, description="åˆ†æ”¯åç§°")
    
    # æ’é™¤æ¨¡å¼
    exclude_patterns: Optional[List[str]] = Field(
        default=["node_modules", "__pycache__", ".git", "*.min.js"],
        description="æ’é™¤æ¨¡å¼"
    )
    
    # æ–‡ä»¶èŒƒå›´
    target_files: Optional[List[str]] = Field(None, description="æŒ‡å®šæ‰«æçš„æ–‡ä»¶")
    
    # Agent é…ç½®
    max_iterations: int = Field(50, ge=1, le=200, description="æœ€å¤§è¿­ä»£æ¬¡æ•°")
    timeout_seconds: int = Field(1800, ge=60, le=7200, description="è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")


class AgentTaskResponse(BaseModel):
    """Agent ä»»åŠ¡å“åº” - åŒ…å«æ‰€æœ‰å‰ç«¯éœ€è¦çš„å­—æ®µ"""
    id: str
    project_id: str
    name: Optional[str]
    description: Optional[str]
    task_type: str = "agent_audit"
    status: str
    current_phase: Optional[str]
    current_step: Optional[str] = None
    
    # è¿›åº¦ç»Ÿè®¡
    total_files: int = 0
    indexed_files: int = 0
    analyzed_files: int = 0
    total_chunks: int = 0
    
    # Agent ç»Ÿè®¡
    total_iterations: int = 0
    tool_calls_count: int = 0
    tokens_used: int = 0
    
    # å‘ç°ç»Ÿè®¡ï¼ˆå…¼å®¹ä¸¤ç§å‘½åï¼‰
    findings_count: int = 0
    total_findings: int = 0  # å…¼å®¹å­—æ®µ
    verified_count: int = 0
    verified_findings: int = 0  # å…¼å®¹å­—æ®µ
    false_positive_count: int = 0
    
    # ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
    critical_count: int = 0
    high_count: int = 0
    medium_count: int = 0
    low_count: int = 0
    
    # è¯„åˆ†
    quality_score: float = 0.0
    security_score: Optional[float] = None
    
    # è¿›åº¦ç™¾åˆ†æ¯”
    progress_percentage: float = 0.0
    
    # æ—¶é—´
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # é…ç½®
    audit_scope: Optional[dict] = None
    target_vulnerabilities: Optional[List[str]] = None
    verification_level: Optional[str] = None
    exclude_patterns: Optional[List[str]] = None
    target_files: Optional[List[str]] = None
    
    # é”™è¯¯ä¿¡æ¯
    error_message: Optional[str] = None
    
    class Config:
        from_attributes = True


class AgentEventResponse(BaseModel):
    """Agent äº‹ä»¶å“åº”"""
    id: str
    task_id: str
    event_type: str
    phase: Optional[str]
    message: Optional[str] = None
    sequence: int
    # ğŸ”¥ ORM å­—æ®µåæ˜¯ created_atï¼Œåºåˆ—åŒ–ä¸º timestamp
    created_at: datetime = Field(serialization_alias="timestamp")

    # å·¥å…·ç›¸å…³å­—æ®µ
    tool_name: Optional[str] = None
    tool_input: Optional[Dict[str, Any]] = None
    tool_output: Optional[Dict[str, Any]] = None
    tool_duration_ms: Optional[int] = None

    # å…¶ä»–å­—æ®µ
    progress_percent: Optional[float] = None
    finding_id: Optional[str] = None
    tokens_used: Optional[int] = None
    # ğŸ”¥ ORM å­—æ®µåæ˜¯ event_metadataï¼Œåºåˆ—åŒ–ä¸º metadata
    event_metadata: Optional[Dict[str, Any]] = Field(default=None, serialization_alias="metadata")

    model_config = {
        "from_attributes": True,
        "populate_by_name": True,
        "by_alias": True,  # ğŸ”¥ å…³é”®ï¼šç¡®ä¿åºåˆ—åŒ–æ—¶ä½¿ç”¨åˆ«å
    }


class AgentFindingResponse(BaseModel):
    """Agent å‘ç°å“åº”"""
    id: str
    task_id: str
    vulnerability_type: str
    severity: str
    title: str
    description: Optional[str]
    file_path: Optional[str]
    line_start: Optional[int]
    line_end: Optional[int]
    code_snippet: Optional[str]
    
    is_verified: bool
    # ğŸ”¥ FIX: Map from ai_confidence in ORM, make Optional with default
    confidence: Optional[float] = Field(default=0.5, validation_alias="ai_confidence")
    status: str
    
    suggestion: Optional[str] = None
    poc: Optional[dict] = None
    
    created_at: datetime
    
    model_config = {
        "from_attributes": True,
        "populate_by_name": True,  # Allow both 'confidence' and 'ai_confidence'
    }


class TaskSummaryResponse(BaseModel):
    """ä»»åŠ¡æ‘˜è¦å“åº”"""
    task_id: str
    status: str
    security_score: Optional[int]
    
    total_findings: int
    verified_findings: int
    
    severity_distribution: Dict[str, int]
    vulnerability_types: Dict[str, int]
    
    duration_seconds: Optional[int]
    phases_completed: List[str]


# ============ åå°ä»»åŠ¡æ‰§è¡Œ ============

# è¿è¡Œä¸­çš„åŠ¨æ€æ‰§è¡Œå™¨
_running_orchestrators: Dict[str, Any] = {}
# è¿è¡Œä¸­çš„äº‹ä»¶ç®¡ç†å™¨ï¼ˆç”¨äº SSE æµï¼‰
_running_event_managers: Dict[str, EventManager] = {}
# ğŸ”¥ å·²å–æ¶ˆçš„ä»»åŠ¡é›†åˆï¼ˆç”¨äºå‰ç½®æ“ä½œçš„å–æ¶ˆæ£€æŸ¥ï¼‰
_cancelled_tasks: Set[str] = set()


def is_task_cancelled(task_id: str) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²è¢«å–æ¶ˆ"""
    return task_id in _cancelled_tasks


async def _execute_agent_task(task_id: str):
    """
    åœ¨åå°æ‰§è¡Œ Agent ä»»åŠ¡ - ä½¿ç”¨åŠ¨æ€ Agent æ ‘æ¶æ„
    
    æ¶æ„ï¼šOrchestratorAgent ä½œä¸ºå¤§è„‘ï¼ŒåŠ¨æ€è°ƒåº¦å­ Agent
    """
    from app.services.agent.agents import OrchestratorAgent, ReconAgent, AnalysisAgent, VerificationAgent
    from app.services.agent.event_manager import EventManager, AgentEventEmitter
    from app.services.llm.service import LLMService
    from app.services.agent.core import agent_registry
    from app.services.agent.tools import SandboxManager
    from app.core.config import settings
    import time
    
    # ğŸ”¥ åœ¨ä»»åŠ¡æœ€å¼€å§‹å°±åˆå§‹åŒ– Docker æ²™ç®±ç®¡ç†å™¨
    # è¿™æ ·å¯ä»¥ç¡®ä¿æ•´ä¸ªä»»åŠ¡ç”Ÿå‘½å‘¨æœŸå†…ä½¿ç”¨åŒä¸€ä¸ªç®¡ç†å™¨ï¼Œå¹¶ä¸”å°½æ—©å‘ç° Docker é—®é¢˜
    logger.info(f"ğŸš€ Starting execution for task {task_id}")
    sandbox_manager = SandboxManager()
    await sandbox_manager.initialize()
    logger.info(f"ğŸ³ Global Sandbox Manager initialized (Available: {sandbox_manager.is_available})")

    # ğŸ”¥ æå‰åˆ›å»ºäº‹ä»¶ç®¡ç†å™¨ï¼Œä»¥ä¾¿åœ¨å…‹éš†ä»“åº“å’Œç´¢å¼•æ—¶å‘é€å®æ—¶æ—¥å¿—
    from app.services.agent.event_manager import EventManager, AgentEventEmitter
    event_manager = EventManager(db_session_factory=async_session_factory)
    event_manager.create_queue(task_id)
    event_emitter = AgentEventEmitter(task_id, event_manager)
    _running_event_managers[task_id] = event_manager

    async with async_session_factory() as db:
        orchestrator = None
        start_time = time.time()

        try:
            # è·å–ä»»åŠ¡
            task = await db.get(AgentTask, task_id, options=[selectinload(AgentTask.project)])
            if not task:
                logger.error(f"Task {task_id} not found")
                return

            # è·å–é¡¹ç›®
            project = task.project
            if not project:
                logger.error(f"Project not found for task {task_id}")
                return

            # ğŸ”¥ å‘é€ä»»åŠ¡å¼€å§‹äº‹ä»¶ - ä½¿ç”¨ phase_start è®©å‰ç«¯çŸ¥é“è¿›å…¥å‡†å¤‡é˜¶æ®µ
            await event_emitter.emit_phase_start("preparation", f"ğŸš€ ä»»åŠ¡å¼€å§‹æ‰§è¡Œ: {project.name}")

            # æ›´æ–°ä»»åŠ¡é˜¶æ®µä¸ºå‡†å¤‡ä¸­
            task.status = AgentTaskStatus.RUNNING
            task.started_at = datetime.now(timezone.utc)
            task.current_phase = AgentTaskPhase.PLANNING  # preparation å¯¹åº” PLANNING
            await db.commit()

            # è·å–ç”¨æˆ·é…ç½®ï¼ˆéœ€è¦åœ¨è·å–é¡¹ç›®æ ¹ç›®å½•ä¹‹å‰ï¼Œä»¥ä¾¿ä¼ é€’ tokenï¼‰
            user_config = await _get_user_config(db, task.created_by)

            # ä»ç”¨æˆ·é…ç½®ä¸­æå– tokenï¼ˆç”¨äºç§æœ‰ä»“åº“å…‹éš†ï¼‰
            other_config = (user_config or {}).get('otherConfig', {})
            github_token = other_config.get('githubToken') or settings.GITHUB_TOKEN
            gitlab_token = other_config.get('gitlabToken') or settings.GITLAB_TOKEN

            # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆä¼ é€’ä»»åŠ¡æŒ‡å®šçš„åˆ†æ”¯å’Œè®¤è¯ tokenï¼‰
            # ğŸ”¥ ä¼ é€’ event_emitter ä»¥å‘é€å…‹éš†è¿›åº¦
            project_root = await _get_project_root(
                project,
                task_id,
                task.branch_name,
                github_token=github_token,
                gitlab_token=gitlab_token,
                event_emitter=event_emitter,  # ğŸ”¥ æ–°å¢
            )

            logger.info(f"ğŸš€ Task {task_id} started with Dynamic Agent Tree architecture")

            # ğŸ”¥ è·å–é¡¹ç›®æ ¹ç›®å½•åæ£€æŸ¥å–æ¶ˆ
            if is_task_cancelled(task_id):
                logger.info(f"[Cancel] Task {task_id} cancelled after project preparation")
                raise asyncio.CancelledError("ä»»åŠ¡å·²å–æ¶ˆ")

            # åˆ›å»º LLM æœåŠ¡
            llm_service = LLMService(user_config=user_config)

            # åˆå§‹åŒ–å·¥å…·é›† - ä¼ é€’æ’é™¤æ¨¡å¼å’Œç›®æ ‡æ–‡ä»¶ä»¥åŠé¢„åˆå§‹åŒ–çš„ sandbox_manager
            # ğŸ”¥ ä¼ é€’ event_emitter ä»¥å‘é€ç´¢å¼•è¿›åº¦ï¼Œä¼ é€’ task_id ä»¥æ”¯æŒå–æ¶ˆ
            tools = await _initialize_tools(
                project_root,
                llm_service,
                user_config,
                sandbox_manager=sandbox_manager,
                exclude_patterns=task.exclude_patterns,
                target_files=task.target_files,
                project_id=str(project.id),  # ğŸ”¥ ä¼ é€’ project_id ç”¨äº RAG
                event_emitter=event_emitter,  # ğŸ”¥ æ–°å¢
                task_id=task_id,  # ğŸ”¥ æ–°å¢ï¼šç”¨äºå–æ¶ˆæ£€æŸ¥
            )

            # ğŸ”¥ åˆå§‹åŒ–å·¥å…·åæ£€æŸ¥å–æ¶ˆ
            if is_task_cancelled(task_id):
                logger.info(f"[Cancel] Task {task_id} cancelled after tools initialization")
                raise asyncio.CancelledError("ä»»åŠ¡å·²å–æ¶ˆ")

            # åˆ›å»ºå­ Agent
            recon_agent = ReconAgent(
                llm_service=llm_service,
                tools=tools.get("recon", {}),
                event_emitter=event_emitter,
            )

            analysis_agent = AnalysisAgent(
                llm_service=llm_service,
                tools=tools.get("analysis", {}),
                event_emitter=event_emitter,
            )

            verification_agent = VerificationAgent(
                llm_service=llm_service,
                tools=tools.get("verification", {}),
                event_emitter=event_emitter,
            )

            # åˆ›å»º Orchestrator Agent
            orchestrator = OrchestratorAgent(
                llm_service=llm_service,
                tools=tools.get("orchestrator", {}),
                event_emitter=event_emitter,
                sub_agents={
                    "recon": recon_agent,
                    "analysis": analysis_agent,
                    "verification": verification_agent,
                },
            )

            # æ³¨å†Œåˆ°å…¨å±€
            _running_orchestrators[task_id] = orchestrator
            _running_tasks[task_id] = orchestrator  # å…¼å®¹æ—§çš„å–æ¶ˆé€»è¾‘
            _running_event_managers[task_id] = event_manager  # ç”¨äº SSE æµ
            
            # ğŸ”¥ æ¸…ç†æ—§çš„ Agent æ³¨å†Œè¡¨ï¼Œé¿å…æ˜¾ç¤ºå¤šä¸ªæ ‘
            from app.services.agent.core import agent_registry
            agent_registry.clear()
            
            # æ³¨å†Œ Orchestrator åˆ° Agent Registryï¼ˆä½¿ç”¨å…¶å†…ç½®æ–¹æ³•ï¼‰
            orchestrator._register_to_registry(task="Root orchestrator for security audit")
            
            await event_emitter.emit_info("ğŸ§  åŠ¨æ€ Agent æ ‘æ¶æ„å¯åŠ¨")
            await event_emitter.emit_info(f"ğŸ“ é¡¹ç›®è·¯å¾„: {project_root}")
            
            # æ”¶é›†é¡¹ç›®ä¿¡æ¯ - ä¼ é€’æ’é™¤æ¨¡å¼å’Œç›®æ ‡æ–‡ä»¶
            project_info = await _collect_project_info(
                project_root, 
                project.name,
                exclude_patterns=task.exclude_patterns,
                target_files=task.target_files,
            )
            
            # æ›´æ–°ä»»åŠ¡æ–‡ä»¶ç»Ÿè®¡
            task.total_files = project_info.get("file_count", 0)
            await db.commit()
            
            # æ„å»ºè¾“å…¥æ•°æ®
            input_data = {
                "project_info": project_info,
                "config": {
                    "target_vulnerabilities": task.target_vulnerabilities or [],
                    "verification_level": task.verification_level or "sandbox",
                    "exclude_patterns": task.exclude_patterns or [],
                    "target_files": task.target_files or [],
                    "max_iterations": task.max_iterations or 50,
                },
                "project_root": project_root,
                "task_id": task_id,
            }
            
            # æ‰§è¡Œ Orchestrator
            await event_emitter.emit_phase_start("orchestration", "ğŸ¯ Orchestrator å¼€å§‹ç¼–æ’å®¡è®¡æµç¨‹")
            task.current_phase = AgentTaskPhase.ANALYSIS
            await db.commit()
            
            # ğŸ”¥ å°† orchestrator.run() åŒ…è£…åœ¨ asyncio.Task ä¸­ï¼Œä»¥ä¾¿å¯ä»¥å¼ºåˆ¶å–æ¶ˆ
            run_task = asyncio.create_task(orchestrator.run(input_data))
            _running_asyncio_tasks[task_id] = run_task
            
            try:
                result = await run_task
            finally:
                _running_asyncio_tasks.pop(task_id, None)
            
            # å¤„ç†ç»“æœ
            duration_ms = int((time.time() - start_time) * 1000)
            
            await db.refresh(task)
            
            if result.success:
                # ğŸ”¥ CRITICAL FIX: Log and save findings with detailed debugging
                findings = result.data.get("findings", [])
                logger.info(f"[AgentTask] Task {task_id} completed with {len(findings)} findings from Orchestrator")

                # ğŸ”¥ Debug: Log each finding for verification
                for i, f in enumerate(findings[:5]):  # Log first 5
                    if isinstance(f, dict):
                        logger.debug(f"[AgentTask] Finding {i+1}: {f.get('title', 'N/A')[:50]} - {f.get('severity', 'N/A')}")

                await _save_findings(db, task_id, findings)

                # æ›´æ–°ä»»åŠ¡ç»Ÿè®¡
                task.status = AgentTaskStatus.COMPLETED
                task.completed_at = datetime.now(timezone.utc)
                task.current_phase = AgentTaskPhase.REPORTING
                task.findings_count = len(findings)
                task.total_iterations = result.iterations
                task.tool_calls_count = result.tool_calls
                task.tokens_used = result.tokens_used

                # ğŸ”¥ ç»Ÿè®¡åˆ†æçš„æ–‡ä»¶æ•°é‡ï¼ˆä» findings ä¸­æå–å”¯ä¸€æ–‡ä»¶ï¼‰
                analyzed_file_set = set()
                for f in findings:
                    if isinstance(f, dict):
                        file_path = f.get("file_path") or f.get("file") or f.get("location", "").split(":")[0]
                        if file_path:
                            analyzed_file_set.add(file_path)
                task.analyzed_files = len(analyzed_file_set) if analyzed_file_set else task.total_files

                # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦å’ŒéªŒè¯çŠ¶æ€
                verified_count = 0
                for f in findings:
                    if isinstance(f, dict):
                        sev = str(f.get("severity", "low")).lower()
                        if sev == "critical":
                            task.critical_count += 1
                        elif sev == "high":
                            task.high_count += 1
                        elif sev == "medium":
                            task.medium_count += 1
                        elif sev == "low":
                            task.low_count += 1
                        # ğŸ”¥ ç»Ÿè®¡å·²éªŒè¯çš„å‘ç°
                        if f.get("is_verified") or f.get("verdict") == "confirmed":
                            verified_count += 1
                task.verified_count = verified_count
                
                # è®¡ç®—å®‰å…¨è¯„åˆ†
                task.security_score = _calculate_security_score(findings)
                # ğŸ”¥ æ³¨æ„: progress_percentage æ˜¯è®¡ç®—å±æ€§ï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®
                # å½“ status = COMPLETED æ—¶ä¼šè‡ªåŠ¨è¿”å› 100.0
                
                await db.commit()
                
                await event_emitter.emit_task_complete(
                    findings_count=len(findings),
                    duration_ms=duration_ms,
                )
                
                logger.info(f"âœ… Task {task_id} completed: {len(findings)} findings, {duration_ms}ms")
            else:
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æ˜¯å–æ¶ˆå¯¼è‡´çš„å¤±è´¥
                if result.error == "ä»»åŠ¡å·²å–æ¶ˆ":
                    # çŠ¶æ€å¯èƒ½å·²ç»è¢« cancel API æ›´æ–°ï¼Œåªéœ€ç¡®ä¿ä¸€è‡´æ€§
                    if task.status != AgentTaskStatus.CANCELLED:
                        task.status = AgentTaskStatus.CANCELLED
                        task.completed_at = datetime.now(timezone.utc)
                        await db.commit()
                    logger.info(f"ğŸ›‘ Task {task_id} cancelled")
                else:
                    task.status = AgentTaskStatus.FAILED
                    task.error_message = result.error or "Unknown error"
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
                    
                    await event_emitter.emit_error(result.error or "Unknown error")
                    logger.error(f"âŒ Task {task_id} failed: {result.error}")
            
        except asyncio.CancelledError:
            logger.info(f"Task {task_id} cancelled")
            try:
                task = await db.get(AgentTask, task_id)
                if task:
                    task.status = AgentTaskStatus.CANCELLED
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
            except Exception:
                pass
                
        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}", exc_info=True)
            
            try:
                task = await db.get(AgentTask, task_id)
                if task:
                    task.status = AgentTaskStatus.FAILED
                    task.error_message = str(e)[:1000]
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
            except Exception as db_error:
                logger.error(f"Failed to update task status: {db_error}")
        
        finally:
            # ğŸ”¥ åœ¨æ¸…ç†ä¹‹å‰ä¿å­˜ Agent æ ‘åˆ°æ•°æ®åº“
            try:
                async with async_session_factory() as save_db:
                    await _save_agent_tree(save_db, task_id)
            except Exception as save_error:
                logger.error(f"Failed to save agent tree: {save_error}")

            # æ¸…ç†
            _running_orchestrators.pop(task_id, None)
            _running_tasks.pop(task_id, None)
            _running_event_managers.pop(task_id, None)
            _running_asyncio_tasks.pop(task_id, None)  # ğŸ”¥ æ¸…ç† asyncio task
            _cancelled_tasks.discard(task_id)  # ğŸ”¥ æ¸…ç†å–æ¶ˆæ ‡å¿—

            # ğŸ”¥ æ¸…ç†æ•´ä¸ª Agent æ³¨å†Œè¡¨ï¼ˆåŒ…æ‹¬æ‰€æœ‰å­ Agentï¼‰
            agent_registry.clear()

            logger.debug(f"Task {task_id} cleaned up")


async def _get_user_config(db: AsyncSession, user_id: Optional[str]) -> Optional[Dict[str, Any]]:
    """è·å–ç”¨æˆ·é…ç½®"""
    if not user_id:
        return None
    
    try:
        from app.api.v1.endpoints.config import (
            decrypt_config, 
            SENSITIVE_LLM_FIELDS, SENSITIVE_OTHER_FIELDS
        )
        
        result = await db.execute(
            select(UserConfig).where(UserConfig.user_id == user_id)
        )
        config = result.scalar_one_or_none()
        
        if config and config.llm_config:
            user_llm_config = json.loads(config.llm_config) if config.llm_config else {}
            user_other_config = json.loads(config.other_config) if config.other_config else {}
            
            user_llm_config = decrypt_config(user_llm_config, SENSITIVE_LLM_FIELDS)
            user_other_config = decrypt_config(user_other_config, SENSITIVE_OTHER_FIELDS)
            
            return {
                "llmConfig": user_llm_config,
                "otherConfig": user_other_config,
            }
    except Exception as e:
        logger.warning(f"Failed to get user config: {e}")
    
    return None


async def _initialize_tools(
    project_root: str,
    llm_service,
    user_config: Optional[Dict[str, Any]],
    sandbox_manager: Any, # ä¼ é€’é¢„åˆå§‹åŒ–çš„ SandboxManager
    exclude_patterns: Optional[List[str]] = None,
    target_files: Optional[List[str]] = None,
    project_id: Optional[str] = None,  # ğŸ”¥ ç”¨äº RAG collection_name
    event_emitter: Optional[Any] = None,  # ğŸ”¥ æ–°å¢ï¼šç”¨äºå‘é€å®æ—¶æ—¥å¿—
    task_id: Optional[str] = None,  # ğŸ”¥ æ–°å¢ï¼šç”¨äºå–æ¶ˆæ£€æŸ¥
) -> Dict[str, Dict[str, Any]]:
    """åˆå§‹åŒ–å·¥å…·é›†

    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
        llm_service: LLM æœåŠ¡
        user_config: ç”¨æˆ·é…ç½®
        sandbox_manager: æ²™ç®±ç®¡ç†å™¨
        exclude_patterns: æ’é™¤æ¨¡å¼åˆ—è¡¨
        target_files: ç›®æ ‡æ–‡ä»¶åˆ—è¡¨
        project_id: é¡¹ç›® IDï¼ˆç”¨äº RAG collection_nameï¼‰
        event_emitter: äº‹ä»¶å‘é€å™¨ï¼ˆç”¨äºå‘é€å®æ—¶æ—¥å¿—ï¼‰
        task_id: ä»»åŠ¡ IDï¼ˆç”¨äºå–æ¶ˆæ£€æŸ¥ï¼‰
    """
    from app.services.agent.tools import (
        FileReadTool, FileSearchTool, ListFilesTool,
        PatternMatchTool, CodeAnalysisTool, DataFlowAnalysisTool,
        SemgrepTool, BanditTool, GitleaksTool,
        NpmAuditTool, SafetyTool, TruffleHogTool, OSVScannerTool,  # ğŸ”¥ Added missing tools
        ThinkTool, ReflectTool,
        CreateVulnerabilityReportTool,
        VulnerabilityValidationTool,
        # ğŸ”¥ RAG å·¥å…·
        RAGQueryTool, SecurityCodeSearchTool, FunctionContextTool,
    )
    from app.services.agent.knowledge import (
        SecurityKnowledgeQueryTool,
        GetVulnerabilityKnowledgeTool,
    )
    # ğŸ”¥ RAG ç›¸å…³å¯¼å…¥
    from app.services.rag import CodeIndexer, CodeRetriever, EmbeddingService, IndexUpdateMode
    from app.core.config import settings

    # è¾…åŠ©å‡½æ•°ï¼šå‘é€äº‹ä»¶
    async def emit(message: str, level: str = "info"):
        if event_emitter:
            logger.debug(f"[EMIT-TOOLS] Sending {level}: {message[:60]}...")
            if level == "info":
                await event_emitter.emit_info(message)
            elif level == "warning":
                await event_emitter.emit_warning(message)
            elif level == "error":
                await event_emitter.emit_error(message)
        else:
            logger.warning(f"[EMIT-TOOLS] No event_emitter, skipping: {message[:60]}...")

    # ============ ğŸ”¥ åˆå§‹åŒ– RAG ç³»ç»Ÿ ============
    retriever = None
    try:
        await emit(f"ğŸ” æ­£åœ¨åˆå§‹åŒ– RAG ç³»ç»Ÿ...")

        # ä»ç”¨æˆ·é…ç½®ä¸­è·å– embedding é…ç½®
        user_llm_config = (user_config or {}).get('llmConfig', {})
        user_other_config = (user_config or {}).get('otherConfig', {})
        user_embedding_config = user_other_config.get('embedding_config', {})

        # Embedding Provider ä¼˜å…ˆçº§ï¼šç”¨æˆ·åµŒå…¥é…ç½® > ç¯å¢ƒå˜é‡
        embedding_provider = (
            user_embedding_config.get('provider') or
            getattr(settings, 'EMBEDDING_PROVIDER', 'openai')
        )

        # Embedding Model ä¼˜å…ˆçº§ï¼šç”¨æˆ·åµŒå…¥é…ç½® > ç¯å¢ƒå˜é‡
        embedding_model = (
            user_embedding_config.get('model') or
            getattr(settings, 'EMBEDDING_MODEL', 'text-embedding-3-small')
        )

        # API Key ä¼˜å…ˆçº§ï¼šç”¨æˆ·åµŒå…¥é…ç½® > ç¯å¢ƒå˜é‡ EMBEDDING_API_KEY > ç”¨æˆ· LLM é…ç½® > ç¯å¢ƒå˜é‡ LLM_API_KEY
        # æ³¨æ„ï¼šAPI Key å¯ä»¥å…±äº«ï¼Œå› ä¸ºå¾ˆå¤šç”¨æˆ·ä½¿ç”¨åŒä¸€ä¸ª OpenAI Key åš LLM å’Œ Embedding
        embedding_api_key = (
            user_embedding_config.get('api_key') or
            getattr(settings, 'EMBEDDING_API_KEY', None) or
            user_llm_config.get('llmApiKey') or
            getattr(settings, 'LLM_API_KEY', '') or
            ''
        )

        # Base URL ä¼˜å…ˆçº§ï¼šç”¨æˆ·åµŒå…¥é…ç½® > ç¯å¢ƒå˜é‡ EMBEDDING_BASE_URL > Noneï¼ˆä½¿ç”¨æä¾›å•†é»˜è®¤åœ°å€ï¼‰
        # ğŸ”¥ é‡è¦ï¼šBase URL ä¸åº”è¯¥å›é€€åˆ° LLM çš„ base_urlï¼Œå› ä¸º Embedding å’Œ LLM å¯èƒ½ä½¿ç”¨å®Œå…¨ä¸åŒçš„æœåŠ¡
        # ä¾‹å¦‚ï¼šLLM ä½¿ç”¨ SiliconFlowï¼Œä½† Embedding ä½¿ç”¨ HuggingFace
        embedding_base_url = (
            user_embedding_config.get('base_url') or
            getattr(settings, 'EMBEDDING_BASE_URL', None) or
            None
        )

        logger.info(f"RAG é…ç½®: provider={embedding_provider}, model={embedding_model}, base_url={embedding_base_url or '(ä½¿ç”¨é»˜è®¤)'}")
        await emit(f"ğŸ“Š Embedding é…ç½®: {embedding_provider}/{embedding_model}")

        # åˆ›å»º Embedding æœåŠ¡
        embedding_service = EmbeddingService(
            provider=embedding_provider,
            model=embedding_model,
            api_key=embedding_api_key,
            base_url=embedding_base_url,
        )

        # åˆ›å»º collection_nameï¼ˆåŸºäº project_idï¼‰
        collection_name = f"project_{project_id}" if project_id else "default_project"

        # ğŸ”¥ v2.0: åˆ›å»º CodeIndexer å¹¶è¿›è¡Œæ™ºèƒ½ç´¢å¼•
        # æ™ºèƒ½ç´¢å¼•ä¼šè‡ªåŠ¨ï¼š
        # - æ£€æµ‹ embedding æ¨¡å‹å˜æ›´ï¼Œå¦‚éœ€è¦åˆ™è‡ªåŠ¨é‡å»º
        # - å¯¹æ¯”æ–‡ä»¶ hashï¼Œåªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶ï¼ˆå¢é‡æ›´æ–°ï¼‰
        indexer = CodeIndexer(
            collection_name=collection_name,
            embedding_service=embedding_service,
            persist_directory=settings.VECTOR_DB_PATH,
        )

        logger.info(f"ğŸ“ å¼€å§‹æ™ºèƒ½ç´¢å¼•é¡¹ç›®: {project_root}")
        await emit(f"ğŸ“ æ­£åœ¨æ„å»ºä»£ç å‘é‡ç´¢å¼•...")

        index_progress = None
        last_progress_update = 0
        last_embedding_progress = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        embedding_total = [0]  # è®°å½•æ€»æ•°

        # ğŸ”¥ åµŒå…¥è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆåŒæ­¥ï¼Œä½†ä¼šè°ƒåº¦å¼‚æ­¥ä»»åŠ¡ï¼‰
        def on_embedding_progress(processed: int, total: int):
            embedding_total[0] = total
            # æ¯å¤„ç† 50 ä¸ªæˆ–å®Œæˆæ—¶æ›´æ–°
            if processed - last_embedding_progress[0] >= 50 or processed == total:
                last_embedding_progress[0] = processed
                percentage = (processed / total * 100) if total > 0 else 0
                msg = f"ğŸ”¢ åµŒå…¥è¿›åº¦: {processed}/{total} ({percentage:.0f}%)"
                logger.info(msg)
                # ä½¿ç”¨ asyncio.create_task è°ƒåº¦å¼‚æ­¥ emit
                try:
                    loop = asyncio.get_running_loop()
                    loop.create_task(emit(msg))
                except Exception as e:
                    logger.warning(f"Failed to emit embedding progress: {e}")

        # ğŸ”¥ åˆ›å»ºå–æ¶ˆæ£€æŸ¥å‡½æ•°ï¼Œç”¨äºåœ¨åµŒå…¥æ‰¹å¤„ç†ä¸­æ£€æŸ¥å–æ¶ˆçŠ¶æ€
        def check_cancelled() -> bool:
            return task_id is not None and is_task_cancelled(task_id)

        async for progress in indexer.smart_index_directory(
            directory=project_root,
            exclude_patterns=exclude_patterns or [],
            include_patterns=target_files,  # ğŸ”¥ ä¼ é€’ target_files é™åˆ¶ç´¢å¼•èŒƒå›´
            update_mode=IndexUpdateMode.SMART,
            embedding_progress_callback=on_embedding_progress,
            cancel_check=check_cancelled,  # ğŸ”¥ ä¼ é€’å–æ¶ˆæ£€æŸ¥å‡½æ•°
        ):
            # ğŸ”¥ åœ¨ç´¢å¼•è¿‡ç¨‹ä¸­æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            if check_cancelled():
                logger.info(f"[Cancel] RAG indexing cancelled for task {task_id}")
                raise asyncio.CancelledError("ä»»åŠ¡å·²å–æ¶ˆ")

            index_progress = progress
            # æ¯å¤„ç† 10 ä¸ªæ–‡ä»¶æˆ–æœ‰é‡è¦å˜åŒ–æ—¶å‘é€è¿›åº¦æ›´æ–°
            if progress.processed_files - last_progress_update >= 10 or progress.processed_files == progress.total_files:
                if progress.total_files > 0:
                    await emit(
                        f"ğŸ“ ç´¢å¼•è¿›åº¦: {progress.processed_files}/{progress.total_files} æ–‡ä»¶ "
                        f"({progress.progress_percentage:.0f}%)"
                    )
                last_progress_update = progress.processed_files

            # ğŸ”¥ å‘é€çŠ¶æ€æ¶ˆæ¯ï¼ˆå¦‚åµŒå…¥å‘é‡ç”Ÿæˆè¿›åº¦ï¼‰
            if progress.status_message:
                await emit(progress.status_message)
                progress.status_message = ""  # æ¸…ç©ºå·²å‘é€çš„æ¶ˆæ¯

        if index_progress:
            summary = (
                f"âœ… ç´¢å¼•å®Œæˆ: æ¨¡å¼={index_progress.update_mode}, "
                f"æ–°å¢={index_progress.added_files}, "
                f"æ›´æ–°={index_progress.updated_files}, "
                f"åˆ é™¤={index_progress.deleted_files}, "
                f"ä»£ç å—={index_progress.indexed_chunks}"
            )
            logger.info(summary)
            await emit(summary)

        # åˆ›å»º CodeRetrieverï¼ˆç”¨äºæœç´¢ï¼‰
        # ğŸ”¥ ä¼ é€’ api_keyï¼Œç”¨äºè‡ªåŠ¨é€‚é… collection çš„ embedding é…ç½®
        retriever = CodeRetriever(
            collection_name=collection_name,
            embedding_service=embedding_service,
            persist_directory=settings.VECTOR_DB_PATH,
            api_key=embedding_api_key,  # ğŸ”¥ ä¼ é€’ api_key ä»¥æ”¯æŒè‡ªåŠ¨åˆ‡æ¢ embedding
        )

        logger.info(f"âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ: collection={collection_name}")
        await emit(f"âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

    except Exception as e:
        logger.warning(f"âš ï¸ RAG ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        await emit(f"âš ï¸ RAG ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}", "warning")
        import traceback
        logger.debug(f"RAG åˆå§‹åŒ–å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
        retriever = None

    # åŸºç¡€å·¥å…· - ä¼ é€’æ’é™¤æ¨¡å¼å’Œç›®æ ‡æ–‡ä»¶
    base_tools = {
        "read_file": FileReadTool(project_root, exclude_patterns, target_files),
        "list_files": ListFilesTool(project_root, exclude_patterns, target_files),
        "search_code": FileSearchTool(project_root, exclude_patterns, target_files),
        "think": ThinkTool(),
        "reflect": ReflectTool(),
    }
    
    # Recon å·¥å…·
    recon_tools = {
        **base_tools,
        # ğŸ”¥ å¤–éƒ¨ä¾¦å¯Ÿå·¥å…· (Recon é˜¶æ®µä¹Ÿéœ€è¦ä½¿ç”¨è¿™äº›å·¥å…·æ¥æ”¶é›†åˆæ­¥ä¿¡æ¯)
        "semgrep_scan": SemgrepTool(project_root, sandbox_manager),
        "bandit_scan": BanditTool(project_root, sandbox_manager),
        "gitleaks_scan": GitleaksTool(project_root, sandbox_manager),
        "npm_audit": NpmAuditTool(project_root, sandbox_manager),
        "safety_scan": SafetyTool(project_root, sandbox_manager),
        "trufflehog_scan": TruffleHogTool(project_root, sandbox_manager),
        "osv_scan": OSVScannerTool(project_root, sandbox_manager),
    }

    # ğŸ”¥ æ³¨å†Œ RAG å·¥å…·åˆ° Recon Agent
    if retriever:
        recon_tools["rag_query"] = RAGQueryTool(retriever)
        logger.info("âœ… RAG å·¥å…· (rag_query) å·²æ³¨å†Œåˆ° Recon Agent")
    
    # Analysis å·¥å…·
    # ğŸ”¥ å¯¼å…¥æ™ºèƒ½æ‰«æå·¥å…·
    from app.services.agent.tools import SmartScanTool, QuickAuditTool
    
    analysis_tools = {
        **base_tools,
        # ğŸ”¥ æ™ºèƒ½æ‰«æå·¥å…·ï¼ˆæ¨èé¦–å…ˆä½¿ç”¨ï¼‰
        "smart_scan": SmartScanTool(project_root),
        "quick_audit": QuickAuditTool(project_root),
        # æ¨¡å¼åŒ¹é…å·¥å…·ï¼ˆå¢å¼ºç‰ˆï¼‰
        "pattern_match": PatternMatchTool(project_root),
        # æ•°æ®æµåˆ†æ
        "dataflow_analysis": DataFlowAnalysisTool(llm_service),
        # å¤–éƒ¨å®‰å…¨å·¥å…· (ä¼ å…¥å…±äº«çš„ sandbox_manager)
        "semgrep_scan": SemgrepTool(project_root, sandbox_manager),
        "bandit_scan": BanditTool(project_root, sandbox_manager),
        "gitleaks_scan": GitleaksTool(project_root, sandbox_manager),
        "npm_audit": NpmAuditTool(project_root, sandbox_manager),
        "safety_scan": SafetyTool(project_root, sandbox_manager),
        "trufflehog_scan": TruffleHogTool(project_root, sandbox_manager),
        "osv_scan": OSVScannerTool(project_root, sandbox_manager),
        # å®‰å…¨çŸ¥è¯†æŸ¥è¯¢
        "query_security_knowledge": SecurityKnowledgeQueryTool(),
        "get_vulnerability_knowledge": GetVulnerabilityKnowledgeTool(),
    }

    # ğŸ”¥ æ³¨å†Œ RAG å·¥å…·åˆ° Analysis Agent
    if retriever:
        analysis_tools["rag_query"] = RAGQueryTool(retriever)
        analysis_tools["security_search"] = SecurityCodeSearchTool(retriever)
        analysis_tools["function_context"] = FunctionContextTool(retriever)
        logger.info("âœ… RAG å·¥å…· (rag_query, security_search, function_context) å·²æ³¨å†Œåˆ° Analysis Agent")
    else:
        logger.warning("âš ï¸ RAG æœªåˆå§‹åŒ–ï¼Œrag_query/security_search/function_context å·¥å…·ä¸å¯ç”¨")
    
    # Verification å·¥å…·
    # ğŸ”¥ å¯¼å…¥æ²™ç®±å·¥å…·
    from app.services.agent.tools import (
        SandboxTool, SandboxHttpTool, VulnerabilityVerifyTool,
        # å¤šè¯­è¨€ä»£ç æµ‹è¯•å·¥å…·
        PhpTestTool, PythonTestTool, JavaScriptTestTool, JavaTestTool,
        GoTestTool, RubyTestTool, ShellTestTool, UniversalCodeTestTool,
        # æ¼æ´éªŒè¯ä¸“ç”¨å·¥å…·
        CommandInjectionTestTool, SqlInjectionTestTool, XssTestTool,
        PathTraversalTestTool, SstiTestTool, DeserializationTestTool,
        UniversalVulnTestTool,
    )

    verification_tools = {
        **base_tools,
        # ğŸ”¥ æ²™ç®±éªŒè¯å·¥å…·
        "sandbox_exec": SandboxTool(sandbox_manager),
        "sandbox_http": SandboxHttpTool(sandbox_manager),
        "verify_vulnerability": VulnerabilityVerifyTool(sandbox_manager),

        # ğŸ”¥ å¤šè¯­è¨€ä»£ç æµ‹è¯•å·¥å…·
        "php_test": PhpTestTool(sandbox_manager, project_root),
        "python_test": PythonTestTool(sandbox_manager, project_root),
        "javascript_test": JavaScriptTestTool(sandbox_manager, project_root),
        "java_test": JavaTestTool(sandbox_manager, project_root),
        "go_test": GoTestTool(sandbox_manager, project_root),
        "ruby_test": RubyTestTool(sandbox_manager, project_root),
        "shell_test": ShellTestTool(sandbox_manager, project_root),
        "universal_code_test": UniversalCodeTestTool(sandbox_manager, project_root),

        # ğŸ”¥ æ¼æ´éªŒè¯ä¸“ç”¨å·¥å…·
        "test_command_injection": CommandInjectionTestTool(sandbox_manager, project_root),
        "test_sql_injection": SqlInjectionTestTool(sandbox_manager, project_root),
        "test_xss": XssTestTool(sandbox_manager, project_root),
        "test_path_traversal": PathTraversalTestTool(sandbox_manager, project_root),
        "test_ssti": SstiTestTool(sandbox_manager, project_root),
        "test_deserialization": DeserializationTestTool(sandbox_manager, project_root),
        "universal_vuln_test": UniversalVulnTestTool(sandbox_manager, project_root),

        # æŠ¥å‘Šå·¥å…·
        "create_vulnerability_report": CreateVulnerabilityReportTool(),
    }
    
    # Orchestrator å·¥å…·ï¼ˆä¸»è¦æ˜¯æ€è€ƒå·¥å…·ï¼‰
    orchestrator_tools = {
        "think": ThinkTool(),
        "reflect": ReflectTool(),
    }
    
    return {
        "recon": recon_tools,
        "analysis": analysis_tools,
        "verification": verification_tools,
        "orchestrator": orchestrator_tools,
    }


async def _collect_project_info(
    project_root: str, 
    project_name: str,
    exclude_patterns: Optional[List[str]] = None,
    target_files: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """æ”¶é›†é¡¹ç›®ä¿¡æ¯
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
        project_name: é¡¹ç›®åç§°
        exclude_patterns: æ’é™¤æ¨¡å¼åˆ—è¡¨
        target_files: ç›®æ ‡æ–‡ä»¶åˆ—è¡¨
    
    ğŸ”¥ é‡è¦ï¼šå½“æŒ‡å®šäº† target_files æ—¶ï¼Œè¿”å›çš„é¡¹ç›®ç»“æ„åº”è¯¥åªåŒ…å«ç›®æ ‡æ–‡ä»¶ç›¸å…³çš„ä¿¡æ¯ï¼Œ
    ä»¥ç¡®ä¿ Orchestrator å’Œå­ Agent çœ‹åˆ°çš„æ˜¯ä¸€è‡´çš„ã€è¿‡æ»¤åçš„è§†å›¾ã€‚
    """
    import fnmatch
    
    info = {
        "name": project_name,
        "root": project_root,
        "languages": [],
        "file_count": 0,
        "structure": {},
    }
    
    try:
        # é»˜è®¤æ’é™¤ç›®å½•
        exclude_dirs = {
            "node_modules", "__pycache__", ".git", "venv", ".venv",
            "build", "dist", "target", ".idea", ".vscode",
        }
        
        # ä»ç”¨æˆ·é…ç½®çš„æ’é™¤æ¨¡å¼ä¸­æå–ç›®å½•
        if exclude_patterns:
            for pattern in exclude_patterns:
                if pattern.endswith("/**"):
                    exclude_dirs.add(pattern[:-3])
                elif "/" not in pattern and "*" not in pattern:
                    exclude_dirs.add(pattern)
        
        # ç›®æ ‡æ–‡ä»¶é›†åˆ
        target_files_set = set(target_files) if target_files else None
        
        lang_map = {
            ".py": "Python", ".js": "JavaScript", ".ts": "TypeScript",
            ".java": "Java", ".go": "Go", ".php": "PHP",
            ".rb": "Ruby", ".rs": "Rust", ".c": "C", ".cpp": "C++",
        }
        
        # ğŸ”¥ æ”¶é›†è¿‡æ»¤åçš„æ–‡ä»¶åˆ—è¡¨
        filtered_files = []
        filtered_dirs = set()
        
        for root, dirs, files in os.walk(project_root):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for f in files:
                relative_path = os.path.relpath(os.path.join(root, f), project_root)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡æ–‡ä»¶åˆ—è¡¨ä¸­
                if target_files_set and relative_path not in target_files_set:
                    continue
                
                # æ£€æŸ¥æ’é™¤æ¨¡å¼
                should_skip = False
                if exclude_patterns:
                    for pattern in exclude_patterns:
                        if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(f, pattern):
                            should_skip = True
                            break
                if should_skip:
                    continue
                
                info["file_count"] += 1
                filtered_files.append(relative_path)
                
                # ğŸ”¥ æ”¶é›†æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
                dir_path = os.path.dirname(relative_path)
                if dir_path:
                    # æ·»åŠ ç›®å½•åŠå…¶çˆ¶ç›®å½•
                    parts = dir_path.split(os.sep)
                    for i in range(len(parts)):
                        filtered_dirs.add(os.sep.join(parts[:i+1]))
                
                ext = os.path.splitext(f)[1].lower()
                if ext in lang_map and lang_map[ext] not in info["languages"]:
                    info["languages"].append(lang_map[ext])
        
        # ğŸ”¥ æ ¹æ®æ˜¯å¦æœ‰ç›®æ ‡æ–‡ä»¶é™åˆ¶ï¼Œç”Ÿæˆä¸åŒçš„ç»“æ„ä¿¡æ¯
        if target_files_set:
            # å½“æŒ‡å®šäº†ç›®æ ‡æ–‡ä»¶æ—¶ï¼Œåªæ˜¾ç¤ºç›®æ ‡æ–‡ä»¶å’Œç›¸å…³ç›®å½•
            info["structure"] = {
                "directories": sorted(list(filtered_dirs))[:20],
                "files": filtered_files[:30],
                "scope_limited": True,  # ğŸ”¥ æ ‡è®°è¿™æ˜¯é™å®šèŒƒå›´çš„è§†å›¾
                "scope_message": f"å®¡è®¡èŒƒå›´é™å®šä¸º {len(filtered_files)} ä¸ªæŒ‡å®šæ–‡ä»¶",
            }
        else:
            # å…¨é¡¹ç›®å®¡è®¡æ—¶ï¼Œæ˜¾ç¤ºé¡¶å±‚ç›®å½•ç»“æ„
            try:
                top_items = os.listdir(project_root)
                info["structure"] = {
                    "directories": [d for d in top_items if os.path.isdir(os.path.join(project_root, d)) and d not in exclude_dirs],
                    "files": [f for f in top_items if os.path.isfile(os.path.join(project_root, f))][:20],
                    "scope_limited": False,
                }
            except Exception:
                pass
            
    except Exception as e:
        logger.warning(f"Failed to collect project info: {e}")
    
    return info


async def _save_findings(db: AsyncSession, task_id: str, findings: List[Dict]) -> None:
    """
    ä¿å­˜å‘ç°åˆ°æ•°æ®åº“

    ğŸ”¥ å¢å¼ºç‰ˆï¼šæ”¯æŒå¤šç§ Agent è¾“å‡ºæ ¼å¼ï¼Œå¥å£®çš„å­—æ®µæ˜ å°„
    """
    from app.models.agent_task import VulnerabilityType

    logger.info(f"[SaveFindings] Starting to save {len(findings)} findings for task {task_id}")

    if not findings:
        logger.warning(f"[SaveFindings] No findings to save for task {task_id}")
        return

    # ğŸ”¥ Case-insensitive mapping preparation
    severity_map = {
        "critical": VulnerabilitySeverity.CRITICAL,
        "high": VulnerabilitySeverity.HIGH,
        "medium": VulnerabilitySeverity.MEDIUM,
        "low": VulnerabilitySeverity.LOW,
        "info": VulnerabilitySeverity.INFO,
    }

    type_map = {
        "sql_injection": VulnerabilityType.SQL_INJECTION,
        "nosql_injection": VulnerabilityType.NOSQL_INJECTION,
        "xss": VulnerabilityType.XSS,
        "command_injection": VulnerabilityType.COMMAND_INJECTION,
        "code_injection": VulnerabilityType.CODE_INJECTION,
        "path_traversal": VulnerabilityType.PATH_TRAVERSAL,
        "ssrf": VulnerabilityType.SSRF,
        "xxe": VulnerabilityType.XXE,
        "auth_bypass": VulnerabilityType.AUTH_BYPASS,
        "idor": VulnerabilityType.IDOR,
        "sensitive_data_exposure": VulnerabilityType.SENSITIVE_DATA_EXPOSURE,
        "hardcoded_secret": VulnerabilityType.HARDCODED_SECRET,
        "deserialization": VulnerabilityType.DESERIALIZATION,
        "weak_crypto": VulnerabilityType.WEAK_CRYPTO,
        "file_inclusion": VulnerabilityType.FILE_INCLUSION,
        "race_condition": VulnerabilityType.RACE_CONDITION,
        "business_logic": VulnerabilityType.BUSINESS_LOGIC,
        "memory_corruption": VulnerabilityType.MEMORY_CORRUPTION,
    }

    saved_count = 0
    logger.info(f"Saving {len(findings)} findings for task {task_id}")

    for finding in findings:
        if not isinstance(finding, dict):
            logger.debug(f"[SaveFindings] Skipping non-dict finding: {type(finding)}")
            continue

        try:
            # ğŸ”¥ Handle severity (case-insensitive, support multiple field names)
            raw_severity = str(
                finding.get("severity") or
                finding.get("risk") or
                "medium"
            ).lower().strip()
            severity_enum = severity_map.get(raw_severity, VulnerabilitySeverity.MEDIUM)

            # ğŸ”¥ Handle vulnerability type (case-insensitive & snake_case normalization)
            # Support multiple field names: vulnerability_type, type, vuln_type
            raw_type = str(
                finding.get("vulnerability_type") or
                finding.get("type") or
                finding.get("vuln_type") or
                "other"
            ).lower().strip().replace(" ", "_").replace("-", "_")

            type_enum = type_map.get(raw_type, VulnerabilityType.OTHER)

            # ğŸ”¥ Additional fallback for common Agent output variations
            if "sqli" in raw_type or "sql" in raw_type:
                type_enum = VulnerabilityType.SQL_INJECTION
            if "xss" in raw_type:
                type_enum = VulnerabilityType.XSS
            if "rce" in raw_type or "command" in raw_type or "cmd" in raw_type:
                type_enum = VulnerabilityType.COMMAND_INJECTION
            if "traversal" in raw_type or "lfi" in raw_type or "rfi" in raw_type:
                type_enum = VulnerabilityType.PATH_TRAVERSAL
            if "ssrf" in raw_type:
                type_enum = VulnerabilityType.SSRF
            if "xxe" in raw_type:
                type_enum = VulnerabilityType.XXE
            if "auth" in raw_type:
                type_enum = VulnerabilityType.AUTH_BYPASS
            if "secret" in raw_type or "credential" in raw_type or "password" in raw_type:
                type_enum = VulnerabilityType.HARDCODED_SECRET
            if "deserial" in raw_type:
                type_enum = VulnerabilityType.DESERIALIZATION

            # ğŸ”¥ Handle file path (support multiple field names)
            file_path = (
                finding.get("file_path") or
                finding.get("file") or
                finding.get("location", "").split(":")[0] if ":" in finding.get("location", "") else finding.get("location")
            )

            # ğŸ”¥ Handle line numbers (support multiple formats)
            line_start = finding.get("line_start") or finding.get("line")
            if not line_start and ":" in finding.get("location", ""):
                try:
                    line_start = int(finding.get("location", "").split(":")[1])
                except (ValueError, IndexError):
                    line_start = None

            line_end = finding.get("line_end") or line_start

            # ğŸ”¥ Handle code snippet (support multiple field names)
            code_snippet = (
                finding.get("code_snippet") or
                finding.get("code") or
                finding.get("vulnerable_code")
            )

            # ğŸ”¥ Handle title (generate from type if not provided)
            title = finding.get("title")
            if not title:
                # Generate title from vulnerability type and file
                type_display = raw_type.replace("_", " ").title()
                if file_path:
                    title = f"{type_display} in {os.path.basename(file_path)}"
                else:
                    title = f"{type_display} Vulnerability"

            # ğŸ”¥ Handle description (support multiple field names)
            description = (
                finding.get("description") or
                finding.get("details") or
                finding.get("explanation") or
                finding.get("impact") or
                ""
            )

            # ğŸ”¥ Handle suggestion/recommendation
            suggestion = (
                finding.get("suggestion") or
                finding.get("recommendation") or
                finding.get("remediation") or
                finding.get("fix")
            )

            # ğŸ”¥ Handle confidence (map to ai_confidence field in model)
            confidence = finding.get("confidence") or finding.get("ai_confidence") or 0.5
            if isinstance(confidence, str):
                try:
                    confidence = float(confidence)
                except ValueError:
                    confidence = 0.5

            # ğŸ”¥ Handle verification status
            is_verified = finding.get("is_verified", False)
            if finding.get("verdict") == "confirmed":
                is_verified = True

            # ğŸ”¥ Handle PoC information
            poc_data = finding.get("poc", {})
            has_poc = bool(poc_data)
            poc_code = None
            poc_description = None
            poc_steps = None

            if isinstance(poc_data, dict):
                poc_description = poc_data.get("description")
                poc_steps = poc_data.get("steps")
                poc_code = poc_data.get("payload") or poc_data.get("code")
            elif isinstance(poc_data, str):
                poc_description = poc_data

            # ğŸ”¥ Handle verification details
            verification_method = finding.get("verification_method")
            verification_result = None
            if finding.get("verification_details"):
                verification_result = {"details": finding.get("verification_details")}

            # ğŸ”¥ Handle CWE and CVSS
            cwe_id = finding.get("cwe_id") or finding.get("cwe")
            cvss_score = finding.get("cvss_score") or finding.get("cvss")
            if isinstance(cvss_score, str):
                try:
                    cvss_score = float(cvss_score)
                except ValueError:
                    cvss_score = None

            db_finding = AgentFinding(
                id=str(uuid4()),
                task_id=task_id,
                vulnerability_type=type_enum,
                severity=severity_enum,
                title=title[:500] if title else "Unknown Vulnerability",
                description=description[:5000] if description else "",
                file_path=file_path[:500] if file_path else None,
                line_start=line_start,
                line_end=line_end,
                code_snippet=code_snippet[:10000] if code_snippet else None,
                suggestion=suggestion[:5000] if suggestion else None,
                is_verified=is_verified,
                ai_confidence=confidence,  # ğŸ”¥ FIX: Use ai_confidence, not confidence
                status=FindingStatus.VERIFIED if is_verified else FindingStatus.NEW,
                # ğŸ”¥ Additional fields
                has_poc=has_poc,
                poc_code=poc_code,
                poc_description=poc_description,
                poc_steps=poc_steps,
                verification_method=verification_method,
                verification_result=verification_result,
                cvss_score=cvss_score,
                # References for CWE
                references=[{"cwe": cwe_id}] if cwe_id else None,
            )
            db.add(db_finding)
            saved_count += 1
            logger.debug(f"[SaveFindings] Prepared finding: {title[:50]}... ({severity_enum})")

        except Exception as e:
            logger.warning(f"Failed to save finding: {e}, data: {finding}")
            import traceback
            logger.debug(f"[SaveFindings] Traceback: {traceback.format_exc()}")

    logger.info(f"Successfully prepared {saved_count} findings for commit")

    try:
        await db.commit()
        logger.info(f"[SaveFindings] Successfully committed {saved_count} findings to database")
    except Exception as e:
        logger.error(f"Failed to commit findings: {e}")
        await db.rollback()


def _calculate_security_score(findings: List[Dict]) -> float:
    """è®¡ç®—å®‰å…¨è¯„åˆ†"""
    if not findings:
        return 100.0

    # åŸºäºå‘ç°çš„ä¸¥é‡ç¨‹åº¦è®¡ç®—æ‰£åˆ†
    deductions = {
        "critical": 25,
        "high": 15,
        "medium": 8,
        "low": 3,
        "info": 1,
    }

    total_deduction = 0
    for f in findings:
        if isinstance(f, dict):
            sev = f.get("severity", "low")
            total_deduction += deductions.get(sev, 3)

    score = max(0, 100 - total_deduction)
    return float(score)


async def _save_agent_tree(db: AsyncSession, task_id: str) -> None:
    """
    ä¿å­˜ Agent æ ‘åˆ°æ•°æ®åº“

    ğŸ”¥ åœ¨ä»»åŠ¡å®Œæˆå‰è°ƒç”¨ï¼Œå°†å†…å­˜ä¸­çš„ Agent æ ‘æŒä¹…åŒ–åˆ°æ•°æ®åº“
    """
    from app.models.agent_task import AgentTreeNode
    from app.services.agent.core import agent_registry

    try:
        tree = agent_registry.get_agent_tree()
        nodes = tree.get("nodes", {})

        if not nodes:
            logger.warning(f"[SaveAgentTree] No agent nodes to save for task {task_id}")
            return

        logger.info(f"[SaveAgentTree] Saving {len(nodes)} agent nodes for task {task_id}")

        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ·±åº¦
        def get_depth(agent_id: str, visited: set = None) -> int:
            if visited is None:
                visited = set()
            if agent_id in visited:
                return 0
            visited.add(agent_id)
            node = nodes.get(agent_id)
            if not node:
                return 0
            parent_id = node.get("parent_id")
            if not parent_id:
                return 0
            return 1 + get_depth(parent_id, visited)

        saved_count = 0
        for agent_id, node_data in nodes.items():
            # è·å– Agent å®ä¾‹çš„ç»Ÿè®¡æ•°æ®
            agent_instance = agent_registry.get_agent(agent_id)
            iterations = 0
            tool_calls = 0
            tokens_used = 0

            if agent_instance and hasattr(agent_instance, 'get_stats'):
                stats = agent_instance.get_stats()
                iterations = stats.get("iterations", 0)
                tool_calls = stats.get("tool_calls", 0)
                tokens_used = stats.get("tokens_used", 0)

            # ä»ç»“æœä¸­è·å–å‘ç°æ•°é‡
            findings_count = 0
            result_summary = None
            if node_data.get("result"):
                result = node_data.get("result", {})
                if isinstance(result, dict):
                    findings_count = len(result.get("findings", []))
                    if result.get("summary"):
                        result_summary = str(result.get("summary"))[:2000]

            tree_node = AgentTreeNode(
                id=str(uuid4()),
                task_id=task_id,
                agent_id=agent_id,
                agent_name=node_data.get("name", "Unknown"),
                agent_type=node_data.get("type", "unknown"),
                parent_agent_id=node_data.get("parent_id"),
                depth=get_depth(agent_id),
                task_description=node_data.get("task"),
                knowledge_modules=node_data.get("knowledge_modules"),
                status=node_data.get("status", "unknown"),
                result_summary=result_summary,
                findings_count=findings_count,
                iterations=iterations,
                tool_calls=tool_calls,
                tokens_used=tokens_used,
            )
            db.add(tree_node)
            saved_count += 1

        await db.commit()
        logger.info(f"[SaveAgentTree] Successfully saved {saved_count} agent nodes to database")

    except Exception as e:
        logger.error(f"[SaveAgentTree] Failed to save agent tree: {e}", exc_info=True)
        await db.rollback()


# ============ API Endpoints ============

@router.post("/", response_model=AgentTaskResponse)
async def create_agent_task(
    request: AgentTaskCreate,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    åˆ›å»ºå¹¶å¯åŠ¨ Agent å®¡è®¡ä»»åŠ¡
    """
    # éªŒè¯é¡¹ç›®
    project = await db.get(Project, request.project_id)
    if not project:
        raise HTTPException(status_code=404, detail="é¡¹ç›®ä¸å­˜åœ¨")
    
    if project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤é¡¹ç›®")
    
    # åˆ›å»ºä»»åŠ¡
    task = AgentTask(
        id=str(uuid4()),
        project_id=project.id,
        name=request.name or f"Agent Audit - {datetime.now().strftime('%Y%m%d_%H%M%S')}",
        description=request.description,
        status=AgentTaskStatus.PENDING,
        current_phase=AgentTaskPhase.PLANNING,
        target_vulnerabilities=request.target_vulnerabilities,
        verification_level=request.verification_level or "sandbox",
        branch_name=request.branch_name,  # ä¿å­˜ç”¨æˆ·é€‰æ‹©çš„åˆ†æ”¯
        exclude_patterns=request.exclude_patterns,
        target_files=request.target_files,
        max_iterations=request.max_iterations or 50,
        timeout_seconds=request.timeout_seconds or 1800,
        created_by=current_user.id,
    )
    
    db.add(task)
    await db.commit()
    await db.refresh(task)
    
    # åœ¨åå°å¯åŠ¨ä»»åŠ¡ï¼ˆé¡¹ç›®æ ¹ç›®å½•åœ¨ä»»åŠ¡å†…éƒ¨è·å–ï¼‰
    background_tasks.add_task(_execute_agent_task, task.id)
    
    logger.info(f"Created agent task {task.id} for project {project.name}")
    
    return task


@router.get("/", response_model=List[AgentTaskResponse])
async def list_agent_tasks(
    project_id: Optional[str] = None,
    status: Optional[str] = None,
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å– Agent ä»»åŠ¡åˆ—è¡¨
    """
    # è·å–ç”¨æˆ·çš„é¡¹ç›®
    projects_result = await db.execute(
        select(Project.id).where(Project.owner_id == current_user.id)
    )
    user_project_ids = [p[0] for p in projects_result.fetchall()]
    
    if not user_project_ids:
        return []
    
    # æ„å»ºæŸ¥è¯¢
    query = select(AgentTask).where(AgentTask.project_id.in_(user_project_ids))
    
    if project_id:
        query = query.where(AgentTask.project_id == project_id)
    
    if status:
        try:
            status_enum = AgentTaskStatus(status)
            query = query.where(AgentTask.status == status_enum)
        except ValueError:
            pass
    
    query = query.order_by(AgentTask.created_at.desc())
    query = query.offset(skip).limit(limit)
    
    result = await db.execute(query)
    tasks = result.scalars().all()
    
    return tasks


@router.get("/{task_id}", response_model=AgentTaskResponse)
async def get_agent_task(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å– Agent ä»»åŠ¡è¯¦æƒ…
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æƒé™
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    # æ„å»ºå“åº”ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½åŒ…å«
    try:
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        progress = 0.0
        if hasattr(task, 'progress_percentage'):
            progress = task.progress_percentage
        elif task.status == AgentTaskStatus.COMPLETED:
            progress = 100.0
        elif task.status in [AgentTaskStatus.FAILED, AgentTaskStatus.CANCELLED]:
            progress = 0.0
        
        # ğŸ”¥ ä»è¿è¡Œä¸­çš„ Orchestrator è·å–å®æ—¶ç»Ÿè®¡
        total_iterations = task.total_iterations or 0
        tool_calls_count = task.tool_calls_count or 0
        tokens_used = task.tokens_used or 0
        
        orchestrator = _running_orchestrators.get(task_id)
        if orchestrator and task.status == AgentTaskStatus.RUNNING:
            # ä» Orchestrator è·å–ç»Ÿè®¡
            stats = orchestrator.get_stats()
            total_iterations = stats.get("iterations", 0)
            tool_calls_count = stats.get("tool_calls", 0)
            tokens_used = stats.get("tokens_used", 0)
            
            # ç´¯åŠ å­ Agent çš„ç»Ÿè®¡
            if hasattr(orchestrator, 'sub_agents'):
                for agent in orchestrator.sub_agents.values():
                    if hasattr(agent, 'get_stats'):
                        sub_stats = agent.get_stats()
                        total_iterations += sub_stats.get("iterations", 0)
                        tool_calls_count += sub_stats.get("tool_calls", 0)
                        tokens_used += sub_stats.get("tokens_used", 0)
        
        # æ‰‹åŠ¨æ„å»ºå“åº”æ•°æ®
        response_data = {
            "id": task.id,
            "project_id": task.project_id,
            "name": task.name,
            "description": task.description,
            "task_type": task.task_type or "agent_audit",
            "status": task.status,
            "current_phase": task.current_phase,
            "current_step": task.current_step,
            "total_files": task.total_files or 0,
            "indexed_files": task.indexed_files or 0,
            "analyzed_files": task.analyzed_files or 0,
            "total_chunks": task.total_chunks or 0,
            "total_iterations": total_iterations,
            "tool_calls_count": tool_calls_count,
            "tokens_used": tokens_used,
            "findings_count": task.findings_count or 0,
            "total_findings": task.findings_count or 0,  # å…¼å®¹å­—æ®µ
            "verified_count": task.verified_count or 0,
            "verified_findings": task.verified_count or 0,  # å…¼å®¹å­—æ®µ
            "false_positive_count": task.false_positive_count or 0,
            "critical_count": task.critical_count or 0,
            "high_count": task.high_count or 0,
            "medium_count": task.medium_count or 0,
            "low_count": task.low_count or 0,
            "quality_score": float(task.quality_score or 0.0),
            "security_score": float(task.security_score) if task.security_score is not None else None,
            "progress_percentage": progress,
            "created_at": task.created_at,
            "started_at": task.started_at,
            "completed_at": task.completed_at,
            "error_message": task.error_message,
            "audit_scope": task.audit_scope,
            "target_vulnerabilities": task.target_vulnerabilities,
            "verification_level": task.verification_level,
            "exclude_patterns": task.exclude_patterns,
            "target_files": task.target_files,
        }
        
        return AgentTaskResponse(**response_data)
    except Exception as e:
        logger.error(f"Error serializing task {task_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åºåˆ—åŒ–ä»»åŠ¡æ•°æ®å¤±è´¥: {str(e)}")


@router.post("/{task_id}/cancel")
async def cancel_agent_task(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    å–æ¶ˆ Agent ä»»åŠ¡
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œæ­¤ä»»åŠ¡")

    if task.status in [AgentTaskStatus.COMPLETED, AgentTaskStatus.FAILED, AgentTaskStatus.CANCELLED]:
        raise HTTPException(status_code=400, detail="ä»»åŠ¡å·²ç»“æŸï¼Œæ— æ³•å–æ¶ˆ")

    # ğŸ”¥ 0. ç«‹å³æ ‡è®°ä»»åŠ¡ä¸ºå·²å–æ¶ˆï¼ˆç”¨äºå‰ç½®æ“ä½œçš„å–æ¶ˆæ£€æŸ¥ï¼‰
    _cancelled_tasks.add(task_id)
    logger.info(f"[Cancel] Added task {task_id} to cancelled set")

    # ğŸ”¥ 1. è®¾ç½® Agent çš„å–æ¶ˆæ ‡å¿—
    runner = _running_tasks.get(task_id)
    if runner:
        runner.cancel()
        logger.info(f"[Cancel] Set cancel flag for task {task_id}")
    
    # ğŸ”¥ 2. å¼ºåˆ¶å–æ¶ˆ asyncio Taskï¼ˆç«‹å³ä¸­æ–­ LLM è°ƒç”¨ï¼‰
    asyncio_task = _running_asyncio_tasks.get(task_id)
    if asyncio_task and not asyncio_task.done():
        asyncio_task.cancel()
        logger.info(f"[Cancel] Cancelled asyncio task for {task_id}")
    
    # æ›´æ–°çŠ¶æ€
    task.status = AgentTaskStatus.CANCELLED
    task.completed_at = datetime.now(timezone.utc)
    await db.commit()
    
    logger.info(f"[Cancel] Task {task_id} cancelled successfully")
    return {"message": "ä»»åŠ¡å·²å–æ¶ˆ", "task_id": task_id}


@router.get("/{task_id}/events")
async def stream_agent_events(
    task_id: str,
    after_sequence: int = Query(0, ge=0, description="ä»å“ªä¸ªåºå·ä¹‹åå¼€å§‹"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
):
    """
    è·å– Agent äº‹ä»¶æµ (SSE)
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    async def event_generator():
        """ç”Ÿæˆ SSE äº‹ä»¶æµ"""
        last_sequence = after_sequence
        poll_interval = 0.5
        max_idle = 300  # 5 åˆ†é’Ÿæ— äº‹ä»¶åå…³é—­
        idle_time = 0
        
        while True:
            # æŸ¥è¯¢æ–°äº‹ä»¶
            async with async_session_factory() as session:
                result = await session.execute(
                    select(AgentEvent)
                    .where(AgentEvent.task_id == task_id)
                    .where(AgentEvent.sequence > last_sequence)
                    .order_by(AgentEvent.sequence)
                    .limit(50)
                )
                events = result.scalars().all()
                
                # è·å–ä»»åŠ¡çŠ¶æ€
                current_task = await session.get(AgentTask, task_id)
                task_status = current_task.status if current_task else None
            
            if events:
                idle_time = 0
                for event in events:
                    last_sequence = event.sequence
                    # event_type å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œä¸éœ€è¦ .value
                    event_type_str = str(event.event_type)
                    phase_str = str(event.phase) if event.phase else None
                    
                    data = {
                        "id": event.id,
                        "type": event_type_str,
                        "phase": phase_str,
                        "message": event.message,
                        "sequence": event.sequence,
                        "timestamp": event.created_at.isoformat() if event.created_at else None,
                        "progress_percent": event.progress_percent,
                        "tool_name": event.tool_name,
                    }
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            else:
                idle_time += poll_interval
            
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦ç»“æŸ
            if task_status:
                # task_status å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                status_str = str(task_status)
                if status_str in ["completed", "failed", "cancelled"]:
                    yield f"data: {json.dumps({'type': 'task_end', 'status': status_str})}\n\n"
                    break
            
            # æ£€æŸ¥ç©ºé—²è¶…æ—¶
            if idle_time >= max_idle:
                yield f"data: {json.dumps({'type': 'timeout'})}\n\n"
                break
            
            await asyncio.sleep(poll_interval)
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.get("/{task_id}/stream")
async def stream_agent_with_thinking(
    task_id: str,
    include_thinking: bool = Query(True, description="æ˜¯å¦åŒ…å« LLM æ€è€ƒè¿‡ç¨‹"),
    include_tool_calls: bool = Query(True, description="æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨è¯¦æƒ…"),
    after_sequence: int = Query(0, ge=0, description="ä»å“ªä¸ªåºå·ä¹‹åå¼€å§‹"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
):
    """
    å¢å¼ºç‰ˆäº‹ä»¶æµ (SSE)
    
    æ”¯æŒ:
    - LLM æ€è€ƒè¿‡ç¨‹çš„ Token çº§æµå¼è¾“å‡º (ä»…è¿è¡Œæ—¶)
    - å·¥å…·è°ƒç”¨çš„è¯¦ç»†è¾“å…¥/è¾“å‡º
    - èŠ‚ç‚¹æ‰§è¡ŒçŠ¶æ€
    - å‘ç°äº‹ä»¶
    
    ä¼˜å…ˆä½¿ç”¨å†…å­˜ä¸­çš„äº‹ä»¶é˜Ÿåˆ— (æ”¯æŒ thinking_token)ï¼Œ
    å¦‚æœä»»åŠ¡æœªåœ¨è¿è¡Œï¼Œåˆ™å›é€€åˆ°æ•°æ®åº“è½®è¯¢ (ä¸æ”¯æŒ thinking_token å¤ç›˜)ã€‚
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    # å®šä¹‰ SSE æ ¼å¼åŒ–å‡½æ•°
    def format_sse_event(event_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶"""
        event_type = event_data.get("event_type") or event_data.get("type")
        
        # ç»Ÿä¸€å­—æ®µ
        if "type" not in event_data:
            event_data["type"] = event_type
            
        return f"event: {event_type}\ndata: {json.dumps(event_data, ensure_ascii=False)}\n\n"

    async def enhanced_event_generator():
        """ç”Ÿæˆå¢å¼ºç‰ˆ SSE äº‹ä»¶æµ"""
        # 1. æ£€æŸ¥ä»»åŠ¡æ˜¯å¦åœ¨è¿è¡Œä¸­ (å†…å­˜)
        event_manager = _running_event_managers.get(task_id)
        
        if event_manager:
            logger.debug(f"Stream {task_id}: Using in-memory event manager")
            try:
                # ä½¿ç”¨ EventManager çš„æµå¼æ¥å£
                # è¿‡æ»¤é€‰é¡¹
                skip_types = set()
                if not include_thinking:
                    skip_types.update(["thinking_start", "thinking_token", "thinking_end"])
                if not include_tool_calls:
                    skip_types.update(["tool_call_start", "tool_call_input", "tool_call_output", "tool_call_end"])
                
                async for event in event_manager.stream_events(task_id, after_sequence=after_sequence):
                    event_type = event.get("event_type")
                    
                    if event_type in skip_types:
                        continue
                    
                    # ğŸ”¥ Debug: è®°å½• thinking_token äº‹ä»¶
                    if event_type == "thinking_token":
                        token = event.get("metadata", {}).get("token", "")[:20]
                        logger.debug(f"Stream {task_id}: Sending thinking_token: '{token}...'")
                        
                    # æ ¼å¼åŒ–å¹¶ yield
                    yield format_sse_event(event)
                    
                    # ğŸ”¥ CRITICAL: ä¸º thinking_token æ·»åŠ å¾®å°å»¶è¿Ÿ
                    # ç¡®ä¿äº‹ä»¶åœ¨ä¸åŒçš„ TCP åŒ…ä¸­å‘é€ï¼Œè®©å‰ç«¯èƒ½å¤Ÿé€ä¸ªå¤„ç†
                    # æ²¡æœ‰è¿™ä¸ªå»¶è¿Ÿï¼Œæ‰€æœ‰ token ä¼šåœ¨ä¸€æ¬¡ read() ä¸­è¢«æ¥æ”¶ï¼Œå¯¼è‡´ React æ‰¹é‡æ›´æ–°
                    if event_type == "thinking_token":
                        await asyncio.sleep(0.01)  # 10ms å»¶è¿Ÿ
                    
            except Exception as e:
                logger.error(f"In-memory stream error: {e}")
                err_data = {"type": "error", "message": str(e)}
                yield format_sse_event(err_data)
                
        else:
            logger.debug(f"Stream {task_id}: Task not running, falling back to DB polling")
            # 2. å›é€€åˆ°æ•°æ®åº“è½®è¯¢ (æ— æ³•è·å– thinking_token)
            last_sequence = after_sequence
            poll_interval = 2.0  # å®Œæˆçš„ä»»åŠ¡è½®è¯¢å¯ä»¥æ…¢ä¸€ç‚¹
            heartbeat_interval = 15
            max_idle = 60  # 1åˆ†é’Ÿæ— äº‹ä»¶å…³é—­
            idle_time = 0
            last_heartbeat = 0
            
            skip_types = set()
            if not include_thinking:
                skip_types.update(["thinking_start", "thinking_token", "thinking_end"])
            
            while True:
                try:
                    async with async_session_factory() as session:
                        # æŸ¥è¯¢æ–°äº‹ä»¶
                        result = await session.execute(
                            select(AgentEvent)
                            .where(AgentEvent.task_id == task_id)
                            .where(AgentEvent.sequence > last_sequence)
                            .order_by(AgentEvent.sequence)
                            .limit(100)
                        )
                        events = result.scalars().all()
                        
                        # è·å–ä»»åŠ¡çŠ¶æ€
                        current_task = await session.get(AgentTask, task_id)
                        task_status = current_task.status if current_task else None
                    
                    if events:
                        idle_time = 0
                        for event in events:
                            last_sequence = event.sequence
                            event_type = str(event.event_type)
                            
                            if event_type in skip_types:
                                continue
                            
                            # æ„å»ºæ•°æ®
                            data = {
                                "id": event.id,
                                "type": event_type,
                                "phase": str(event.phase) if event.phase else None,
                                "message": event.message,
                                "sequence": event.sequence,
                                "timestamp": event.created_at.isoformat() if event.created_at else None,
                            }
                            
                            # æ·»åŠ è¯¦æƒ…
                            if include_tool_calls and event.tool_name:
                                data["tool"] = {
                                    "name": event.tool_name,
                                    "input": event.tool_input,
                                    "output": event.tool_output,
                                    "duration_ms": event.tool_duration_ms,
                                }
                                
                            if event.event_metadata:
                                data["metadata"] = event.event_metadata
                                
                            if event.tokens_used:
                                data["tokens_used"] = event.tokens_used
                            
                            yield format_sse_event(data)
                    else:
                        idle_time += poll_interval
                        
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸ
                        if task_status:
                            status_str = str(task_status)
                            # å¦‚æœä»»åŠ¡å·²å®Œæˆä¸”æ²¡æœ‰æ–°äº‹ä»¶ï¼Œç»“æŸæµ
                            if status_str in ["completed", "failed", "cancelled"]:
                                end_data = {
                                    "type": "task_end",
                                    "status": status_str,
                                    "message": f"ä»»åŠ¡å·²{status_str}"
                                }
                                yield format_sse_event(end_data)
                                break
                    
                    # å¿ƒè·³
                    last_heartbeat += poll_interval
                    if last_heartbeat >= heartbeat_interval:
                        last_heartbeat = 0
                        yield format_sse_event({"type": "heartbeat", "timestamp": datetime.now(timezone.utc).isoformat()})
                    
                    # è¶…æ—¶
                    if idle_time >= max_idle:
                        break
                    
                    await asyncio.sleep(poll_interval)
                    
                except Exception as e:
                    logger.error(f"DB poll stream error: {e}")
                    yield format_sse_event({"type": "error", "message": str(e)})
                    break
    
    return StreamingResponse(
        enhanced_event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Type": "text/event-stream; charset=utf-8",
        }
    )


@router.get("/{task_id}/events/list", response_model=List[AgentEventResponse])
async def list_agent_events(
    task_id: str,
    after_sequence: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=500),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å– Agent äº‹ä»¶åˆ—è¡¨
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")

    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")

    result = await db.execute(
        select(AgentEvent)
        .where(AgentEvent.task_id == task_id)
        .where(AgentEvent.sequence > after_sequence)
        .order_by(AgentEvent.sequence)
        .limit(limit)
    )
    events = result.scalars().all()

    # ğŸ”¥ Debug logging
    logger.debug(f"[EventsList] Task {task_id}: returning {len(events)} events (after_sequence={after_sequence})")
    if events:
        logger.debug(f"[EventsList] First event: type={events[0].event_type}, seq={events[0].sequence}")
        if len(events) > 1:
            logger.debug(f"[EventsList] Last event: type={events[-1].event_type}, seq={events[-1].sequence}")

    return events


@router.get("/{task_id}/findings", response_model=List[AgentFindingResponse])
async def list_agent_findings(
    task_id: str,
    severity: Optional[str] = None,
    verified_only: bool = False,
    skip: int = Query(0, ge=0),
    limit: int = Query(50, ge=1, le=200),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å– Agent å‘ç°åˆ—è¡¨
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    query = select(AgentFinding).where(AgentFinding.task_id == task_id)
    
    if severity:
        try:
            sev_enum = VulnerabilitySeverity(severity)
            query = query.where(AgentFinding.severity == sev_enum)
        except ValueError:
            pass
    
    if verified_only:
        query = query.where(AgentFinding.is_verified == True)
    
    # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
    severity_order = {
        VulnerabilitySeverity.CRITICAL: 0,
        VulnerabilitySeverity.HIGH: 1,
        VulnerabilitySeverity.MEDIUM: 2,
        VulnerabilitySeverity.LOW: 3,
        VulnerabilitySeverity.INFO: 4,
    }
    
    query = query.order_by(AgentFinding.severity, AgentFinding.created_at.desc())
    query = query.offset(skip).limit(limit)
    
    result = await db.execute(query)
    findings = result.scalars().all()
    
    return findings


@router.get("/{task_id}/summary", response_model=TaskSummaryResponse)
async def get_task_summary(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å–ä»»åŠ¡æ‘˜è¦
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    # è·å–æ‰€æœ‰å‘ç°
    result = await db.execute(
        select(AgentFinding).where(AgentFinding.task_id == task_id)
    )
    findings = result.scalars().all()
    
    # ç»Ÿè®¡
    severity_distribution = {}
    vulnerability_types = {}
    verified_count = 0
    
    for f in findings:
        # severity å’Œ vulnerability_type å·²ç»æ˜¯å­—ç¬¦ä¸²
        sev = str(f.severity)
        vtype = str(f.vulnerability_type)
        
        severity_distribution[sev] = severity_distribution.get(sev, 0) + 1
        vulnerability_types[vtype] = vulnerability_types.get(vtype, 0) + 1
        
        if f.is_verified:
            verified_count += 1
    
    # è®¡ç®—æŒç»­æ—¶é—´
    duration = None
    if task.started_at and task.completed_at:
        duration = int((task.completed_at - task.started_at).total_seconds())
    
    # è·å–å·²å®Œæˆçš„é˜¶æ®µ
    phases_result = await db.execute(
        select(AgentEvent.phase)
        .where(AgentEvent.task_id == task_id)
        .where(AgentEvent.event_type == AgentEventType.PHASE_COMPLETE)
        .distinct()
    )
    phases = [str(p[0]) for p in phases_result.fetchall() if p[0]]
    
    return TaskSummaryResponse(
        task_id=task_id,
        status=str(task.status),  # status å·²ç»æ˜¯å­—ç¬¦ä¸²
        security_score=task.security_score,
        total_findings=len(findings),
        verified_findings=verified_count,
        severity_distribution=severity_distribution,
        vulnerability_types=vulnerability_types,
        duration_seconds=duration,
        phases_completed=phases,
    )


@router.patch("/{task_id}/findings/{finding_id}/status")
async def update_finding_status(
    task_id: str,
    finding_id: str,
    status: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    æ›´æ–°å‘ç°çŠ¶æ€
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒæ“ä½œ")
    
    finding = await db.get(AgentFinding, finding_id)
    if not finding or finding.task_id != task_id:
        raise HTTPException(status_code=404, detail="å‘ç°ä¸å­˜åœ¨")
    
    try:
        finding.status = FindingStatus(status)
    except ValueError:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„çŠ¶æ€: {status}")
    
    await db.commit()
    
    return {"message": "çŠ¶æ€å·²æ›´æ–°", "finding_id": finding_id, "status": status}


# ============ Helper Functions ============

async def _get_project_root(
    project: Project,
    task_id: str,
    branch_name: Optional[str] = None,
    github_token: Optional[str] = None,
    gitlab_token: Optional[str] = None,
    event_emitter: Optional[Any] = None,  # ğŸ”¥ æ–°å¢ï¼šç”¨äºå‘é€å®æ—¶æ—¥å¿—
) -> str:
    """
    è·å–é¡¹ç›®æ ¹ç›®å½•

    æ”¯æŒä¸¤ç§é¡¹ç›®ç±»å‹ï¼š
    - ZIP é¡¹ç›®ï¼šè§£å‹ ZIP æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    - ä»“åº“é¡¹ç›®ï¼šå…‹éš†ä»“åº“åˆ°ä¸´æ—¶ç›®å½•

    Args:
        project: é¡¹ç›®å¯¹è±¡
        task_id: ä»»åŠ¡ID
        branch_name: åˆ†æ”¯åç§°ï¼ˆä»“åº“é¡¹ç›®ä½¿ç”¨ï¼Œä¼˜å…ˆäº project.default_branchï¼‰
        github_token: GitHub è®¿é—®ä»¤ç‰Œï¼ˆç”¨äºç§æœ‰ä»“åº“ï¼‰
        gitlab_token: GitLab è®¿é—®ä»¤ç‰Œï¼ˆç”¨äºç§æœ‰ä»“åº“ï¼‰
        event_emitter: äº‹ä»¶å‘é€å™¨ï¼ˆç”¨äºå‘é€å®æ—¶æ—¥å¿—ï¼‰

    Returns:
        é¡¹ç›®æ ¹ç›®å½•è·¯å¾„

    Raises:
        RuntimeError: å½“é¡¹ç›®æ–‡ä»¶è·å–å¤±è´¥æ—¶
    """
    import zipfile
    import subprocess
    import shutil
    from urllib.parse import urlparse, urlunparse

    # è¾…åŠ©å‡½æ•°ï¼šå‘é€äº‹ä»¶
    async def emit(message: str, level: str = "info"):
        if event_emitter:
            if level == "info":
                await event_emitter.emit_info(message)
            elif level == "warning":
                await event_emitter.emit_warning(message)
            elif level == "error":
                await event_emitter.emit_error(message)

    # ğŸ”¥ è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥å–æ¶ˆçŠ¶æ€
    def check_cancelled():
        if is_task_cancelled(task_id):
            raise asyncio.CancelledError("ä»»åŠ¡å·²å–æ¶ˆ")

    base_path = f"/tmp/deepaudit/{task_id}"

    # ç¡®ä¿ç›®å½•å­˜åœ¨ä¸”ä¸ºç©º
    if os.path.exists(base_path):
        shutil.rmtree(base_path)
    os.makedirs(base_path, exist_ok=True)

    # ğŸ”¥ åœ¨å¼€å§‹ä»»ä½•æ“ä½œå‰æ£€æŸ¥å–æ¶ˆ
    check_cancelled()

    # æ ¹æ®é¡¹ç›®ç±»å‹å¤„ç†
    if project.source_type == "zip":
        # ğŸ”¥ ZIP é¡¹ç›®ï¼šè§£å‹ ZIP æ–‡ä»¶
        check_cancelled()  # ğŸ”¥ è§£å‹å‰æ£€æŸ¥
        await emit(f"ğŸ“¦ æ­£åœ¨è§£å‹é¡¹ç›®æ–‡ä»¶...")
        from app.services.zip_storage import load_project_zip

        zip_path = await load_project_zip(project.id)

        if zip_path and os.path.exists(zip_path):
            try:
                check_cancelled()  # ğŸ”¥ è§£å‹å‰å†æ¬¡æ£€æŸ¥
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    # ğŸ”¥ é€ä¸ªæ–‡ä»¶è§£å‹ï¼Œæ”¯æŒå–æ¶ˆæ£€æŸ¥
                    file_list = zip_ref.namelist()
                    for i, file_name in enumerate(file_list):
                        if i % 50 == 0:  # æ¯50ä¸ªæ–‡ä»¶æ£€æŸ¥ä¸€æ¬¡
                            check_cancelled()
                        zip_ref.extract(file_name, base_path)
                logger.info(f"âœ… Extracted ZIP project {project.id} to {base_path}")
                await emit(f"âœ… ZIP æ–‡ä»¶è§£å‹å®Œæˆ")
            except Exception as e:
                logger.error(f"Failed to extract ZIP {zip_path}: {e}")
                await emit(f"âŒ è§£å‹å¤±è´¥: {e}", "error")
                raise RuntimeError(f"æ— æ³•è§£å‹é¡¹ç›®æ–‡ä»¶: {e}")
        else:
            logger.warning(f"âš ï¸ ZIP file not found for project {project.id}")
            await emit(f"âŒ ZIP æ–‡ä»¶ä¸å­˜åœ¨", "error")
            raise RuntimeError(f"é¡¹ç›® ZIP æ–‡ä»¶ä¸å­˜åœ¨: {project.id}")

    elif project.source_type == "repository" and project.repository_url:
        # ğŸ”¥ ä»“åº“é¡¹ç›®ï¼šä¼˜å…ˆä½¿ç”¨ ZIP ä¸‹è½½ï¼ˆæ›´å¿«æ›´ç¨³å®šï¼‰ï¼Œgit clone ä½œä¸ºå›é€€
        repo_url = project.repository_url
        repo_type = project.repository_type or "other"

        await emit(f"ğŸ”„ æ­£åœ¨è·å–ä»“åº“: {repo_url}")

        # è§£æä»“åº“ URL è·å– owner/repo
        parsed = urlparse(repo_url)
        path_parts = parsed.path.strip('/').replace('.git', '').split('/')
        if len(path_parts) >= 2:
            owner, repo = path_parts[0], path_parts[1]
        else:
            owner, repo = None, None

        # æ„å»ºåˆ†æ”¯å°è¯•é¡ºåº
        branches_to_try = []
        if branch_name:
            branches_to_try.append(branch_name)
        if project.default_branch and project.default_branch not in branches_to_try:
            branches_to_try.append(project.default_branch)
        for common_branch in ["main", "master"]:
            if common_branch not in branches_to_try:
                branches_to_try.append(common_branch)

        download_success = False
        last_error = ""

        # ============ æ–¹æ¡ˆ1: ä¼˜å…ˆä½¿ç”¨ ZIP ä¸‹è½½ï¼ˆæ›´å¿«æ›´ç¨³å®šï¼‰============
        if owner and repo:
            import httpx

            for branch in branches_to_try:
                check_cancelled()

                # æ¸…ç†ç›®å½•
                if os.path.exists(base_path) and os.listdir(base_path):
                    shutil.rmtree(base_path)
                os.makedirs(base_path, exist_ok=True)

                # æ„å»º ZIP ä¸‹è½½ URL
                if repo_type == "github" or "github.com" in repo_url:
                    # GitHub ZIP ä¸‹è½½ URL
                    zip_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/{branch}.zip"
                    headers = {}
                    if github_token:
                        headers["Authorization"] = f"token {github_token}"
                elif repo_type == "gitlab" or "gitlab" in repo_url:
                    # GitLab ZIP ä¸‹è½½ URLï¼ˆéœ€è¦å¯¹ owner/repo è¿›è¡Œ URL ç¼–ç ï¼‰
                    import urllib.parse
                    project_path = urllib.parse.quote(f"{owner}/{repo}", safe='')
                    gitlab_host = parsed.netloc
                    zip_url = f"https://{gitlab_host}/api/v4/projects/{project_path}/repository/archive.zip?sha={branch}"
                    headers = {}
                    if gitlab_token:
                        headers["PRIVATE-TOKEN"] = gitlab_token
                else:
                    # å…¶ä»–å¹³å°ï¼Œè·³è¿‡ ZIP ä¸‹è½½
                    break

                logger.info(f"ğŸ“¦ å°è¯•ä¸‹è½½ ZIP å½’æ¡£ (åˆ†æ”¯: {branch})...")
                await emit(f"ğŸ“¦ å°è¯•ä¸‹è½½ ZIP å½’æ¡£ (åˆ†æ”¯: {branch})")

                try:
                    zip_temp_path = f"/tmp/repo_{task_id}_{branch}.zip"

                    async def download_zip():
                        async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:
                            resp = await client.get(zip_url, headers=headers)
                            if resp.status_code == 200:
                                with open(zip_temp_path, 'wb') as f:
                                    f.write(resp.content)
                                return True, None
                            else:
                                return False, f"HTTP {resp.status_code}"

                    # ä½¿ç”¨å–æ¶ˆæ£€æŸ¥å¾ªç¯
                    download_task = asyncio.create_task(download_zip())
                    while not download_task.done():
                        check_cancelled()
                        try:
                            success, error = await asyncio.wait_for(asyncio.shield(download_task), timeout=1.0)
                            break
                        except asyncio.TimeoutError:
                            continue

                    if download_task.done():
                        success, error = download_task.result()

                    if success and os.path.exists(zip_temp_path):
                        # è§£å‹ ZIP
                        check_cancelled()
                        with zipfile.ZipFile(zip_temp_path, 'r') as zip_ref:
                            # ZIP å†…é€šå¸¸æœ‰ä¸€ä¸ªæ ¹ç›®å½•å¦‚ repo-branch/
                            file_list = zip_ref.namelist()
                            # æ‰¾åˆ°å…¬å…±å‰ç¼€
                            if file_list:
                                common_prefix = file_list[0].split('/')[0] + '/'
                                for i, file_name in enumerate(file_list):
                                    if i % 50 == 0:
                                        check_cancelled()
                                    # å»æ‰å…¬å…±å‰ç¼€
                                    if file_name.startswith(common_prefix):
                                        target_path = file_name[len(common_prefix):]
                                        if target_path:  # è·³è¿‡ç©ºè·¯å¾„ï¼ˆæ ¹ç›®å½•æœ¬èº«ï¼‰
                                            full_target = os.path.join(base_path, target_path)
                                            if file_name.endswith('/'):
                                                os.makedirs(full_target, exist_ok=True)
                                            else:
                                                os.makedirs(os.path.dirname(full_target), exist_ok=True)
                                                with zip_ref.open(file_name) as src, open(full_target, 'wb') as dst:
                                                    dst.write(src.read())

                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        os.remove(zip_temp_path)
                        logger.info(f"âœ… ZIP ä¸‹è½½æˆåŠŸ (åˆ†æ”¯: {branch})")
                        await emit(f"âœ… ä»“åº“è·å–æˆåŠŸ (ZIPä¸‹è½½, åˆ†æ”¯: {branch})")
                        download_success = True
                        break
                    else:
                        last_error = error or "ä¸‹è½½å¤±è´¥"
                        logger.warning(f"ZIP ä¸‹è½½å¤±è´¥ (åˆ†æ”¯ {branch}): {last_error}")
                        await emit(f"âš ï¸ ZIP ä¸‹è½½å¤±è´¥ï¼Œå°è¯•å…¶ä»–åˆ†æ”¯...", "warning")
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if os.path.exists(zip_temp_path):
                            os.remove(zip_temp_path)

                except asyncio.CancelledError:
                    logger.info(f"[Cancel] ZIP download cancelled for task {task_id}")
                    raise
                except Exception as e:
                    last_error = str(e)
                    logger.warning(f"ZIP ä¸‹è½½å¼‚å¸¸ (åˆ†æ”¯ {branch}): {e}")
                    await emit(f"âš ï¸ ZIP ä¸‹è½½å¼‚å¸¸: {str(e)[:50]}...", "warning")

        # ============ æ–¹æ¡ˆ2: å›é€€åˆ° git clone ============
        if not download_success:
            await emit(f"ğŸ”„ ZIP ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ° Git å…‹éš†...")
            logger.info("ZIP download failed, falling back to git clone")

            # æ£€æŸ¥ git æ˜¯å¦å¯ç”¨
            try:
                git_check = subprocess.run(
                    ["git", "--version"],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if git_check.returncode != 0:
                    await emit(f"âŒ Git æœªå®‰è£…", "error")
                    raise RuntimeError("Git æœªå®‰è£…ï¼Œæ— æ³•å…‹éš†ä»“åº“ã€‚")
            except FileNotFoundError:
                await emit(f"âŒ Git æœªå®‰è£…", "error")
                raise RuntimeError("Git æœªå®‰è£…ï¼Œæ— æ³•å…‹éš†ä»“åº“ã€‚")
            except subprocess.TimeoutExpired:
                await emit(f"âŒ Git æ£€æµ‹è¶…æ—¶", "error")
                raise RuntimeError("Git æ£€æµ‹è¶…æ—¶")

            # æ„å»ºå¸¦è®¤è¯çš„ URL
            auth_url = repo_url
            if repo_type == "github" and github_token:
                auth_url = urlunparse((
                    parsed.scheme,
                    f"{github_token}@{parsed.netloc}",
                    parsed.path,
                    parsed.params,
                    parsed.query,
                    parsed.fragment
                ))
                await emit(f"ğŸ” ä½¿ç”¨ GitHub Token è®¤è¯")
            elif repo_type == "gitlab" and gitlab_token:
                auth_url = urlunparse((
                    parsed.scheme,
                    f"oauth2:{gitlab_token}@{parsed.netloc}",
                    parsed.path,
                    parsed.params,
                    parsed.query,
                    parsed.fragment
                ))
                await emit(f"ğŸ” ä½¿ç”¨ GitLab Token è®¤è¯")

            for branch in branches_to_try:
                check_cancelled()

                if os.path.exists(base_path) and os.listdir(base_path):
                    shutil.rmtree(base_path)
                    os.makedirs(base_path, exist_ok=True)

                logger.info(f"ğŸ”„ å°è¯•å…‹éš†åˆ†æ”¯: {branch}")
                await emit(f"ğŸ”„ å°è¯•å…‹éš†åˆ†æ”¯: {branch}")

                try:
                    async def run_clone():
                        return await asyncio.to_thread(
                            subprocess.run,
                            ["git", "clone", "--depth", "1", "--branch", branch, auth_url, base_path],
                            capture_output=True,
                            text=True,
                            timeout=120,
                        )

                    clone_task = asyncio.create_task(run_clone())
                    while not clone_task.done():
                        check_cancelled()
                        try:
                            result = await asyncio.wait_for(asyncio.shield(clone_task), timeout=1.0)
                            break
                        except asyncio.TimeoutError:
                            continue

                    if clone_task.done():
                        result = clone_task.result()

                    if result.returncode == 0:
                        logger.info(f"âœ… Git å…‹éš†æˆåŠŸ (åˆ†æ”¯: {branch})")
                        await emit(f"âœ… ä»“åº“è·å–æˆåŠŸ (Gitå…‹éš†, åˆ†æ”¯: {branch})")
                        download_success = True
                        break
                    else:
                        last_error = result.stderr
                        logger.warning(f"å…‹éš†å¤±è´¥ (åˆ†æ”¯ {branch}): {last_error[:200]}")
                        await emit(f"âš ï¸ åˆ†æ”¯ {branch} å…‹éš†å¤±è´¥...", "warning")
                except subprocess.TimeoutExpired:
                    last_error = f"å…‹éš†åˆ†æ”¯ {branch} è¶…æ—¶"
                    logger.warning(last_error)
                    await emit(f"âš ï¸ åˆ†æ”¯ {branch} å…‹éš†è¶…æ—¶...", "warning")
                except asyncio.CancelledError:
                    logger.info(f"[Cancel] Git clone cancelled for task {task_id}")
                    raise

            # å°è¯•é»˜è®¤åˆ†æ”¯
            if not download_success:
                check_cancelled()
                await emit(f"ğŸ”„ å°è¯•ä½¿ç”¨ä»“åº“é»˜è®¤åˆ†æ”¯...")

                if os.path.exists(base_path) and os.listdir(base_path):
                    shutil.rmtree(base_path)
                    os.makedirs(base_path, exist_ok=True)

                try:
                    async def run_default_clone():
                        return await asyncio.to_thread(
                            subprocess.run,
                            ["git", "clone", "--depth", "1", auth_url, base_path],
                            capture_output=True,
                            text=True,
                            timeout=120,
                        )

                    clone_task = asyncio.create_task(run_default_clone())
                    while not clone_task.done():
                        check_cancelled()
                        try:
                            result = await asyncio.wait_for(asyncio.shield(clone_task), timeout=1.0)
                            break
                        except asyncio.TimeoutError:
                            continue

                    if clone_task.done():
                        result = clone_task.result()

                    if result.returncode == 0:
                        logger.info(f"âœ… Git å…‹éš†æˆåŠŸ (é»˜è®¤åˆ†æ”¯)")
                        await emit(f"âœ… ä»“åº“è·å–æˆåŠŸ (Gitå…‹éš†, é»˜è®¤åˆ†æ”¯)")
                        download_success = True
                    else:
                        last_error = result.stderr
                except subprocess.TimeoutExpired:
                    last_error = "å…‹éš†è¶…æ—¶"
                except asyncio.CancelledError:
                    logger.info(f"[Cancel] Git clone cancelled for task {task_id}")
                    raise

        if not download_success:
            # åˆ†æé”™è¯¯åŸå› 
            error_msg = "å…‹éš†ä»“åº“å¤±è´¥"
            if "Authentication failed" in last_error or "401" in last_error:
                error_msg = "è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ GitHub/GitLab Token é…ç½®"
            elif "not found" in last_error.lower() or "404" in last_error:
                error_msg = "ä»“åº“ä¸å­˜åœ¨æˆ–æ— è®¿é—®æƒé™"
            elif "Could not resolve host" in last_error:
                error_msg = "æ— æ³•è§£æä¸»æœºåï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            elif "Permission denied" in last_error or "403" in last_error:
                error_msg = "æ— è®¿é—®æƒé™ï¼Œè¯·æ£€æŸ¥ä»“åº“æƒé™æˆ– Token"
            else:
                error_msg = f"å…‹éš†ä»“åº“å¤±è´¥: {last_error[:200]}"

            logger.error(f"âŒ {error_msg}")
            await emit(f"âŒ {error_msg}", "error")
            raise RuntimeError(error_msg)

    # éªŒè¯ç›®å½•ä¸ä¸ºç©º
    if not os.listdir(base_path):
        await emit(f"âŒ é¡¹ç›®ç›®å½•ä¸ºç©º", "error")
        raise RuntimeError(f"é¡¹ç›®ç›®å½•ä¸ºç©ºï¼Œå¯èƒ½æ˜¯å…‹éš†/è§£å‹å¤±è´¥: {base_path}")

    await emit(f"ğŸ“ é¡¹ç›®å‡†å¤‡å®Œæˆ: {base_path}")
    return base_path


# ============ Agent Tree API ============

class AgentTreeNodeResponse(BaseModel):
    """Agent æ ‘èŠ‚ç‚¹å“åº”"""
    id: str
    agent_id: str
    agent_name: str
    agent_type: str
    parent_agent_id: Optional[str] = None
    depth: int = 0
    task_description: Optional[str] = None
    knowledge_modules: Optional[List[str]] = None
    status: str = "created"
    result_summary: Optional[str] = None
    findings_count: int = 0
    iterations: int = 0
    tokens_used: int = 0
    tool_calls: int = 0
    duration_ms: Optional[int] = None
    children: List["AgentTreeNodeResponse"] = []
    
    class Config:
        from_attributes = True


class AgentTreeResponse(BaseModel):
    """Agent æ ‘å“åº”"""
    task_id: str
    root_agent_id: Optional[str] = None
    total_agents: int = 0
    running_agents: int = 0
    completed_agents: int = 0
    failed_agents: int = 0
    total_findings: int = 0
    nodes: List[AgentTreeNodeResponse] = []


@router.get("/{task_id}/agent-tree", response_model=AgentTreeResponse)
async def get_agent_tree(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å–ä»»åŠ¡çš„ Agent æ ‘ç»“æ„
    
    è¿”å›åŠ¨æ€ Agent æ ‘çš„å®Œæ•´ç»“æ„ï¼ŒåŒ…æ‹¬ï¼š
    - æ‰€æœ‰ Agent èŠ‚ç‚¹
    - çˆ¶å­å…³ç³»
    - æ‰§è¡ŒçŠ¶æ€
    - å‘ç°ç»Ÿè®¡
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    # å°è¯•ä»å†…å­˜ä¸­è·å– Agent æ ‘ï¼ˆè¿è¡Œä¸­çš„ä»»åŠ¡ï¼‰
    runner = _running_tasks.get(task_id)
    logger.debug(f"[AgentTree API] task_id={task_id}, runner exists={runner is not None}")
    
    if runner:
        from app.services.agent.core import agent_registry
        
        tree = agent_registry.get_agent_tree()
        stats = agent_registry.get_statistics()
        logger.debug(f"[AgentTree API] tree nodes={len(tree.get('nodes', {}))}, root={tree.get('root_agent_id')}")
        logger.debug(f"[AgentTree API] èŠ‚ç‚¹è¯¦æƒ…: {list(tree.get('nodes', {}).keys())}")
        
        # ğŸ”¥ è·å– root agent IDï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦æ˜¯ Orchestrator
        root_agent_id = tree.get("root_agent_id")
        
        # æ„å»ºèŠ‚ç‚¹åˆ—è¡¨
        nodes = []
        for agent_id, node_data in tree.get("nodes", {}).items():
            # ğŸ”¥ ä» Agent å®ä¾‹è·å–å®æ—¶ç»Ÿè®¡æ•°æ®
            iterations = 0
            tool_calls = 0
            tokens_used = 0
            findings_count = 0
            
            agent_instance = agent_registry.get_agent(agent_id)
            if agent_instance and hasattr(agent_instance, 'get_stats'):
                agent_stats = agent_instance.get_stats()
                iterations = agent_stats.get("iterations", 0)
                tool_calls = agent_stats.get("tool_calls", 0)
                tokens_used = agent_stats.get("tokens_used", 0)
            
            # ğŸ”¥ FIX: å¯¹äº Orchestrator (root agent)ï¼Œä½¿ç”¨ task çš„ findings_count
            # è¿™ç¡®ä¿äº†æ­£ç¡®æ˜¾ç¤ºèšåˆçš„ findings æ€»æ•°
            if agent_id == root_agent_id:
                findings_count = task.findings_count or 0
            else:
                # ä»ç»“æœä¸­è·å–å‘ç°æ•°é‡ï¼ˆå¯¹äºå­ agentï¼‰
                if node_data.get("result"):
                    result = node_data.get("result", {})
                    findings_count = len(result.get("findings", []))
            
            nodes.append(AgentTreeNodeResponse(
                id=node_data.get("id", agent_id),
                agent_id=agent_id,
                agent_name=node_data.get("name", "Unknown"),
                agent_type=node_data.get("type", "unknown"),
                parent_agent_id=node_data.get("parent_id"),
                task_description=node_data.get("task"),
                knowledge_modules=node_data.get("knowledge_modules", []),
                status=node_data.get("status", "unknown"),
                findings_count=findings_count,
                iterations=iterations,
                tool_calls=tool_calls,
                tokens_used=tokens_used,
                children=[],
            ))
        
        # ğŸ”¥ ä½¿ç”¨ task.findings_count ä½œä¸º total_findingsï¼Œç¡®ä¿ä¸€è‡´æ€§
        return AgentTreeResponse(
            task_id=task_id,
            root_agent_id=root_agent_id,
            total_agents=stats.get("total", 0),
            running_agents=stats.get("running", 0),
            completed_agents=stats.get("completed", 0),
            failed_agents=stats.get("failed", 0),
            total_findings=task.findings_count or 0,
            nodes=nodes,
        )
    
    # ä»æ•°æ®åº“è·å–ï¼ˆå·²å®Œæˆçš„ä»»åŠ¡ï¼‰
    from app.models.agent_task import AgentTreeNode
    
    result = await db.execute(
        select(AgentTreeNode)
        .where(AgentTreeNode.task_id == task_id)
        .order_by(AgentTreeNode.depth, AgentTreeNode.created_at)
    )
    db_nodes = result.scalars().all()
    
    if not db_nodes:
        return AgentTreeResponse(
            task_id=task_id,
            nodes=[],
        )
    
    # æ„å»ºå“åº”
    nodes = []
    root_id = None
    running = 0
    completed = 0
    failed = 0
    
    for node in db_nodes:
        if node.parent_agent_id is None:
            root_id = node.agent_id
        
        if node.status == "running":
            running += 1
        elif node.status == "completed":
            completed += 1
        elif node.status == "failed":
            failed += 1
        
        # ğŸ”¥ FIX: å¯¹äº Orchestrator (root agent)ï¼Œä½¿ç”¨ task çš„ findings_count
        # è¿™ç¡®ä¿äº†æ­£ç¡®æ˜¾ç¤ºèšåˆçš„ findings æ€»æ•°
        if node.parent_agent_id is None:
            # Root agent uses task's total findings
            node_findings_count = task.findings_count or 0
        else:
            node_findings_count = node.findings_count or 0
        
        nodes.append(AgentTreeNodeResponse(
            id=node.id,
            agent_id=node.agent_id,
            agent_name=node.agent_name,
            agent_type=node.agent_type,
            parent_agent_id=node.parent_agent_id,
            depth=node.depth,
            task_description=node.task_description,
            knowledge_modules=node.knowledge_modules,
            status=node.status,
            result_summary=node.result_summary,
            findings_count=node_findings_count,
            iterations=node.iterations or 0,
            tokens_used=node.tokens_used or 0,
            tool_calls=node.tool_calls or 0,
            duration_ms=node.duration_ms,
            children=[],
        ))
    
    # ğŸ”¥ ä½¿ç”¨ task.findings_count ä½œä¸º total_findingsï¼Œç¡®ä¿ä¸€è‡´æ€§
    return AgentTreeResponse(
        task_id=task_id,
        root_agent_id=root_id,
        total_agents=len(nodes),
        running_agents=running,
        completed_agents=completed,
        failed_agents=failed,
        total_findings=task.findings_count or 0,
        nodes=nodes,
    )


# ============ Checkpoint API ============

class CheckpointResponse(BaseModel):
    """æ£€æŸ¥ç‚¹å“åº”"""
    id: str
    agent_id: str
    agent_name: str
    agent_type: str
    iteration: int
    status: str
    total_tokens: int = 0
    tool_calls: int = 0
    findings_count: int = 0
    checkpoint_type: str = "auto"
    checkpoint_name: Optional[str] = None
    created_at: Optional[str] = None
    
    class Config:
        from_attributes = True


@router.get("/{task_id}/checkpoints", response_model=List[CheckpointResponse])
async def list_checkpoints(
    task_id: str,
    agent_id: Optional[str] = None,
    limit: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å–ä»»åŠ¡çš„æ£€æŸ¥ç‚¹åˆ—è¡¨
    
    ç”¨äºï¼š
    - æŸ¥çœ‹æ‰§è¡Œå†å²
    - çŠ¶æ€æ¢å¤
    - è°ƒè¯•åˆ†æ
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    from app.models.agent_task import AgentCheckpoint
    
    query = select(AgentCheckpoint).where(AgentCheckpoint.task_id == task_id)
    
    if agent_id:
        query = query.where(AgentCheckpoint.agent_id == agent_id)
    
    query = query.order_by(AgentCheckpoint.created_at.desc()).limit(limit)
    
    result = await db.execute(query)
    checkpoints = result.scalars().all()
    
    return [
        CheckpointResponse(
            id=cp.id,
            agent_id=cp.agent_id,
            agent_name=cp.agent_name,
            agent_type=cp.agent_type,
            iteration=cp.iteration,
            status=cp.status,
            total_tokens=cp.total_tokens or 0,
            tool_calls=cp.tool_calls or 0,
            findings_count=cp.findings_count or 0,
            checkpoint_type=cp.checkpoint_type or "auto",
            checkpoint_name=cp.checkpoint_name,
            created_at=cp.created_at.isoformat() if cp.created_at else None,
        )
        for cp in checkpoints
    ]


@router.get("/{task_id}/checkpoints/{checkpoint_id}")
async def get_checkpoint_detail(
    task_id: str,
    checkpoint_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    è·å–æ£€æŸ¥ç‚¹è¯¦æƒ…
    
    è¿”å›å®Œæ•´çš„ Agent çŠ¶æ€æ•°æ®
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    from app.models.agent_task import AgentCheckpoint
    
    checkpoint = await db.get(AgentCheckpoint, checkpoint_id)
    if not checkpoint or checkpoint.task_id != task_id:
        raise HTTPException(status_code=404, detail="æ£€æŸ¥ç‚¹ä¸å­˜åœ¨")
    
    # è§£æçŠ¶æ€æ•°æ®
    state_data = {}
    if checkpoint.state_data:
        try:
            state_data = json.loads(checkpoint.state_data)
        except json.JSONDecodeError:
            pass
    
    return {
        "id": checkpoint.id,
        "task_id": checkpoint.task_id,
        "agent_id": checkpoint.agent_id,
        "agent_name": checkpoint.agent_name,
        "agent_type": checkpoint.agent_type,
        "parent_agent_id": checkpoint.parent_agent_id,
        "iteration": checkpoint.iteration,
        "status": checkpoint.status,
        "total_tokens": checkpoint.total_tokens,
        "tool_calls": checkpoint.tool_calls,
        "findings_count": checkpoint.findings_count,
        "checkpoint_type": checkpoint.checkpoint_type,
        "checkpoint_name": checkpoint.checkpoint_name,
        "state_data": state_data,
        "metadata": checkpoint.checkpoint_metadata,
        "created_at": checkpoint.created_at.isoformat() if checkpoint.created_at else None,
    }


# ============ Report Generation API ============

@router.get("/{task_id}/report")
async def generate_audit_report(
    task_id: str,
    format: str = Query("markdown", regex="^(markdown|json)$"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
):
    """
    ç”Ÿæˆå®¡è®¡æŠ¥å‘Š
    
    æ”¯æŒ Markdown å’Œ JSON æ ¼å¼
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="æ— æƒè®¿é—®æ­¤ä»»åŠ¡")
    
    # è·å–æ­¤ä»»åŠ¡çš„æ‰€æœ‰å‘ç°
    findings = await db.execute(
        select(AgentFinding)
        .where(AgentFinding.task_id == task_id)
        .order_by(
            case(
                (AgentFinding.severity == 'critical', 1),
                (AgentFinding.severity == 'high', 2),
                (AgentFinding.severity == 'medium', 3),
                (AgentFinding.severity == 'low', 4),
                else_=5
            ),
            AgentFinding.created_at.desc()
        )
    )
    findings = findings.scalars().all()
    
    # ğŸ”¥ Helper function to normalize severity for comparison (case-insensitive)
    def normalize_severity(sev: str) -> str:
        return str(sev).lower().strip() if sev else ""
    
    # Log findings for debugging
    logger.info(f"[Report] Task {task_id}: Found {len(findings)} findings from database")
    if findings:
        for i, f in enumerate(findings[:3]):  # Log first 3
            logger.debug(f"[Report] Finding {i+1}: severity='{f.severity}', title='{f.title[:50] if f.title else 'N/A'}'")
    
    if format == "json":
        # Enhanced JSON report with full metadata
        return {
            "report_metadata": {
                "task_id": task.id,
                "project_id": task.project_id,
                "project_name": project.name,
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "task_status": task.status,
                "duration_seconds": int((task.completed_at - task.started_at).total_seconds()) if task.completed_at and task.started_at else None,
            },
            "summary": {
                "security_score": task.security_score,
                "total_files_analyzed": task.analyzed_files,
                "total_findings": len(findings),
                "verified_findings": sum(1 for f in findings if f.is_verified),
                "severity_distribution": {
                    "critical": sum(1 for f in findings if normalize_severity(f.severity) == 'critical'),
                    "high": sum(1 for f in findings if normalize_severity(f.severity) == 'high'),
                    "medium": sum(1 for f in findings if normalize_severity(f.severity) == 'medium'),
                    "low": sum(1 for f in findings if normalize_severity(f.severity) == 'low'),
                },
                "agent_metrics": {
                    "total_iterations": task.total_iterations,
                    "tool_calls": task.tool_calls_count,
                    "tokens_used": task.tokens_used,
                }
            },
            "findings": [
                {
                    "id": f.id,
                    "title": f.title,
                    "severity": f.severity,
                    "vulnerability_type": f.vulnerability_type,
                    "description": f.description,
                    "file_path": f.file_path,
                    "line_start": f.line_start,
                    "line_end": f.line_end,
                    "code_snippet": f.code_snippet,
                    "is_verified": f.is_verified,
                    "has_poc": f.has_poc,
                    "poc_code": f.poc_code,
                    "poc_description": f.poc_description,
                    "poc_steps": f.poc_steps,
                    "confidence": f.ai_confidence,
                    "suggestion": f.suggestion,
                    "fix_code": f.fix_code,
                    "created_at": f.created_at.isoformat() if f.created_at else None,
                } for f in findings
            ]
        }

    # Generate Enhanced Markdown Report
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Calculate statistics
    total = len(findings)
    critical = sum(1 for f in findings if normalize_severity(f.severity) == 'critical')
    high = sum(1 for f in findings if normalize_severity(f.severity) == 'high')
    medium = sum(1 for f in findings if normalize_severity(f.severity) == 'medium')
    low = sum(1 for f in findings if normalize_severity(f.severity) == 'low')
    verified = sum(1 for f in findings if f.is_verified)
    with_poc = sum(1 for f in findings if f.has_poc)

    # Calculate duration
    duration_str = "N/A"
    if task.completed_at and task.started_at:
        duration = (task.completed_at - task.started_at).total_seconds()
        if duration >= 3600:
            duration_str = f"{duration / 3600:.1f} å°æ—¶"
        elif duration >= 60:
            duration_str = f"{duration / 60:.1f} åˆ†é’Ÿ"
        else:
            duration_str = f"{int(duration)} ç§’"

    md_lines = []

    # Header
    md_lines.append("# DeepAudit å®‰å…¨å®¡è®¡æŠ¥å‘Š")
    md_lines.append("")
    md_lines.append("---")
    md_lines.append("")

    # Report Info
    md_lines.append("## æŠ¥å‘Šä¿¡æ¯")
    md_lines.append("")
    md_lines.append(f"| å±æ€§ | å†…å®¹ |")
    md_lines.append(f"|----------|-------|")
    md_lines.append(f"| **é¡¹ç›®åç§°** | {project.name} |")
    md_lines.append(f"| **ä»»åŠ¡ ID** | `{task.id[:8]}...` |")
    md_lines.append(f"| **ç”Ÿæˆæ—¶é—´** | {timestamp} |")
    md_lines.append(f"| **ä»»åŠ¡çŠ¶æ€** | {task.status.upper()} |")
    md_lines.append(f"| **è€—æ—¶** | {duration_str} |")
    md_lines.append("")

    # Executive Summary
    md_lines.append("## æ‰§è¡Œæ‘˜è¦")
    md_lines.append("")

    score = task.security_score
    if score is not None:
        if score >= 80:
            score_assessment = "è‰¯å¥½ - å»ºè®®è¿›è¡Œå°‘é‡ä¼˜åŒ–"
            score_icon = "é€šè¿‡"
        elif score >= 60:
            score_assessment = "ä¸­ç­‰ - å­˜åœ¨è‹¥å¹²é—®é¢˜éœ€è¦å…³æ³¨"
            score_icon = "è­¦å‘Š"
        else:
            score_assessment = "ä¸¥é‡ - éœ€è¦ç«‹å³è¿›è¡Œä¿®å¤"
            score_icon = "æœªé€šè¿‡"
        md_lines.append(f"**å®‰å…¨è¯„åˆ†: {int(score)}/100** [{score_icon}]")
        md_lines.append(f"*{score_assessment}*")
    else:
        md_lines.append("**å®‰å…¨è¯„åˆ†:** æœªè®¡ç®—")
    md_lines.append("")

    # Findings Summary
    md_lines.append("### æ¼æ´å‘ç°æ¦‚è§ˆ")
    md_lines.append("")
    md_lines.append(f"| ä¸¥é‡ç¨‹åº¦ | æ•°é‡ | å·²éªŒè¯ |")
    md_lines.append(f"|----------|-------|----------|")
    if critical > 0:
        md_lines.append(f"| **ä¸¥é‡ (CRITICAL)** | {critical} | {sum(1 for f in findings if normalize_severity(f.severity) == 'critical' and f.is_verified)} |")
    if high > 0:
        md_lines.append(f"| **é«˜å± (HIGH)** | {high} | {sum(1 for f in findings if normalize_severity(f.severity) == 'high' and f.is_verified)} |")
    if medium > 0:
        md_lines.append(f"| **ä¸­å± (MEDIUM)** | {medium} | {sum(1 for f in findings if normalize_severity(f.severity) == 'medium' and f.is_verified)} |")
    if low > 0:
        md_lines.append(f"| **ä½å± (LOW)** | {low} | {sum(1 for f in findings if normalize_severity(f.severity) == 'low' and f.is_verified)} |")
    md_lines.append(f"| **æ€»è®¡** | {total} | {verified} |")
    md_lines.append("")

    # Audit Metrics
    md_lines.append("### å®¡è®¡æŒ‡æ ‡")
    md_lines.append("")
    md_lines.append(f"- **åˆ†ææ–‡ä»¶æ•°:** {task.analyzed_files} / {task.total_files}")
    md_lines.append(f"- **Agent è¿­ä»£æ¬¡æ•°:** {task.total_iterations}")
    md_lines.append(f"- **å·¥å…·è°ƒç”¨æ¬¡æ•°:** {task.tool_calls_count}")
    md_lines.append(f"- **Token æ¶ˆè€—:** {task.tokens_used:,}")
    if with_poc > 0:
        md_lines.append(f"- **ç”Ÿæˆçš„ PoC:** {with_poc}")
    md_lines.append("")

    # Detailed Findings
    if not findings:
        md_lines.append("## æ¼æ´è¯¦æƒ…")
        md_lines.append("")
        md_lines.append("*æœ¬æ¬¡å®¡è®¡æœªå‘ç°å®‰å…¨æ¼æ´ã€‚*")
        md_lines.append("")
    else:
        # Group findings by severity
        severity_map = {
            'critical': 'ä¸¥é‡ (Critical)',
            'high': 'é«˜å± (High)',
            'medium': 'ä¸­å± (Medium)',
            'low': 'ä½å± (Low)'
        }
        
        for severity_level, severity_name in severity_map.items():
            severity_findings = [f for f in findings if normalize_severity(f.severity) == severity_level]
            if not severity_findings:
                continue

            md_lines.append(f"## {severity_name} æ¼æ´")
            md_lines.append("")

            for i, f in enumerate(severity_findings, 1):
                verified_badge = "[å·²éªŒè¯]" if f.is_verified else "[æœªéªŒè¯]"
                poc_badge = " [å« PoC]" if f.has_poc else ""

                md_lines.append(f"### {severity_level.upper()}-{i}: {f.title}")
                md_lines.append("")
                md_lines.append(f"**{verified_badge}**{poc_badge} | ç±»å‹: `{f.vulnerability_type}`")
                md_lines.append("")

                if f.file_path:
                    location = f"`{f.file_path}"
                    if f.line_start:
                        location += f":{f.line_start}"
                        if f.line_end and f.line_end != f.line_start:
                            location += f"-{f.line_end}"
                    location += "`"
                    md_lines.append(f"**ä½ç½®:** {location}")
                    md_lines.append("")

                if f.ai_confidence:
                    md_lines.append(f"**AI ç½®ä¿¡åº¦:** {int(f.ai_confidence * 100)}%")
                    md_lines.append("")

                if f.description:
                    md_lines.append("**æ¼æ´æè¿°:**")
                    md_lines.append("")
                    md_lines.append(f.description)
                    md_lines.append("")

                if f.code_snippet:
                    # Detect language from file extension
                    lang = "python"
                    if f.file_path:
                        ext = f.file_path.split('.')[-1].lower()
                        lang_map = {
                            'py': 'python', 'js': 'javascript', 'ts': 'typescript',
                            'jsx': 'jsx', 'tsx': 'tsx', 'java': 'java', 'go': 'go',
                            'rs': 'rust', 'rb': 'ruby', 'php': 'php', 'c': 'c',
                            'cpp': 'cpp', 'cs': 'csharp', 'sol': 'solidity'
                        }
                        lang = lang_map.get(ext, 'text')
                    md_lines.append("**æ¼æ´ä»£ç :**")
                    md_lines.append("")
                    md_lines.append(f"```{lang}")
                    md_lines.append(f.code_snippet.strip())
                    md_lines.append("```")
                    md_lines.append("")

                if f.suggestion:
                    md_lines.append("**ä¿®å¤å»ºè®®:**")
                    md_lines.append("")
                    md_lines.append(f.suggestion)
                    md_lines.append("")

                if f.fix_code:
                    md_lines.append("**å‚è€ƒä¿®å¤ä»£ç :**")
                    md_lines.append("")
                    md_lines.append(f"```{lang if f.file_path else 'text'}")
                    md_lines.append(f.fix_code.strip())
                    md_lines.append("```")
                    md_lines.append("")

                # ğŸ”¥ æ·»åŠ  PoC è¯¦æƒ…
                if f.has_poc:
                    md_lines.append("**æ¦‚å¿µéªŒè¯ (PoC):**")
                    md_lines.append("")

                    if f.poc_description:
                        md_lines.append(f"*{f.poc_description}*")
                        md_lines.append("")

                    if f.poc_steps:
                        md_lines.append("**å¤ç°æ­¥éª¤:**")
                        md_lines.append("")
                        for step_idx, step in enumerate(f.poc_steps, 1):
                            md_lines.append(f"{step_idx}. {step}")
                        md_lines.append("")

                    if f.poc_code:
                        md_lines.append("**PoC ä»£ç :**")
                        md_lines.append("")
                        md_lines.append("```")
                        md_lines.append(f.poc_code.strip())
                        md_lines.append("```")
                        md_lines.append("")

                md_lines.append("---")
                md_lines.append("")

    # Remediation Priority
    if critical > 0 or high > 0:
        md_lines.append("## ä¿®å¤ä¼˜å…ˆçº§å»ºè®®")
        md_lines.append("")
        md_lines.append("åŸºäºå·²å‘ç°çš„æ¼æ´ï¼Œæˆ‘ä»¬å»ºè®®æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§è¿›è¡Œä¿®å¤ï¼š")
        md_lines.append("")
        priority_idx = 1
        if critical > 0:
            md_lines.append(f"{priority_idx}. **ç«‹å³ä¿®å¤:** å¤„ç† {critical} ä¸ªä¸¥é‡æ¼æ´ - å¯èƒ½é€ æˆä¸¥é‡å½±å“")
            priority_idx += 1
        if high > 0:
            md_lines.append(f"{priority_idx}. **é«˜ä¼˜å…ˆçº§:** åœ¨ 1 å‘¨å†…ä¿®å¤ {high} ä¸ªé«˜å±æ¼æ´")
            priority_idx += 1
        if medium > 0:
            md_lines.append(f"{priority_idx}. **ä¸­ä¼˜å…ˆçº§:** åœ¨ 2-4 å‘¨å†…ä¿®å¤ {medium} ä¸ªä¸­å±æ¼æ´")
            priority_idx += 1
        if low > 0:
            md_lines.append(f"{priority_idx}. **ä½ä¼˜å…ˆçº§:** åœ¨æ—¥å¸¸ç»´æŠ¤ä¸­å¤„ç† {low} ä¸ªä½å±æ¼æ´")
            priority_idx += 1
        md_lines.append("")

    # Footer
    md_lines.append("---")
    md_lines.append("")
    md_lines.append("*æœ¬æŠ¥å‘Šç”± DeepAudit - AI é©±åŠ¨çš„å®‰å…¨åˆ†æç³»ç»Ÿç”Ÿæˆ*")
    md_lines.append("")
    content = "\n".join(md_lines)
    
    filename = f"audit_report_{task.id[:8]}_{datetime.now().strftime('%Y%m%d')}.md"
    
    from fastapi.responses import Response
    return Response(
        content=content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f"attachment; filename={filename}"
        }
    )
