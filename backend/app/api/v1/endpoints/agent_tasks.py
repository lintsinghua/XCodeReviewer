"""
DeepAudit Agent ÂÆ°ËÆ°‰ªªÂä° API
Âü∫‰∫é LangGraph ÁöÑ Agent ÂÆ°ËÆ°
"""

import asyncio
import json
import logging
import os
import shutil
from typing import Any, List, Optional, Dict, Set
from datetime import datetime, timezone
from uuid import uuid4

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import case
from sqlalchemy.future import select
from sqlalchemy.orm import selectinload
from pydantic import BaseModel, Field

from app.api import deps
from app.db.session import get_db, async_session_factory
from app.models.agent_task import (
    AgentTask, AgentEvent, AgentFinding,
    AgentTaskStatus, AgentTaskPhase, AgentEventType,
    VulnerabilitySeverity, FindingStatus,
)
from app.models.project import Project
from app.models.user import User
from app.models.user_config import UserConfig
from app.services.agent.event_manager import EventManager
from app.services.agent.streaming import StreamHandler, StreamEvent, StreamEventType

logger = logging.getLogger(__name__)
router = APIRouter()

# ËøêË°å‰∏≠ÁöÑ‰ªªÂä°ÔºàÂÖºÂÆπÊóßÊé•Âè£Ôºâ
_running_tasks: Dict[str, Any] = {}

# üî• ËøêË°å‰∏≠ÁöÑ asyncio TasksÔºàÁî®‰∫éÂº∫Âà∂ÂèñÊ∂àÔºâ
_running_asyncio_tasks: Dict[str, asyncio.Task] = {}


# ============ Schemas ============

class AgentTaskCreate(BaseModel):
    """ÂàõÂª∫ Agent ‰ªªÂä°ËØ∑Ê±Ç"""
    project_id: str = Field(..., description="È°πÁõÆ ID")
    name: Optional[str] = Field(None, description="‰ªªÂä°ÂêçÁß∞")
    description: Optional[str] = Field(None, description="‰ªªÂä°ÊèèËø∞")
    
    # ÂÆ°ËÆ°ÈÖçÁΩÆ
    audit_scope: Optional[dict] = Field(None, description="ÂÆ°ËÆ°ËåÉÂõ¥")
    target_vulnerabilities: Optional[List[str]] = Field(
        default=["sql_injection", "xss", "command_injection", "path_traversal", "ssrf"],
        description="ÁõÆÊ†áÊºèÊ¥ûÁ±ªÂûã"
    )
    verification_level: str = Field(
        "sandbox", 
        description="È™åËØÅÁ∫ßÂà´: analysis_only, sandbox, generate_poc"
    )
    
    # ÂàÜÊîØ
    branch_name: Optional[str] = Field(None, description="ÂàÜÊîØÂêçÁß∞")
    
    # ÊéíÈô§Ê®°Âºè
    exclude_patterns: Optional[List[str]] = Field(
        default=["node_modules", "__pycache__", ".git", "*.min.js"],
        description="ÊéíÈô§Ê®°Âºè"
    )
    
    # Êñá‰ª∂ËåÉÂõ¥
    target_files: Optional[List[str]] = Field(None, description="ÊåáÂÆöÊâ´ÊèèÁöÑÊñá‰ª∂")
    
    # Agent ÈÖçÁΩÆ
    max_iterations: int = Field(50, ge=1, le=200, description="ÊúÄÂ§ßËø≠‰ª£Ê¨°Êï∞")
    timeout_seconds: int = Field(1800, ge=60, le=7200, description="Ë∂ÖÊó∂Êó∂Èó¥ÔºàÁßíÔºâ")


class AgentTaskResponse(BaseModel):
    """Agent ‰ªªÂä°ÂìçÂ∫î - ÂåÖÂê´ÊâÄÊúâÂâçÁ´ØÈúÄË¶ÅÁöÑÂ≠óÊÆµ"""
    id: str
    project_id: str
    name: Optional[str]
    description: Optional[str]
    task_type: str = "agent_audit"
    status: str
    current_phase: Optional[str]
    current_step: Optional[str] = None
    
    # ËøõÂ∫¶ÁªüËÆ°
    total_files: int = 0
    indexed_files: int = 0
    analyzed_files: int = 0
    total_chunks: int = 0
    
    # Agent ÁªüËÆ°
    total_iterations: int = 0
    tool_calls_count: int = 0
    tokens_used: int = 0
    
    # ÂèëÁé∞ÁªüËÆ°ÔºàÂÖºÂÆπ‰∏§ÁßçÂëΩÂêçÔºâ
    findings_count: int = 0
    total_findings: int = 0  # ÂÖºÂÆπÂ≠óÊÆµ
    verified_count: int = 0
    verified_findings: int = 0  # ÂÖºÂÆπÂ≠óÊÆµ
    false_positive_count: int = 0
    
    # ‰∏•ÈáçÁ®ãÂ∫¶ÁªüËÆ°
    critical_count: int = 0
    high_count: int = 0
    medium_count: int = 0
    low_count: int = 0
    
    # ËØÑÂàÜ
    quality_score: float = 0.0
    security_score: Optional[float] = None
    
    # ËøõÂ∫¶ÁôæÂàÜÊØî
    progress_percentage: float = 0.0
    
    # Êó∂Èó¥
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    
    # ÈÖçÁΩÆ
    audit_scope: Optional[dict] = None
    target_vulnerabilities: Optional[List[str]] = None
    verification_level: Optional[str] = None
    exclude_patterns: Optional[List[str]] = None
    target_files: Optional[List[str]] = None
    
    # ÈîôËØØ‰ø°ÊÅØ
    error_message: Optional[str] = None
    
    class Config:
        from_attributes = True


class AgentEventResponse(BaseModel):
    """Agent ‰∫ã‰ª∂ÂìçÂ∫î"""
    id: str
    task_id: str
    event_type: str
    phase: Optional[str]
    message: Optional[str] = None
    sequence: int
    # üî• ORM Â≠óÊÆµÂêçÊòØ created_atÔºåÂ∫èÂàóÂåñ‰∏∫ timestamp
    created_at: datetime = Field(serialization_alias="timestamp")

    # Â∑•ÂÖ∑Áõ∏ÂÖ≥Â≠óÊÆµ
    tool_name: Optional[str] = None
    tool_input: Optional[Dict[str, Any]] = None
    tool_output: Optional[Dict[str, Any]] = None
    tool_duration_ms: Optional[int] = None

    # ÂÖ∂‰ªñÂ≠óÊÆµ
    progress_percent: Optional[float] = None
    finding_id: Optional[str] = None
    tokens_used: Optional[int] = None
    # üî• ORM Â≠óÊÆµÂêçÊòØ event_metadataÔºåÂ∫èÂàóÂåñ‰∏∫ metadata
    event_metadata: Optional[Dict[str, Any]] = Field(default=None, serialization_alias="metadata")

    model_config = {
        "from_attributes": True,
        "populate_by_name": True,
        "by_alias": True,  # üî• ÂÖ≥ÈîÆÔºöÁ°Æ‰øùÂ∫èÂàóÂåñÊó∂‰ΩøÁî®Âà´Âêç
    }


class AgentFindingResponse(BaseModel):
    """Agent ÂèëÁé∞ÂìçÂ∫î"""
    id: str
    task_id: str
    vulnerability_type: str
    severity: str
    title: str
    description: Optional[str]
    file_path: Optional[str]
    line_start: Optional[int]
    line_end: Optional[int]
    code_snippet: Optional[str]
    
    is_verified: bool
    # üî• FIX: Map from ai_confidence in ORM, make Optional with default
    confidence: Optional[float] = Field(default=0.5, validation_alias="ai_confidence")
    status: str
    
    suggestion: Optional[str] = None
    poc: Optional[dict] = None
    
    created_at: datetime
    
    model_config = {
        "from_attributes": True,
        "populate_by_name": True,  # Allow both 'confidence' and 'ai_confidence'
    }


class TaskSummaryResponse(BaseModel):
    """‰ªªÂä°ÊëòË¶ÅÂìçÂ∫î"""
    task_id: str
    status: str
    security_score: Optional[int]
    
    total_findings: int
    verified_findings: int
    
    severity_distribution: Dict[str, int]
    vulnerability_types: Dict[str, int]
    
    duration_seconds: Optional[int]
    phases_completed: List[str]


# ============ ÂêéÂè∞‰ªªÂä°ÊâßË°å ============

# ËøêË°å‰∏≠ÁöÑÂä®ÊÄÅÊâßË°åÂô®
_running_orchestrators: Dict[str, Any] = {}
# ËøêË°å‰∏≠ÁöÑ‰∫ã‰ª∂ÁÆ°ÁêÜÂô®ÔºàÁî®‰∫é SSE ÊµÅÔºâ
_running_event_managers: Dict[str, EventManager] = {}
# üî• Â∑≤ÂèñÊ∂àÁöÑ‰ªªÂä°ÈõÜÂêàÔºàÁî®‰∫éÂâçÁΩÆÊìç‰ΩúÁöÑÂèñÊ∂àÊ£ÄÊü•Ôºâ
_cancelled_tasks: Set[str] = set()


def is_task_cancelled(task_id: str) -> bool:
    """Ê£ÄÊü•‰ªªÂä°ÊòØÂê¶Â∑≤Ë¢´ÂèñÊ∂à"""
    return task_id in _cancelled_tasks


async def _execute_agent_task(task_id: str):
    """
    Âú®ÂêéÂè∞ÊâßË°å Agent ‰ªªÂä° - ‰ΩøÁî®Âä®ÊÄÅ Agent Ê†ëÊû∂ÊûÑ
    
    Êû∂ÊûÑÔºöOrchestratorAgent ‰Ωú‰∏∫Â§ßËÑëÔºåÂä®ÊÄÅË∞ÉÂ∫¶Â≠ê Agent
    """
    from app.services.agent.agents import OrchestratorAgent, ReconAgent, AnalysisAgent, VerificationAgent
    from app.services.agent.event_manager import EventManager, AgentEventEmitter
    from app.services.llm.service import LLMService
    from app.services.agent.core import agent_registry
    from app.services.agent.tools import SandboxManager
    from app.core.config import settings
    import time
    
    # üî• Âú®‰ªªÂä°ÊúÄÂºÄÂßãÂ∞±ÂàùÂßãÂåñ Docker Ê≤ôÁÆ±ÁÆ°ÁêÜÂô®
    # ËøôÊ†∑ÂèØ‰ª•Á°Æ‰øùÊï¥‰∏™‰ªªÂä°ÁîüÂëΩÂë®ÊúüÂÜÖ‰ΩøÁî®Âêå‰∏Ä‰∏™ÁÆ°ÁêÜÂô®ÔºåÂπ∂‰∏îÂ∞ΩÊó©ÂèëÁé∞ Docker ÈóÆÈ¢ò
    logger.info(f"üöÄ Starting execution for task {task_id}")
    sandbox_manager = SandboxManager()
    await sandbox_manager.initialize()
    logger.info(f"üê≥ Global Sandbox Manager initialized (Available: {sandbox_manager.is_available})")

    # üî• ÊèêÂâçÂàõÂª∫‰∫ã‰ª∂ÁÆ°ÁêÜÂô®Ôºå‰ª•‰æøÂú®ÂÖãÈöÜ‰ªìÂ∫ìÂíåÁ¥¢ÂºïÊó∂ÂèëÈÄÅÂÆûÊó∂Êó•Âøó
    from app.services.agent.event_manager import EventManager, AgentEventEmitter
    event_manager = EventManager(db_session_factory=async_session_factory)
    event_manager.create_queue(task_id)
    event_emitter = AgentEventEmitter(task_id, event_manager)
    _running_event_managers[task_id] = event_manager

    async with async_session_factory() as db:
        orchestrator = None
        start_time = time.time()

        try:
            # Ëé∑Âèñ‰ªªÂä°
            task = await db.get(AgentTask, task_id, options=[selectinload(AgentTask.project)])
            if not task:
                logger.error(f"Task {task_id} not found")
                return

            # Ëé∑ÂèñÈ°πÁõÆ
            project = task.project
            if not project:
                logger.error(f"Project not found for task {task_id}")
                return

            # üî• ÂèëÈÄÅ‰ªªÂä°ÂºÄÂßã‰∫ã‰ª∂ - ‰ΩøÁî® phase_start ËÆ©ÂâçÁ´ØÁü•ÈÅìËøõÂÖ•ÂáÜÂ§áÈò∂ÊÆµ
            await event_emitter.emit_phase_start("preparation", f"üöÄ ‰ªªÂä°ÂºÄÂßãÊâßË°å: {project.name}")

            # Êõ¥Êñ∞‰ªªÂä°Èò∂ÊÆµ‰∏∫ÂáÜÂ§á‰∏≠
            task.status = AgentTaskStatus.RUNNING
            task.started_at = datetime.now(timezone.utc)
            task.current_phase = AgentTaskPhase.PLANNING  # preparation ÂØπÂ∫î PLANNING
            await db.commit()

            # Ëé∑ÂèñÁî®Êà∑ÈÖçÁΩÆÔºàÈúÄË¶ÅÂú®Ëé∑ÂèñÈ°πÁõÆÊ†πÁõÆÂΩï‰πãÂâçÔºå‰ª•‰æø‰º†ÈÄí tokenÔºâ
            user_config = await _get_user_config(db, task.created_by)

            # ‰ªéÁî®Êà∑ÈÖçÁΩÆ‰∏≠ÊèêÂèñ tokenÔºàÁî®‰∫éÁßÅÊúâ‰ªìÂ∫ìÂÖãÈöÜÔºâ
            other_config = (user_config or {}).get('otherConfig', {})
            github_token = other_config.get('githubToken') or settings.GITHUB_TOKEN
            gitlab_token = other_config.get('gitlabToken') or settings.GITLAB_TOKEN

            # Ëé∑ÂèñÈ°πÁõÆÊ†πÁõÆÂΩïÔºà‰º†ÈÄí‰ªªÂä°ÊåáÂÆöÁöÑÂàÜÊîØÂíåËÆ§ËØÅ tokenÔºâ
            # üî• ‰º†ÈÄí event_emitter ‰ª•ÂèëÈÄÅÂÖãÈöÜËøõÂ∫¶
            project_root = await _get_project_root(
                project,
                task_id,
                task.branch_name,
                github_token=github_token,
                gitlab_token=gitlab_token,
                event_emitter=event_emitter,  # üî• Êñ∞Â¢û
            )

            logger.info(f"üöÄ Task {task_id} started with Dynamic Agent Tree architecture")

            # üî• Ëé∑ÂèñÈ°πÁõÆÊ†πÁõÆÂΩïÂêéÊ£ÄÊü•ÂèñÊ∂à
            if is_task_cancelled(task_id):
                logger.info(f"[Cancel] Task {task_id} cancelled after project preparation")
                raise asyncio.CancelledError("‰ªªÂä°Â∑≤ÂèñÊ∂à")

            # ÂàõÂª∫ LLM ÊúçÂä°
            llm_service = LLMService(user_config=user_config)

            # ÂàùÂßãÂåñÂ∑•ÂÖ∑ÈõÜ - ‰º†ÈÄíÊéíÈô§Ê®°ÂºèÂíåÁõÆÊ†áÊñá‰ª∂‰ª•ÂèäÈ¢ÑÂàùÂßãÂåñÁöÑ sandbox_manager
            # üî• ‰º†ÈÄí event_emitter ‰ª•ÂèëÈÄÅÁ¥¢ÂºïËøõÂ∫¶Ôºå‰º†ÈÄí task_id ‰ª•ÊîØÊåÅÂèñÊ∂à
            tools = await _initialize_tools(
                project_root,
                llm_service,
                user_config,
                sandbox_manager=sandbox_manager,
                exclude_patterns=task.exclude_patterns,
                target_files=task.target_files,
                project_id=str(project.id),  # üî• ‰º†ÈÄí project_id Áî®‰∫é RAG
                event_emitter=event_emitter,  # üî• Êñ∞Â¢û
                task_id=task_id,  # üî• Êñ∞Â¢ûÔºöÁî®‰∫éÂèñÊ∂àÊ£ÄÊü•
            )

            # üî• ÂàùÂßãÂåñÂ∑•ÂÖ∑ÂêéÊ£ÄÊü•ÂèñÊ∂à
            if is_task_cancelled(task_id):
                logger.info(f"[Cancel] Task {task_id} cancelled after tools initialization")
                raise asyncio.CancelledError("‰ªªÂä°Â∑≤ÂèñÊ∂à")

            # ÂàõÂª∫Â≠ê Agent
            recon_agent = ReconAgent(
                llm_service=llm_service,
                tools=tools.get("recon", {}),
                event_emitter=event_emitter,
            )

            analysis_agent = AnalysisAgent(
                llm_service=llm_service,
                tools=tools.get("analysis", {}),
                event_emitter=event_emitter,
            )

            verification_agent = VerificationAgent(
                llm_service=llm_service,
                tools=tools.get("verification", {}),
                event_emitter=event_emitter,
            )

            # ÂàõÂª∫ Orchestrator Agent
            orchestrator = OrchestratorAgent(
                llm_service=llm_service,
                tools=tools.get("orchestrator", {}),
                event_emitter=event_emitter,
                sub_agents={
                    "recon": recon_agent,
                    "analysis": analysis_agent,
                    "verification": verification_agent,
                },
            )

            # Ê≥®ÂÜåÂà∞ÂÖ®Â±Ä
            _running_orchestrators[task_id] = orchestrator
            _running_tasks[task_id] = orchestrator  # ÂÖºÂÆπÊóßÁöÑÂèñÊ∂àÈÄªËæë
            _running_event_managers[task_id] = event_manager  # Áî®‰∫é SSE ÊµÅ
            
            # üî• Ê∏ÖÁêÜÊóßÁöÑ Agent Ê≥®ÂÜåË°®ÔºåÈÅøÂÖçÊòæÁ§∫Â§ö‰∏™Ê†ë
            from app.services.agent.core import agent_registry
            agent_registry.clear()
            
            # Ê≥®ÂÜå Orchestrator Âà∞ Agent RegistryÔºà‰ΩøÁî®ÂÖ∂ÂÜÖÁΩÆÊñπÊ≥ïÔºâ
            orchestrator._register_to_registry(task="Root orchestrator for security audit")
            
            await event_emitter.emit_info("üß† Âä®ÊÄÅ Agent Ê†ëÊû∂ÊûÑÂêØÂä®")
            await event_emitter.emit_info(f"üìÅ È°πÁõÆË∑ØÂæÑ: {project_root}")
            
            # Êî∂ÈõÜÈ°πÁõÆ‰ø°ÊÅØ - ‰º†ÈÄíÊéíÈô§Ê®°ÂºèÂíåÁõÆÊ†áÊñá‰ª∂
            project_info = await _collect_project_info(
                project_root, 
                project.name,
                exclude_patterns=task.exclude_patterns,
                target_files=task.target_files,
            )
            
            # Êõ¥Êñ∞‰ªªÂä°Êñá‰ª∂ÁªüËÆ°
            task.total_files = project_info.get("file_count", 0)
            await db.commit()
            
            # ÊûÑÂª∫ËæìÂÖ•Êï∞ÊçÆ
            input_data = {
                "project_info": project_info,
                "config": {
                    "target_vulnerabilities": task.target_vulnerabilities or [],
                    "verification_level": task.verification_level or "sandbox",
                    "exclude_patterns": task.exclude_patterns or [],
                    "target_files": task.target_files or [],
                    "max_iterations": task.max_iterations or 50,
                },
                "project_root": project_root,
                "task_id": task_id,
            }
            
            # ÊâßË°å Orchestrator
            await event_emitter.emit_phase_start("orchestration", "üéØ Orchestrator ÂºÄÂßãÁºñÊéíÂÆ°ËÆ°ÊµÅÁ®ã")
            task.current_phase = AgentTaskPhase.ANALYSIS
            await db.commit()
            
            # üî• Â∞Ü orchestrator.run() ÂåÖË£ÖÂú® asyncio.Task ‰∏≠Ôºå‰ª•‰æøÂèØ‰ª•Âº∫Âà∂ÂèñÊ∂à
            run_task = asyncio.create_task(orchestrator.run(input_data))
            _running_asyncio_tasks[task_id] = run_task
            
            try:
                result = await run_task
            finally:
                _running_asyncio_tasks.pop(task_id, None)
            
            # Â§ÑÁêÜÁªìÊûú
            duration_ms = int((time.time() - start_time) * 1000)
            
            await db.refresh(task)
            
            if result.success:
                # üî• CRITICAL FIX: Log and save findings with detailed debugging
                findings = result.data.get("findings", [])
                logger.info(f"[AgentTask] Task {task_id} completed with {len(findings)} findings from Orchestrator")

                # üî• Debug: Log each finding for verification
                for i, f in enumerate(findings[:5]):  # Log first 5
                    if isinstance(f, dict):
                        logger.debug(f"[AgentTask] Finding {i+1}: {f.get('title', 'N/A')[:50]} - {f.get('severity', 'N/A')}")

                await _save_findings(db, task_id, findings)

                # Êõ¥Êñ∞‰ªªÂä°ÁªüËÆ°
                task.status = AgentTaskStatus.COMPLETED
                task.completed_at = datetime.now(timezone.utc)
                task.current_phase = AgentTaskPhase.REPORTING
                task.findings_count = len(findings)
                task.total_iterations = result.iterations
                task.tool_calls_count = result.tool_calls
                task.tokens_used = result.tokens_used

                # üî• ÁªüËÆ°ÂàÜÊûêÁöÑÊñá‰ª∂Êï∞ÈáèÔºà‰ªé findings ‰∏≠ÊèêÂèñÂîØ‰∏ÄÊñá‰ª∂Ôºâ
                analyzed_file_set = set()
                for f in findings:
                    if isinstance(f, dict):
                        file_path = f.get("file_path") or f.get("file") or f.get("location", "").split(":")[0]
                        if file_path:
                            analyzed_file_set.add(file_path)
                task.analyzed_files = len(analyzed_file_set) if analyzed_file_set else task.total_files

                # ÁªüËÆ°‰∏•ÈáçÁ®ãÂ∫¶ÂíåÈ™åËØÅÁä∂ÊÄÅ
                verified_count = 0
                for f in findings:
                    if isinstance(f, dict):
                        sev = str(f.get("severity", "low")).lower()
                        if sev == "critical":
                            task.critical_count += 1
                        elif sev == "high":
                            task.high_count += 1
                        elif sev == "medium":
                            task.medium_count += 1
                        elif sev == "low":
                            task.low_count += 1
                        # üî• ÁªüËÆ°Â∑≤È™åËØÅÁöÑÂèëÁé∞
                        if f.get("is_verified") or f.get("verdict") == "confirmed":
                            verified_count += 1
                task.verified_count = verified_count
                
                # ËÆ°ÁÆóÂÆâÂÖ®ËØÑÂàÜ
                task.security_score = _calculate_security_score(findings)
                # üî• Ê≥®ÊÑè: progress_percentage ÊòØËÆ°ÁÆóÂ±ûÊÄßÔºå‰∏çÈúÄË¶ÅÊâãÂä®ËÆæÁΩÆ
                # ÂΩì status = COMPLETED Êó∂‰ºöËá™Âä®ËøîÂõû 100.0
                
                await db.commit()
                
                await event_emitter.emit_task_complete(
                    findings_count=len(findings),
                    duration_ms=duration_ms,
                )
                
                logger.info(f"‚úÖ Task {task_id} completed: {len(findings)} findings, {duration_ms}ms")
            else:
                # üî• Ê£ÄÊü•ÊòØÂê¶ÊòØÂèñÊ∂àÂØºËá¥ÁöÑÂ§±Ë¥•
                if result.error == "‰ªªÂä°Â∑≤ÂèñÊ∂à":
                    # Áä∂ÊÄÅÂèØËÉΩÂ∑≤ÁªèË¢´ cancel API Êõ¥Êñ∞ÔºåÂè™ÈúÄÁ°Æ‰øù‰∏ÄËá¥ÊÄß
                    if task.status != AgentTaskStatus.CANCELLED:
                        task.status = AgentTaskStatus.CANCELLED
                        task.completed_at = datetime.now(timezone.utc)
                        await db.commit()
                    logger.info(f"üõë Task {task_id} cancelled")
                else:
                    task.status = AgentTaskStatus.FAILED
                    task.error_message = result.error or "Unknown error"
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
                    
                    await event_emitter.emit_error(result.error or "Unknown error")
                    logger.error(f"‚ùå Task {task_id} failed: {result.error}")
            
        except asyncio.CancelledError:
            logger.info(f"Task {task_id} cancelled")
            try:
                task = await db.get(AgentTask, task_id)
                if task:
                    task.status = AgentTaskStatus.CANCELLED
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
            except Exception:
                pass
                
        except Exception as e:
            logger.error(f"Task {task_id} failed: {e}", exc_info=True)
            
            try:
                task = await db.get(AgentTask, task_id)
                if task:
                    task.status = AgentTaskStatus.FAILED
                    task.error_message = str(e)[:1000]
                    task.completed_at = datetime.now(timezone.utc)
                    await db.commit()
            except Exception as db_error:
                logger.error(f"Failed to update task status: {db_error}")
        
        finally:
            # üî• Âú®Ê∏ÖÁêÜ‰πãÂâç‰øùÂ≠ò Agent Ê†ëÂà∞Êï∞ÊçÆÂ∫ì
            try:
                async with async_session_factory() as save_db:
                    await _save_agent_tree(save_db, task_id)
            except Exception as save_error:
                logger.error(f"Failed to save agent tree: {save_error}")

            # Ê∏ÖÁêÜ
            _running_orchestrators.pop(task_id, None)
            _running_tasks.pop(task_id, None)
            _running_event_managers.pop(task_id, None)
            _running_asyncio_tasks.pop(task_id, None)  # üî• Ê∏ÖÁêÜ asyncio task
            _cancelled_tasks.discard(task_id)  # üî• Ê∏ÖÁêÜÂèñÊ∂àÊ†áÂøó

            # üî• Ê∏ÖÁêÜÊï¥‰∏™ Agent Ê≥®ÂÜåË°®ÔºàÂåÖÊã¨ÊâÄÊúâÂ≠ê AgentÔºâ
            agent_registry.clear()

            logger.debug(f"Task {task_id} cleaned up")


async def _get_user_config(db: AsyncSession, user_id: Optional[str]) -> Optional[Dict[str, Any]]:
    """Ëé∑ÂèñÁî®Êà∑ÈÖçÁΩÆ"""
    if not user_id:
        return None
    
    try:
        from app.api.v1.endpoints.config import (
            decrypt_config, 
            SENSITIVE_LLM_FIELDS, SENSITIVE_OTHER_FIELDS
        )
        
        result = await db.execute(
            select(UserConfig).where(UserConfig.user_id == user_id)
        )
        config = result.scalar_one_or_none()
        
        if config and config.llm_config:
            user_llm_config = json.loads(config.llm_config) if config.llm_config else {}
            user_other_config = json.loads(config.other_config) if config.other_config else {}
            
            user_llm_config = decrypt_config(user_llm_config, SENSITIVE_LLM_FIELDS)
            user_other_config = decrypt_config(user_other_config, SENSITIVE_OTHER_FIELDS)
            
            return {
                "llmConfig": user_llm_config,
                "otherConfig": user_other_config,
            }
    except Exception as e:
        logger.warning(f"Failed to get user config: {e}")
    
    return None


async def _initialize_tools(
    project_root: str,
    llm_service,
    user_config: Optional[Dict[str, Any]],
    sandbox_manager: Any, # ‰º†ÈÄíÈ¢ÑÂàùÂßãÂåñÁöÑ SandboxManager
    exclude_patterns: Optional[List[str]] = None,
    target_files: Optional[List[str]] = None,
    project_id: Optional[str] = None,  # üî• Áî®‰∫é RAG collection_name
    event_emitter: Optional[Any] = None,  # üî• Êñ∞Â¢ûÔºöÁî®‰∫éÂèëÈÄÅÂÆûÊó∂Êó•Âøó
    task_id: Optional[str] = None,  # üî• Êñ∞Â¢ûÔºöÁî®‰∫éÂèñÊ∂àÊ£ÄÊü•
) -> Dict[str, Dict[str, Any]]:
    """ÂàùÂßãÂåñÂ∑•ÂÖ∑ÈõÜ

    Args:
        project_root: È°πÁõÆÊ†πÁõÆÂΩï
        llm_service: LLM ÊúçÂä°
        user_config: Áî®Êà∑ÈÖçÁΩÆ
        sandbox_manager: Ê≤ôÁÆ±ÁÆ°ÁêÜÂô®
        exclude_patterns: ÊéíÈô§Ê®°ÂºèÂàóË°®
        target_files: ÁõÆÊ†áÊñá‰ª∂ÂàóË°®
        project_id: È°πÁõÆ IDÔºàÁî®‰∫é RAG collection_nameÔºâ
        event_emitter: ‰∫ã‰ª∂ÂèëÈÄÅÂô®ÔºàÁî®‰∫éÂèëÈÄÅÂÆûÊó∂Êó•ÂøóÔºâ
        task_id: ‰ªªÂä° IDÔºàÁî®‰∫éÂèñÊ∂àÊ£ÄÊü•Ôºâ
    """
    from app.services.agent.tools import (
        FileReadTool, FileSearchTool, ListFilesTool,
        PatternMatchTool, CodeAnalysisTool, DataFlowAnalysisTool,
        SemgrepTool, BanditTool, GitleaksTool,
        NpmAuditTool, SafetyTool, TruffleHogTool, OSVScannerTool,  # üî• Added missing tools
        ThinkTool, ReflectTool,
        CreateVulnerabilityReportTool,
        VulnerabilityValidationTool,
        # üî• RAG Â∑•ÂÖ∑
        RAGQueryTool, SecurityCodeSearchTool, FunctionContextTool,
    )
    from app.services.agent.knowledge import (
        SecurityKnowledgeQueryTool,
        GetVulnerabilityKnowledgeTool,
    )
    # üî• RAG Áõ∏ÂÖ≥ÂØºÂÖ•
    from app.services.rag import CodeIndexer, CodeRetriever, EmbeddingService, IndexUpdateMode
    from app.core.config import settings

    # ËæÖÂä©ÂáΩÊï∞ÔºöÂèëÈÄÅ‰∫ã‰ª∂
    async def emit(message: str, level: str = "info"):
        if event_emitter:
            logger.debug(f"[EMIT-TOOLS] Sending {level}: {message[:60]}...")
            if level == "info":
                await event_emitter.emit_info(message)
            elif level == "warning":
                await event_emitter.emit_warning(message)
            elif level == "error":
                await event_emitter.emit_error(message)
        else:
            logger.warning(f"[EMIT-TOOLS] No event_emitter, skipping: {message[:60]}...")

    # ============ üî• ÂàùÂßãÂåñ RAG Á≥ªÁªü ============
    retriever = None
    try:
        await emit(f"üîç Ê≠£Âú®ÂàùÂßãÂåñ RAG Á≥ªÁªü...")

        # ‰ªéÁî®Êà∑ÈÖçÁΩÆ‰∏≠Ëé∑Âèñ embedding ÈÖçÁΩÆ
        user_llm_config = (user_config or {}).get('llmConfig', {})
        user_other_config = (user_config or {}).get('otherConfig', {})
        user_embedding_config = user_other_config.get('embedding_config', {})

        # Embedding Provider ‰ºòÂÖàÁ∫ßÔºöÁî®Êà∑ÂµåÂÖ•ÈÖçÁΩÆ > ÁéØÂ¢ÉÂèòÈáè
        embedding_provider = (
            user_embedding_config.get('provider') or
            getattr(settings, 'EMBEDDING_PROVIDER', 'openai')
        )

        # Embedding Model ‰ºòÂÖàÁ∫ßÔºöÁî®Êà∑ÂµåÂÖ•ÈÖçÁΩÆ > ÁéØÂ¢ÉÂèòÈáè
        embedding_model = (
            user_embedding_config.get('model') or
            getattr(settings, 'EMBEDDING_MODEL', 'text-embedding-3-small')
        )

        # API Key ‰ºòÂÖàÁ∫ßÔºöÁî®Êà∑ÂµåÂÖ•ÈÖçÁΩÆ > ÁéØÂ¢ÉÂèòÈáè EMBEDDING_API_KEY > Áî®Êà∑ LLM ÈÖçÁΩÆ > ÁéØÂ¢ÉÂèòÈáè LLM_API_KEY
        # Ê≥®ÊÑèÔºöAPI Key ÂèØ‰ª•ÂÖ±‰∫´ÔºåÂõ†‰∏∫ÂæàÂ§öÁî®Êà∑‰ΩøÁî®Âêå‰∏Ä‰∏™ OpenAI Key ÂÅö LLM Âíå Embedding
        embedding_api_key = (
            user_embedding_config.get('api_key') or
            getattr(settings, 'EMBEDDING_API_KEY', None) or
            user_llm_config.get('llmApiKey') or
            getattr(settings, 'LLM_API_KEY', '') or
            ''
        )

        # Base URL ‰ºòÂÖàÁ∫ßÔºöÁî®Êà∑ÂµåÂÖ•ÈÖçÁΩÆ > ÁéØÂ¢ÉÂèòÈáè EMBEDDING_BASE_URL > NoneÔºà‰ΩøÁî®Êèê‰æõÂïÜÈªòËÆ§Âú∞ÂùÄÔºâ
        # üî• ÈáçË¶ÅÔºöBase URL ‰∏çÂ∫îËØ•ÂõûÈÄÄÂà∞ LLM ÁöÑ base_urlÔºåÂõ†‰∏∫ Embedding Âíå LLM ÂèØËÉΩ‰ΩøÁî®ÂÆåÂÖ®‰∏çÂêåÁöÑÊúçÂä°
        # ‰æãÂ¶ÇÔºöLLM ‰ΩøÁî® SiliconFlowÔºå‰ΩÜ Embedding ‰ΩøÁî® HuggingFace
        embedding_base_url = (
            user_embedding_config.get('base_url') or
            getattr(settings, 'EMBEDDING_BASE_URL', None) or
            None
        )

        logger.info(f"RAG ÈÖçÁΩÆ: provider={embedding_provider}, model={embedding_model}, base_url={embedding_base_url or '(‰ΩøÁî®ÈªòËÆ§)'}")
        await emit(f"üìä Embedding ÈÖçÁΩÆ: {embedding_provider}/{embedding_model}")

        # ÂàõÂª∫ Embedding ÊúçÂä°
        embedding_service = EmbeddingService(
            provider=embedding_provider,
            model=embedding_model,
            api_key=embedding_api_key,
            base_url=embedding_base_url,
        )

        # ÂàõÂª∫ collection_nameÔºàÂü∫‰∫é project_idÔºâ
        collection_name = f"project_{project_id}" if project_id else "default_project"

        # üî• v2.0: ÂàõÂª∫ CodeIndexer Âπ∂ËøõË°åÊô∫ËÉΩÁ¥¢Âºï
        # Êô∫ËÉΩÁ¥¢Âºï‰ºöËá™Âä®Ôºö
        # - Ê£ÄÊµã embedding Ê®°ÂûãÂèòÊõ¥ÔºåÂ¶ÇÈúÄË¶ÅÂàôËá™Âä®ÈáçÂª∫
        # - ÂØπÊØîÊñá‰ª∂ hashÔºåÂè™Êõ¥Êñ∞ÂèòÂåñÁöÑÊñá‰ª∂ÔºàÂ¢ûÈáèÊõ¥Êñ∞Ôºâ
        indexer = CodeIndexer(
            collection_name=collection_name,
            embedding_service=embedding_service,
            persist_directory=settings.VECTOR_DB_PATH,
        )

        logger.info(f"üìù ÂºÄÂßãÊô∫ËÉΩÁ¥¢ÂºïÈ°πÁõÆ: {project_root}")
        await emit(f"üìù Ê≠£Âú®ÊûÑÂª∫‰ª£Á†ÅÂêëÈáèÁ¥¢Âºï...")

        index_progress = None
        last_progress_update = 0
        async for progress in indexer.smart_index_directory(
            directory=project_root,
            exclude_patterns=exclude_patterns or [],
            update_mode=IndexUpdateMode.SMART,
        ):
            # üî• Âú®Á¥¢ÂºïËøáÁ®ã‰∏≠Ê£ÄÊü•ÂèñÊ∂àÁä∂ÊÄÅ
            if task_id and is_task_cancelled(task_id):
                logger.info(f"[Cancel] RAG indexing cancelled for task {task_id}")
                raise asyncio.CancelledError("‰ªªÂä°Â∑≤ÂèñÊ∂à")

            index_progress = progress
            # ÊØèÂ§ÑÁêÜ 10 ‰∏™Êñá‰ª∂ÊàñÊúâÈáçË¶ÅÂèòÂåñÊó∂ÂèëÈÄÅËøõÂ∫¶Êõ¥Êñ∞
            if progress.processed_files - last_progress_update >= 10 or progress.processed_files == progress.total_files:
                if progress.total_files > 0:
                    await emit(
                        f"üìù Á¥¢ÂºïËøõÂ∫¶: {progress.processed_files}/{progress.total_files} Êñá‰ª∂ "
                        f"({progress.progress_percentage:.0f}%)"
                    )
                last_progress_update = progress.processed_files

        if index_progress:
            summary = (
                f"‚úÖ Á¥¢ÂºïÂÆåÊàê: Ê®°Âºè={index_progress.update_mode}, "
                f"Êñ∞Â¢û={index_progress.added_files}, "
                f"Êõ¥Êñ∞={index_progress.updated_files}, "
                f"Âà†Èô§={index_progress.deleted_files}, "
                f"‰ª£Á†ÅÂùó={index_progress.indexed_chunks}"
            )
            logger.info(summary)
            await emit(summary)

        # ÂàõÂª∫ CodeRetrieverÔºàÁî®‰∫éÊêúÁ¥¢Ôºâ
        # üî• ‰º†ÈÄí api_keyÔºåÁî®‰∫éËá™Âä®ÈÄÇÈÖç collection ÁöÑ embedding ÈÖçÁΩÆ
        retriever = CodeRetriever(
            collection_name=collection_name,
            embedding_service=embedding_service,
            persist_directory=settings.VECTOR_DB_PATH,
            api_key=embedding_api_key,  # üî• ‰º†ÈÄí api_key ‰ª•ÊîØÊåÅËá™Âä®ÂàáÊç¢ embedding
        )

        logger.info(f"‚úÖ RAG Á≥ªÁªüÂàùÂßãÂåñÊàêÂäü: collection={collection_name}")
        await emit(f"‚úÖ RAG Á≥ªÁªüÂàùÂßãÂåñÊàêÂäü")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è RAG Á≥ªÁªüÂàùÂßãÂåñÂ§±Ë¥•: {e}")
        await emit(f"‚ö†Ô∏è RAG Á≥ªÁªüÂàùÂßãÂåñÂ§±Ë¥•: {e}", "warning")
        import traceback
        logger.debug(f"RAG ÂàùÂßãÂåñÂºÇÂ∏∏ËØ¶ÊÉÖ:\n{traceback.format_exc()}")
        retriever = None

    # Âü∫Á°ÄÂ∑•ÂÖ∑ - ‰º†ÈÄíÊéíÈô§Ê®°ÂºèÂíåÁõÆÊ†áÊñá‰ª∂
    base_tools = {
        "read_file": FileReadTool(project_root, exclude_patterns, target_files),
        "list_files": ListFilesTool(project_root, exclude_patterns, target_files),
        "search_code": FileSearchTool(project_root, exclude_patterns, target_files),
        "think": ThinkTool(),
        "reflect": ReflectTool(),
    }
    
    # Recon Â∑•ÂÖ∑
    recon_tools = {
        **base_tools,
        # üî• Â§ñÈÉ®‰æ¶ÂØüÂ∑•ÂÖ∑ (Recon Èò∂ÊÆµ‰πüÈúÄË¶Å‰ΩøÁî®Ëøô‰∫õÂ∑•ÂÖ∑Êù•Êî∂ÈõÜÂàùÊ≠•‰ø°ÊÅØ)
        "semgrep_scan": SemgrepTool(project_root, sandbox_manager),
        "bandit_scan": BanditTool(project_root, sandbox_manager),
        "gitleaks_scan": GitleaksTool(project_root, sandbox_manager),
        "npm_audit": NpmAuditTool(project_root, sandbox_manager),
        "safety_scan": SafetyTool(project_root, sandbox_manager),
        "trufflehog_scan": TruffleHogTool(project_root, sandbox_manager),
        "osv_scan": OSVScannerTool(project_root, sandbox_manager),
    }

    # üî• Ê≥®ÂÜå RAG Â∑•ÂÖ∑Âà∞ Recon Agent
    if retriever:
        recon_tools["rag_query"] = RAGQueryTool(retriever)
        logger.info("‚úÖ RAG Â∑•ÂÖ∑ (rag_query) Â∑≤Ê≥®ÂÜåÂà∞ Recon Agent")
    
    # Analysis Â∑•ÂÖ∑
    # üî• ÂØºÂÖ•Êô∫ËÉΩÊâ´ÊèèÂ∑•ÂÖ∑
    from app.services.agent.tools import SmartScanTool, QuickAuditTool
    
    analysis_tools = {
        **base_tools,
        # üî• Êô∫ËÉΩÊâ´ÊèèÂ∑•ÂÖ∑ÔºàÊé®ËçêÈ¶ñÂÖà‰ΩøÁî®Ôºâ
        "smart_scan": SmartScanTool(project_root),
        "quick_audit": QuickAuditTool(project_root),
        # Ê®°ÂºèÂåπÈÖçÂ∑•ÂÖ∑ÔºàÂ¢ûÂº∫ÁâàÔºâ
        "pattern_match": PatternMatchTool(project_root),
        # Êï∞ÊçÆÊµÅÂàÜÊûê
        "dataflow_analysis": DataFlowAnalysisTool(llm_service),
        # Â§ñÈÉ®ÂÆâÂÖ®Â∑•ÂÖ∑ (‰º†ÂÖ•ÂÖ±‰∫´ÁöÑ sandbox_manager)
        "semgrep_scan": SemgrepTool(project_root, sandbox_manager),
        "bandit_scan": BanditTool(project_root, sandbox_manager),
        "gitleaks_scan": GitleaksTool(project_root, sandbox_manager),
        "npm_audit": NpmAuditTool(project_root, sandbox_manager),
        "safety_scan": SafetyTool(project_root, sandbox_manager),
        "trufflehog_scan": TruffleHogTool(project_root, sandbox_manager),
        "osv_scan": OSVScannerTool(project_root, sandbox_manager),
        # ÂÆâÂÖ®Áü•ËØÜÊü•ËØ¢
        "query_security_knowledge": SecurityKnowledgeQueryTool(),
        "get_vulnerability_knowledge": GetVulnerabilityKnowledgeTool(),
    }

    # üî• Ê≥®ÂÜå RAG Â∑•ÂÖ∑Âà∞ Analysis Agent
    if retriever:
        analysis_tools["rag_query"] = RAGQueryTool(retriever)
        analysis_tools["security_search"] = SecurityCodeSearchTool(retriever)
        analysis_tools["function_context"] = FunctionContextTool(retriever)
        logger.info("‚úÖ RAG Â∑•ÂÖ∑ (rag_query, security_search, function_context) Â∑≤Ê≥®ÂÜåÂà∞ Analysis Agent")
    else:
        logger.warning("‚ö†Ô∏è RAG Êú™ÂàùÂßãÂåñÔºårag_query/security_search/function_context Â∑•ÂÖ∑‰∏çÂèØÁî®")
    
    # Verification Â∑•ÂÖ∑
    # üî• ÂØºÂÖ•Ê≤ôÁÆ±Â∑•ÂÖ∑
    from app.services.agent.tools import (
        SandboxTool, SandboxHttpTool, VulnerabilityVerifyTool,
        # Â§öËØ≠Ë®Ä‰ª£Á†ÅÊµãËØïÂ∑•ÂÖ∑
        PhpTestTool, PythonTestTool, JavaScriptTestTool, JavaTestTool,
        GoTestTool, RubyTestTool, ShellTestTool, UniversalCodeTestTool,
        # ÊºèÊ¥ûÈ™åËØÅ‰∏ìÁî®Â∑•ÂÖ∑
        CommandInjectionTestTool, SqlInjectionTestTool, XssTestTool,
        PathTraversalTestTool, SstiTestTool, DeserializationTestTool,
        UniversalVulnTestTool,
    )

    verification_tools = {
        **base_tools,
        # üî• Ê≤ôÁÆ±È™åËØÅÂ∑•ÂÖ∑
        "sandbox_exec": SandboxTool(sandbox_manager),
        "sandbox_http": SandboxHttpTool(sandbox_manager),
        "verify_vulnerability": VulnerabilityVerifyTool(sandbox_manager),

        # üî• Â§öËØ≠Ë®Ä‰ª£Á†ÅÊµãËØïÂ∑•ÂÖ∑
        "php_test": PhpTestTool(sandbox_manager, project_root),
        "python_test": PythonTestTool(sandbox_manager, project_root),
        "javascript_test": JavaScriptTestTool(sandbox_manager, project_root),
        "java_test": JavaTestTool(sandbox_manager, project_root),
        "go_test": GoTestTool(sandbox_manager, project_root),
        "ruby_test": RubyTestTool(sandbox_manager, project_root),
        "shell_test": ShellTestTool(sandbox_manager, project_root),
        "universal_code_test": UniversalCodeTestTool(sandbox_manager, project_root),

        # üî• ÊºèÊ¥ûÈ™åËØÅ‰∏ìÁî®Â∑•ÂÖ∑
        "test_command_injection": CommandInjectionTestTool(sandbox_manager, project_root),
        "test_sql_injection": SqlInjectionTestTool(sandbox_manager, project_root),
        "test_xss": XssTestTool(sandbox_manager, project_root),
        "test_path_traversal": PathTraversalTestTool(sandbox_manager, project_root),
        "test_ssti": SstiTestTool(sandbox_manager, project_root),
        "test_deserialization": DeserializationTestTool(sandbox_manager, project_root),
        "universal_vuln_test": UniversalVulnTestTool(sandbox_manager, project_root),

        # Êä•ÂëäÂ∑•ÂÖ∑
        "create_vulnerability_report": CreateVulnerabilityReportTool(),
    }
    
    # Orchestrator Â∑•ÂÖ∑Ôºà‰∏ªË¶ÅÊòØÊÄùËÄÉÂ∑•ÂÖ∑Ôºâ
    orchestrator_tools = {
        "think": ThinkTool(),
        "reflect": ReflectTool(),
    }
    
    return {
        "recon": recon_tools,
        "analysis": analysis_tools,
        "verification": verification_tools,
        "orchestrator": orchestrator_tools,
    }


async def _collect_project_info(
    project_root: str, 
    project_name: str,
    exclude_patterns: Optional[List[str]] = None,
    target_files: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Êî∂ÈõÜÈ°πÁõÆ‰ø°ÊÅØ
    
    Args:
        project_root: È°πÁõÆÊ†πÁõÆÂΩï
        project_name: È°πÁõÆÂêçÁß∞
        exclude_patterns: ÊéíÈô§Ê®°ÂºèÂàóË°®
        target_files: ÁõÆÊ†áÊñá‰ª∂ÂàóË°®
    
    üî• ÈáçË¶ÅÔºöÂΩìÊåáÂÆö‰∫Ü target_files Êó∂ÔºåËøîÂõûÁöÑÈ°πÁõÆÁªìÊûÑÂ∫îËØ•Âè™ÂåÖÂê´ÁõÆÊ†áÊñá‰ª∂Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºå
    ‰ª•Á°Æ‰øù Orchestrator ÂíåÂ≠ê Agent ÁúãÂà∞ÁöÑÊòØ‰∏ÄËá¥ÁöÑ„ÄÅËøáÊª§ÂêéÁöÑËßÜÂõæ„ÄÇ
    """
    import fnmatch
    
    info = {
        "name": project_name,
        "root": project_root,
        "languages": [],
        "file_count": 0,
        "structure": {},
    }
    
    try:
        # ÈªòËÆ§ÊéíÈô§ÁõÆÂΩï
        exclude_dirs = {
            "node_modules", "__pycache__", ".git", "venv", ".venv",
            "build", "dist", "target", ".idea", ".vscode",
        }
        
        # ‰ªéÁî®Êà∑ÈÖçÁΩÆÁöÑÊéíÈô§Ê®°Âºè‰∏≠ÊèêÂèñÁõÆÂΩï
        if exclude_patterns:
            for pattern in exclude_patterns:
                if pattern.endswith("/**"):
                    exclude_dirs.add(pattern[:-3])
                elif "/" not in pattern and "*" not in pattern:
                    exclude_dirs.add(pattern)
        
        # ÁõÆÊ†áÊñá‰ª∂ÈõÜÂêà
        target_files_set = set(target_files) if target_files else None
        
        lang_map = {
            ".py": "Python", ".js": "JavaScript", ".ts": "TypeScript",
            ".java": "Java", ".go": "Go", ".php": "PHP",
            ".rb": "Ruby", ".rs": "Rust", ".c": "C", ".cpp": "C++",
        }
        
        # üî• Êî∂ÈõÜËøáÊª§ÂêéÁöÑÊñá‰ª∂ÂàóË°®
        filtered_files = []
        filtered_dirs = set()
        
        for root, dirs, files in os.walk(project_root):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for f in files:
                relative_path = os.path.relpath(os.path.join(root, f), project_root)
                
                # Ê£ÄÊü•ÊòØÂê¶Âú®ÁõÆÊ†áÊñá‰ª∂ÂàóË°®‰∏≠
                if target_files_set and relative_path not in target_files_set:
                    continue
                
                # Ê£ÄÊü•ÊéíÈô§Ê®°Âºè
                should_skip = False
                if exclude_patterns:
                    for pattern in exclude_patterns:
                        if fnmatch.fnmatch(relative_path, pattern) or fnmatch.fnmatch(f, pattern):
                            should_skip = True
                            break
                if should_skip:
                    continue
                
                info["file_count"] += 1
                filtered_files.append(relative_path)
                
                # üî• Êî∂ÈõÜÊñá‰ª∂ÊâÄÂú®ÁöÑÁõÆÂΩï
                dir_path = os.path.dirname(relative_path)
                if dir_path:
                    # Ê∑ªÂä†ÁõÆÂΩïÂèäÂÖ∂Áà∂ÁõÆÂΩï
                    parts = dir_path.split(os.sep)
                    for i in range(len(parts)):
                        filtered_dirs.add(os.sep.join(parts[:i+1]))
                
                ext = os.path.splitext(f)[1].lower()
                if ext in lang_map and lang_map[ext] not in info["languages"]:
                    info["languages"].append(lang_map[ext])
        
        # üî• Ê†πÊçÆÊòØÂê¶ÊúâÁõÆÊ†áÊñá‰ª∂ÈôêÂà∂ÔºåÁîüÊàê‰∏çÂêåÁöÑÁªìÊûÑ‰ø°ÊÅØ
        if target_files_set:
            # ÂΩìÊåáÂÆö‰∫ÜÁõÆÊ†áÊñá‰ª∂Êó∂ÔºåÂè™ÊòæÁ§∫ÁõÆÊ†áÊñá‰ª∂ÂíåÁõ∏ÂÖ≥ÁõÆÂΩï
            info["structure"] = {
                "directories": sorted(list(filtered_dirs))[:20],
                "files": filtered_files[:30],
                "scope_limited": True,  # üî• Ê†áËÆ∞ËøôÊòØÈôêÂÆöËåÉÂõ¥ÁöÑËßÜÂõæ
                "scope_message": f"ÂÆ°ËÆ°ËåÉÂõ¥ÈôêÂÆö‰∏∫ {len(filtered_files)} ‰∏™ÊåáÂÆöÊñá‰ª∂",
            }
        else:
            # ÂÖ®È°πÁõÆÂÆ°ËÆ°Êó∂ÔºåÊòæÁ§∫È°∂Â±ÇÁõÆÂΩïÁªìÊûÑ
            try:
                top_items = os.listdir(project_root)
                info["structure"] = {
                    "directories": [d for d in top_items if os.path.isdir(os.path.join(project_root, d)) and d not in exclude_dirs],
                    "files": [f for f in top_items if os.path.isfile(os.path.join(project_root, f))][:20],
                    "scope_limited": False,
                }
            except Exception:
                pass
            
    except Exception as e:
        logger.warning(f"Failed to collect project info: {e}")
    
    return info


async def _save_findings(db: AsyncSession, task_id: str, findings: List[Dict]) -> None:
    """
    ‰øùÂ≠òÂèëÁé∞Âà∞Êï∞ÊçÆÂ∫ì

    üî• Â¢ûÂº∫ÁâàÔºöÊîØÊåÅÂ§öÁßç Agent ËæìÂá∫Ê†ºÂºèÔºåÂÅ•Â£ÆÁöÑÂ≠óÊÆµÊò†Â∞Ñ
    """
    from app.models.agent_task import VulnerabilityType

    logger.info(f"[SaveFindings] Starting to save {len(findings)} findings for task {task_id}")

    if not findings:
        logger.warning(f"[SaveFindings] No findings to save for task {task_id}")
        return

    # üî• Case-insensitive mapping preparation
    severity_map = {
        "critical": VulnerabilitySeverity.CRITICAL,
        "high": VulnerabilitySeverity.HIGH,
        "medium": VulnerabilitySeverity.MEDIUM,
        "low": VulnerabilitySeverity.LOW,
        "info": VulnerabilitySeverity.INFO,
    }

    type_map = {
        "sql_injection": VulnerabilityType.SQL_INJECTION,
        "nosql_injection": VulnerabilityType.NOSQL_INJECTION,
        "xss": VulnerabilityType.XSS,
        "command_injection": VulnerabilityType.COMMAND_INJECTION,
        "code_injection": VulnerabilityType.CODE_INJECTION,
        "path_traversal": VulnerabilityType.PATH_TRAVERSAL,
        "ssrf": VulnerabilityType.SSRF,
        "xxe": VulnerabilityType.XXE,
        "auth_bypass": VulnerabilityType.AUTH_BYPASS,
        "idor": VulnerabilityType.IDOR,
        "sensitive_data_exposure": VulnerabilityType.SENSITIVE_DATA_EXPOSURE,
        "hardcoded_secret": VulnerabilityType.HARDCODED_SECRET,
        "deserialization": VulnerabilityType.DESERIALIZATION,
        "weak_crypto": VulnerabilityType.WEAK_CRYPTO,
        "file_inclusion": VulnerabilityType.FILE_INCLUSION,
        "race_condition": VulnerabilityType.RACE_CONDITION,
        "business_logic": VulnerabilityType.BUSINESS_LOGIC,
        "memory_corruption": VulnerabilityType.MEMORY_CORRUPTION,
    }

    saved_count = 0
    logger.info(f"Saving {len(findings)} findings for task {task_id}")

    for finding in findings:
        if not isinstance(finding, dict):
            logger.debug(f"[SaveFindings] Skipping non-dict finding: {type(finding)}")
            continue

        try:
            # üî• Handle severity (case-insensitive, support multiple field names)
            raw_severity = str(
                finding.get("severity") or
                finding.get("risk") or
                "medium"
            ).lower().strip()
            severity_enum = severity_map.get(raw_severity, VulnerabilitySeverity.MEDIUM)

            # üî• Handle vulnerability type (case-insensitive & snake_case normalization)
            # Support multiple field names: vulnerability_type, type, vuln_type
            raw_type = str(
                finding.get("vulnerability_type") or
                finding.get("type") or
                finding.get("vuln_type") or
                "other"
            ).lower().strip().replace(" ", "_").replace("-", "_")

            type_enum = type_map.get(raw_type, VulnerabilityType.OTHER)

            # üî• Additional fallback for common Agent output variations
            if "sqli" in raw_type or "sql" in raw_type:
                type_enum = VulnerabilityType.SQL_INJECTION
            if "xss" in raw_type:
                type_enum = VulnerabilityType.XSS
            if "rce" in raw_type or "command" in raw_type or "cmd" in raw_type:
                type_enum = VulnerabilityType.COMMAND_INJECTION
            if "traversal" in raw_type or "lfi" in raw_type or "rfi" in raw_type:
                type_enum = VulnerabilityType.PATH_TRAVERSAL
            if "ssrf" in raw_type:
                type_enum = VulnerabilityType.SSRF
            if "xxe" in raw_type:
                type_enum = VulnerabilityType.XXE
            if "auth" in raw_type:
                type_enum = VulnerabilityType.AUTH_BYPASS
            if "secret" in raw_type or "credential" in raw_type or "password" in raw_type:
                type_enum = VulnerabilityType.HARDCODED_SECRET
            if "deserial" in raw_type:
                type_enum = VulnerabilityType.DESERIALIZATION

            # üî• Handle file path (support multiple field names)
            file_path = (
                finding.get("file_path") or
                finding.get("file") or
                finding.get("location", "").split(":")[0] if ":" in finding.get("location", "") else finding.get("location")
            )

            # üî• Handle line numbers (support multiple formats)
            line_start = finding.get("line_start") or finding.get("line")
            if not line_start and ":" in finding.get("location", ""):
                try:
                    line_start = int(finding.get("location", "").split(":")[1])
                except (ValueError, IndexError):
                    line_start = None

            line_end = finding.get("line_end") or line_start

            # üî• Handle code snippet (support multiple field names)
            code_snippet = (
                finding.get("code_snippet") or
                finding.get("code") or
                finding.get("vulnerable_code")
            )

            # üî• Handle title (generate from type if not provided)
            title = finding.get("title")
            if not title:
                # Generate title from vulnerability type and file
                type_display = raw_type.replace("_", " ").title()
                if file_path:
                    title = f"{type_display} in {os.path.basename(file_path)}"
                else:
                    title = f"{type_display} Vulnerability"

            # üî• Handle description (support multiple field names)
            description = (
                finding.get("description") or
                finding.get("details") or
                finding.get("explanation") or
                finding.get("impact") or
                ""
            )

            # üî• Handle suggestion/recommendation
            suggestion = (
                finding.get("suggestion") or
                finding.get("recommendation") or
                finding.get("remediation") or
                finding.get("fix")
            )

            # üî• Handle confidence (map to ai_confidence field in model)
            confidence = finding.get("confidence") or finding.get("ai_confidence") or 0.5
            if isinstance(confidence, str):
                try:
                    confidence = float(confidence)
                except ValueError:
                    confidence = 0.5

            # üî• Handle verification status
            is_verified = finding.get("is_verified", False)
            if finding.get("verdict") == "confirmed":
                is_verified = True

            # üî• Handle PoC information
            poc_data = finding.get("poc", {})
            has_poc = bool(poc_data)
            poc_code = None
            poc_description = None
            poc_steps = None

            if isinstance(poc_data, dict):
                poc_description = poc_data.get("description")
                poc_steps = poc_data.get("steps")
                poc_code = poc_data.get("payload") or poc_data.get("code")
            elif isinstance(poc_data, str):
                poc_description = poc_data

            # üî• Handle verification details
            verification_method = finding.get("verification_method")
            verification_result = None
            if finding.get("verification_details"):
                verification_result = {"details": finding.get("verification_details")}

            # üî• Handle CWE and CVSS
            cwe_id = finding.get("cwe_id") or finding.get("cwe")
            cvss_score = finding.get("cvss_score") or finding.get("cvss")
            if isinstance(cvss_score, str):
                try:
                    cvss_score = float(cvss_score)
                except ValueError:
                    cvss_score = None

            db_finding = AgentFinding(
                id=str(uuid4()),
                task_id=task_id,
                vulnerability_type=type_enum,
                severity=severity_enum,
                title=title[:500] if title else "Unknown Vulnerability",
                description=description[:5000] if description else "",
                file_path=file_path[:500] if file_path else None,
                line_start=line_start,
                line_end=line_end,
                code_snippet=code_snippet[:10000] if code_snippet else None,
                suggestion=suggestion[:5000] if suggestion else None,
                is_verified=is_verified,
                ai_confidence=confidence,  # üî• FIX: Use ai_confidence, not confidence
                status=FindingStatus.VERIFIED if is_verified else FindingStatus.NEW,
                # üî• Additional fields
                has_poc=has_poc,
                poc_code=poc_code,
                poc_description=poc_description,
                poc_steps=poc_steps,
                verification_method=verification_method,
                verification_result=verification_result,
                cvss_score=cvss_score,
                # References for CWE
                references=[{"cwe": cwe_id}] if cwe_id else None,
            )
            db.add(db_finding)
            saved_count += 1
            logger.debug(f"[SaveFindings] Prepared finding: {title[:50]}... ({severity_enum})")

        except Exception as e:
            logger.warning(f"Failed to save finding: {e}, data: {finding}")
            import traceback
            logger.debug(f"[SaveFindings] Traceback: {traceback.format_exc()}")

    logger.info(f"Successfully prepared {saved_count} findings for commit")

    try:
        await db.commit()
        logger.info(f"[SaveFindings] Successfully committed {saved_count} findings to database")
    except Exception as e:
        logger.error(f"Failed to commit findings: {e}")
        await db.rollback()


def _calculate_security_score(findings: List[Dict]) -> float:
    """ËÆ°ÁÆóÂÆâÂÖ®ËØÑÂàÜ"""
    if not findings:
        return 100.0

    # Âü∫‰∫éÂèëÁé∞ÁöÑ‰∏•ÈáçÁ®ãÂ∫¶ËÆ°ÁÆóÊâ£ÂàÜ
    deductions = {
        "critical": 25,
        "high": 15,
        "medium": 8,
        "low": 3,
        "info": 1,
    }

    total_deduction = 0
    for f in findings:
        if isinstance(f, dict):
            sev = f.get("severity", "low")
            total_deduction += deductions.get(sev, 3)

    score = max(0, 100 - total_deduction)
    return float(score)


async def _save_agent_tree(db: AsyncSession, task_id: str) -> None:
    """
    ‰øùÂ≠ò Agent Ê†ëÂà∞Êï∞ÊçÆÂ∫ì

    üî• Âú®‰ªªÂä°ÂÆåÊàêÂâçË∞ÉÁî®ÔºåÂ∞ÜÂÜÖÂ≠ò‰∏≠ÁöÑ Agent Ê†ëÊåÅ‰πÖÂåñÂà∞Êï∞ÊçÆÂ∫ì
    """
    from app.models.agent_task import AgentTreeNode
    from app.services.agent.core import agent_registry

    try:
        tree = agent_registry.get_agent_tree()
        nodes = tree.get("nodes", {})

        if not nodes:
            logger.warning(f"[SaveAgentTree] No agent nodes to save for task {task_id}")
            return

        logger.info(f"[SaveAgentTree] Saving {len(nodes)} agent nodes for task {task_id}")

        # ËÆ°ÁÆóÊØè‰∏™ËäÇÁÇπÁöÑÊ∑±Â∫¶
        def get_depth(agent_id: str, visited: set = None) -> int:
            if visited is None:
                visited = set()
            if agent_id in visited:
                return 0
            visited.add(agent_id)
            node = nodes.get(agent_id)
            if not node:
                return 0
            parent_id = node.get("parent_id")
            if not parent_id:
                return 0
            return 1 + get_depth(parent_id, visited)

        saved_count = 0
        for agent_id, node_data in nodes.items():
            # Ëé∑Âèñ Agent ÂÆû‰æãÁöÑÁªüËÆ°Êï∞ÊçÆ
            agent_instance = agent_registry.get_agent(agent_id)
            iterations = 0
            tool_calls = 0
            tokens_used = 0

            if agent_instance and hasattr(agent_instance, 'get_stats'):
                stats = agent_instance.get_stats()
                iterations = stats.get("iterations", 0)
                tool_calls = stats.get("tool_calls", 0)
                tokens_used = stats.get("tokens_used", 0)

            # ‰ªéÁªìÊûú‰∏≠Ëé∑ÂèñÂèëÁé∞Êï∞Èáè
            findings_count = 0
            result_summary = None
            if node_data.get("result"):
                result = node_data.get("result", {})
                if isinstance(result, dict):
                    findings_count = len(result.get("findings", []))
                    if result.get("summary"):
                        result_summary = str(result.get("summary"))[:2000]

            tree_node = AgentTreeNode(
                id=str(uuid4()),
                task_id=task_id,
                agent_id=agent_id,
                agent_name=node_data.get("name", "Unknown"),
                agent_type=node_data.get("type", "unknown"),
                parent_agent_id=node_data.get("parent_id"),
                depth=get_depth(agent_id),
                task_description=node_data.get("task"),
                knowledge_modules=node_data.get("knowledge_modules"),
                status=node_data.get("status", "unknown"),
                result_summary=result_summary,
                findings_count=findings_count,
                iterations=iterations,
                tool_calls=tool_calls,
                tokens_used=tokens_used,
            )
            db.add(tree_node)
            saved_count += 1

        await db.commit()
        logger.info(f"[SaveAgentTree] Successfully saved {saved_count} agent nodes to database")

    except Exception as e:
        logger.error(f"[SaveAgentTree] Failed to save agent tree: {e}", exc_info=True)
        await db.rollback()


# ============ API Endpoints ============

@router.post("/", response_model=AgentTaskResponse)
async def create_agent_task(
    request: AgentTaskCreate,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    ÂàõÂª∫Âπ∂ÂêØÂä® Agent ÂÆ°ËÆ°‰ªªÂä°
    """
    # È™åËØÅÈ°πÁõÆ
    project = await db.get(Project, request.project_id)
    if not project:
        raise HTTPException(status_code=404, detail="È°πÁõÆ‰∏çÂ≠òÂú®")
    
    if project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§È°πÁõÆ")
    
    # ÂàõÂª∫‰ªªÂä°
    task = AgentTask(
        id=str(uuid4()),
        project_id=project.id,
        name=request.name or f"Agent Audit - {datetime.now().strftime('%Y%m%d_%H%M%S')}",
        description=request.description,
        status=AgentTaskStatus.PENDING,
        current_phase=AgentTaskPhase.PLANNING,
        target_vulnerabilities=request.target_vulnerabilities,
        verification_level=request.verification_level or "sandbox",
        branch_name=request.branch_name,  # ‰øùÂ≠òÁî®Êà∑ÈÄâÊã©ÁöÑÂàÜÊîØ
        exclude_patterns=request.exclude_patterns,
        target_files=request.target_files,
        max_iterations=request.max_iterations or 50,
        timeout_seconds=request.timeout_seconds or 1800,
        created_by=current_user.id,
    )
    
    db.add(task)
    await db.commit()
    await db.refresh(task)
    
    # Âú®ÂêéÂè∞ÂêØÂä®‰ªªÂä°ÔºàÈ°πÁõÆÊ†πÁõÆÂΩïÂú®‰ªªÂä°ÂÜÖÈÉ®Ëé∑ÂèñÔºâ
    background_tasks.add_task(_execute_agent_task, task.id)
    
    logger.info(f"Created agent task {task.id} for project {project.name}")
    
    return task


@router.get("/", response_model=List[AgentTaskResponse])
async def list_agent_tasks(
    project_id: Optional[str] = None,
    status: Optional[str] = None,
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ Agent ‰ªªÂä°ÂàóË°®
    """
    # Ëé∑ÂèñÁî®Êà∑ÁöÑÈ°πÁõÆ
    projects_result = await db.execute(
        select(Project.id).where(Project.owner_id == current_user.id)
    )
    user_project_ids = [p[0] for p in projects_result.fetchall()]
    
    if not user_project_ids:
        return []
    
    # ÊûÑÂª∫Êü•ËØ¢
    query = select(AgentTask).where(AgentTask.project_id.in_(user_project_ids))
    
    if project_id:
        query = query.where(AgentTask.project_id == project_id)
    
    if status:
        try:
            status_enum = AgentTaskStatus(status)
            query = query.where(AgentTask.status == status_enum)
        except ValueError:
            pass
    
    query = query.order_by(AgentTask.created_at.desc())
    query = query.offset(skip).limit(limit)
    
    result = await db.execute(query)
    tasks = result.scalars().all()
    
    return tasks


@router.get("/{task_id}", response_model=AgentTaskResponse)
async def get_agent_task(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ Agent ‰ªªÂä°ËØ¶ÊÉÖ
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    # Ê£ÄÊü•ÊùÉÈôê
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    # ÊûÑÂª∫ÂìçÂ∫îÔºåÁ°Æ‰øùÊâÄÊúâÂ≠óÊÆµÈÉΩÂåÖÂê´
    try:
        # ËÆ°ÁÆóËøõÂ∫¶ÁôæÂàÜÊØî
        progress = 0.0
        if hasattr(task, 'progress_percentage'):
            progress = task.progress_percentage
        elif task.status == AgentTaskStatus.COMPLETED:
            progress = 100.0
        elif task.status in [AgentTaskStatus.FAILED, AgentTaskStatus.CANCELLED]:
            progress = 0.0
        
        # üî• ‰ªéËøêË°å‰∏≠ÁöÑ Orchestrator Ëé∑ÂèñÂÆûÊó∂ÁªüËÆ°
        total_iterations = task.total_iterations or 0
        tool_calls_count = task.tool_calls_count or 0
        tokens_used = task.tokens_used or 0
        
        orchestrator = _running_orchestrators.get(task_id)
        if orchestrator and task.status == AgentTaskStatus.RUNNING:
            # ‰ªé Orchestrator Ëé∑ÂèñÁªüËÆ°
            stats = orchestrator.get_stats()
            total_iterations = stats.get("iterations", 0)
            tool_calls_count = stats.get("tool_calls", 0)
            tokens_used = stats.get("tokens_used", 0)
            
            # Á¥ØÂä†Â≠ê Agent ÁöÑÁªüËÆ°
            if hasattr(orchestrator, 'sub_agents'):
                for agent in orchestrator.sub_agents.values():
                    if hasattr(agent, 'get_stats'):
                        sub_stats = agent.get_stats()
                        total_iterations += sub_stats.get("iterations", 0)
                        tool_calls_count += sub_stats.get("tool_calls", 0)
                        tokens_used += sub_stats.get("tokens_used", 0)
        
        # ÊâãÂä®ÊûÑÂª∫ÂìçÂ∫îÊï∞ÊçÆ
        response_data = {
            "id": task.id,
            "project_id": task.project_id,
            "name": task.name,
            "description": task.description,
            "task_type": task.task_type or "agent_audit",
            "status": task.status,
            "current_phase": task.current_phase,
            "current_step": task.current_step,
            "total_files": task.total_files or 0,
            "indexed_files": task.indexed_files or 0,
            "analyzed_files": task.analyzed_files or 0,
            "total_chunks": task.total_chunks or 0,
            "total_iterations": total_iterations,
            "tool_calls_count": tool_calls_count,
            "tokens_used": tokens_used,
            "findings_count": task.findings_count or 0,
            "total_findings": task.findings_count or 0,  # ÂÖºÂÆπÂ≠óÊÆµ
            "verified_count": task.verified_count or 0,
            "verified_findings": task.verified_count or 0,  # ÂÖºÂÆπÂ≠óÊÆµ
            "false_positive_count": task.false_positive_count or 0,
            "critical_count": task.critical_count or 0,
            "high_count": task.high_count or 0,
            "medium_count": task.medium_count or 0,
            "low_count": task.low_count or 0,
            "quality_score": float(task.quality_score or 0.0),
            "security_score": float(task.security_score) if task.security_score is not None else None,
            "progress_percentage": progress,
            "created_at": task.created_at,
            "started_at": task.started_at,
            "completed_at": task.completed_at,
            "error_message": task.error_message,
            "audit_scope": task.audit_scope,
            "target_vulnerabilities": task.target_vulnerabilities,
            "verification_level": task.verification_level,
            "exclude_patterns": task.exclude_patterns,
            "target_files": task.target_files,
        }
        
        return AgentTaskResponse(**response_data)
    except Exception as e:
        logger.error(f"Error serializing task {task_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Â∫èÂàóÂåñ‰ªªÂä°Êï∞ÊçÆÂ§±Ë¥•: {str(e)}")


@router.post("/{task_id}/cancel")
async def cancel_agent_task(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    ÂèñÊ∂à Agent ‰ªªÂä°
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")

    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉÊìç‰ΩúÊ≠§‰ªªÂä°")

    if task.status in [AgentTaskStatus.COMPLETED, AgentTaskStatus.FAILED, AgentTaskStatus.CANCELLED]:
        raise HTTPException(status_code=400, detail="‰ªªÂä°Â∑≤ÁªìÊùüÔºåÊó†Ê≥ïÂèñÊ∂à")

    # üî• 0. Á´ãÂç≥Ê†áËÆ∞‰ªªÂä°‰∏∫Â∑≤ÂèñÊ∂àÔºàÁî®‰∫éÂâçÁΩÆÊìç‰ΩúÁöÑÂèñÊ∂àÊ£ÄÊü•Ôºâ
    _cancelled_tasks.add(task_id)
    logger.info(f"[Cancel] Added task {task_id} to cancelled set")

    # üî• 1. ËÆæÁΩÆ Agent ÁöÑÂèñÊ∂àÊ†áÂøó
    runner = _running_tasks.get(task_id)
    if runner:
        runner.cancel()
        logger.info(f"[Cancel] Set cancel flag for task {task_id}")
    
    # üî• 2. Âº∫Âà∂ÂèñÊ∂à asyncio TaskÔºàÁ´ãÂç≥‰∏≠Êñ≠ LLM Ë∞ÉÁî®Ôºâ
    asyncio_task = _running_asyncio_tasks.get(task_id)
    if asyncio_task and not asyncio_task.done():
        asyncio_task.cancel()
        logger.info(f"[Cancel] Cancelled asyncio task for {task_id}")
    
    # Êõ¥Êñ∞Áä∂ÊÄÅ
    task.status = AgentTaskStatus.CANCELLED
    task.completed_at = datetime.now(timezone.utc)
    await db.commit()
    
    logger.info(f"[Cancel] Task {task_id} cancelled successfully")
    return {"message": "‰ªªÂä°Â∑≤ÂèñÊ∂à", "task_id": task_id}


@router.get("/{task_id}/events")
async def stream_agent_events(
    task_id: str,
    after_sequence: int = Query(0, ge=0, description="‰ªéÂì™‰∏™Â∫èÂè∑‰πãÂêéÂºÄÂßã"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
):
    """
    Ëé∑Âèñ Agent ‰∫ã‰ª∂ÊµÅ (SSE)
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    async def event_generator():
        """ÁîüÊàê SSE ‰∫ã‰ª∂ÊµÅ"""
        last_sequence = after_sequence
        poll_interval = 0.5
        max_idle = 300  # 5 ÂàÜÈíüÊó†‰∫ã‰ª∂ÂêéÂÖ≥Èó≠
        idle_time = 0
        
        while True:
            # Êü•ËØ¢Êñ∞‰∫ã‰ª∂
            async with async_session_factory() as session:
                result = await session.execute(
                    select(AgentEvent)
                    .where(AgentEvent.task_id == task_id)
                    .where(AgentEvent.sequence > last_sequence)
                    .order_by(AgentEvent.sequence)
                    .limit(50)
                )
                events = result.scalars().all()
                
                # Ëé∑Âèñ‰ªªÂä°Áä∂ÊÄÅ
                current_task = await session.get(AgentTask, task_id)
                task_status = current_task.status if current_task else None
            
            if events:
                idle_time = 0
                for event in events:
                    last_sequence = event.sequence
                    # event_type Â∑≤ÁªèÊòØÂ≠óÁ¨¶‰∏≤Ôºå‰∏çÈúÄË¶Å .value
                    event_type_str = str(event.event_type)
                    phase_str = str(event.phase) if event.phase else None
                    
                    data = {
                        "id": event.id,
                        "type": event_type_str,
                        "phase": phase_str,
                        "message": event.message,
                        "sequence": event.sequence,
                        "timestamp": event.created_at.isoformat() if event.created_at else None,
                        "progress_percent": event.progress_percent,
                        "tool_name": event.tool_name,
                    }
                    yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            else:
                idle_time += poll_interval
            
            # Ê£ÄÊü•‰ªªÂä°ÊòØÂê¶ÁªìÊùü
            if task_status:
                # task_status ÂèØËÉΩÊòØÂ≠óÁ¨¶‰∏≤ÊàñÊûö‰∏æÔºåÁªü‰∏ÄËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤
                status_str = str(task_status)
                if status_str in ["completed", "failed", "cancelled"]:
                    yield f"data: {json.dumps({'type': 'task_end', 'status': status_str})}\n\n"
                    break
            
            # Ê£ÄÊü•Á©∫Èó≤Ë∂ÖÊó∂
            if idle_time >= max_idle:
                yield f"data: {json.dumps({'type': 'timeout'})}\n\n"
                break
            
            await asyncio.sleep(poll_interval)
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.get("/{task_id}/stream")
async def stream_agent_with_thinking(
    task_id: str,
    include_thinking: bool = Query(True, description="ÊòØÂê¶ÂåÖÂê´ LLM ÊÄùËÄÉËøáÁ®ã"),
    include_tool_calls: bool = Query(True, description="ÊòØÂê¶ÂåÖÂê´Â∑•ÂÖ∑Ë∞ÉÁî®ËØ¶ÊÉÖ"),
    after_sequence: int = Query(0, ge=0, description="‰ªéÂì™‰∏™Â∫èÂè∑‰πãÂêéÂºÄÂßã"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
):
    """
    Â¢ûÂº∫Áâà‰∫ã‰ª∂ÊµÅ (SSE)
    
    ÊîØÊåÅ:
    - LLM ÊÄùËÄÉËøáÁ®ãÁöÑ Token Á∫ßÊµÅÂºèËæìÂá∫ (‰ªÖËøêË°åÊó∂)
    - Â∑•ÂÖ∑Ë∞ÉÁî®ÁöÑËØ¶ÁªÜËæìÂÖ•/ËæìÂá∫
    - ËäÇÁÇπÊâßË°åÁä∂ÊÄÅ
    - ÂèëÁé∞‰∫ã‰ª∂
    
    ‰ºòÂÖà‰ΩøÁî®ÂÜÖÂ≠ò‰∏≠ÁöÑ‰∫ã‰ª∂ÈòüÂàó (ÊîØÊåÅ thinking_token)Ôºå
    Â¶ÇÊûú‰ªªÂä°Êú™Âú®ËøêË°åÔºåÂàôÂõûÈÄÄÂà∞Êï∞ÊçÆÂ∫ìËΩÆËØ¢ (‰∏çÊîØÊåÅ thinking_token Â§çÁõò)„ÄÇ
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    # ÂÆö‰πâ SSE Ê†ºÂºèÂåñÂáΩÊï∞
    def format_sse_event(event_data: Dict[str, Any]) -> str:
        """Ê†ºÂºèÂåñ‰∏∫ SSE ‰∫ã‰ª∂"""
        event_type = event_data.get("event_type") or event_data.get("type")
        
        # Áªü‰∏ÄÂ≠óÊÆµ
        if "type" not in event_data:
            event_data["type"] = event_type
            
        return f"event: {event_type}\ndata: {json.dumps(event_data, ensure_ascii=False)}\n\n"

    async def enhanced_event_generator():
        """ÁîüÊàêÂ¢ûÂº∫Áâà SSE ‰∫ã‰ª∂ÊµÅ"""
        # 1. Ê£ÄÊü•‰ªªÂä°ÊòØÂê¶Âú®ËøêË°å‰∏≠ (ÂÜÖÂ≠ò)
        event_manager = _running_event_managers.get(task_id)
        
        if event_manager:
            logger.debug(f"Stream {task_id}: Using in-memory event manager")
            try:
                # ‰ΩøÁî® EventManager ÁöÑÊµÅÂºèÊé•Âè£
                # ËøáÊª§ÈÄâÈ°π
                skip_types = set()
                if not include_thinking:
                    skip_types.update(["thinking_start", "thinking_token", "thinking_end"])
                if not include_tool_calls:
                    skip_types.update(["tool_call_start", "tool_call_input", "tool_call_output", "tool_call_end"])
                
                async for event in event_manager.stream_events(task_id, after_sequence=after_sequence):
                    event_type = event.get("event_type")
                    
                    if event_type in skip_types:
                        continue
                    
                    # üî• Debug: ËÆ∞ÂΩï thinking_token ‰∫ã‰ª∂
                    if event_type == "thinking_token":
                        token = event.get("metadata", {}).get("token", "")[:20]
                        logger.debug(f"Stream {task_id}: Sending thinking_token: '{token}...'")
                        
                    # Ê†ºÂºèÂåñÂπ∂ yield
                    yield format_sse_event(event)
                    
                    # üî• CRITICAL: ‰∏∫ thinking_token Ê∑ªÂä†ÂæÆÂ∞èÂª∂Ëøü
                    # Á°Æ‰øù‰∫ã‰ª∂Âú®‰∏çÂêåÁöÑ TCP ÂåÖ‰∏≠ÂèëÈÄÅÔºåËÆ©ÂâçÁ´ØËÉΩÂ§üÈÄê‰∏™Â§ÑÁêÜ
                    # Ê≤°ÊúâËøô‰∏™Âª∂ËøüÔºåÊâÄÊúâ token ‰ºöÂú®‰∏ÄÊ¨° read() ‰∏≠Ë¢´Êé•Êî∂ÔºåÂØºËá¥ React ÊâπÈáèÊõ¥Êñ∞
                    if event_type == "thinking_token":
                        await asyncio.sleep(0.01)  # 10ms Âª∂Ëøü
                    
            except Exception as e:
                logger.error(f"In-memory stream error: {e}")
                err_data = {"type": "error", "message": str(e)}
                yield format_sse_event(err_data)
                
        else:
            logger.debug(f"Stream {task_id}: Task not running, falling back to DB polling")
            # 2. ÂõûÈÄÄÂà∞Êï∞ÊçÆÂ∫ìËΩÆËØ¢ (Êó†Ê≥ïËé∑Âèñ thinking_token)
            last_sequence = after_sequence
            poll_interval = 2.0  # ÂÆåÊàêÁöÑ‰ªªÂä°ËΩÆËØ¢ÂèØ‰ª•ÊÖ¢‰∏ÄÁÇπ
            heartbeat_interval = 15
            max_idle = 60  # 1ÂàÜÈíüÊó†‰∫ã‰ª∂ÂÖ≥Èó≠
            idle_time = 0
            last_heartbeat = 0
            
            skip_types = set()
            if not include_thinking:
                skip_types.update(["thinking_start", "thinking_token", "thinking_end"])
            
            while True:
                try:
                    async with async_session_factory() as session:
                        # Êü•ËØ¢Êñ∞‰∫ã‰ª∂
                        result = await session.execute(
                            select(AgentEvent)
                            .where(AgentEvent.task_id == task_id)
                            .where(AgentEvent.sequence > last_sequence)
                            .order_by(AgentEvent.sequence)
                            .limit(100)
                        )
                        events = result.scalars().all()
                        
                        # Ëé∑Âèñ‰ªªÂä°Áä∂ÊÄÅ
                        current_task = await session.get(AgentTask, task_id)
                        task_status = current_task.status if current_task else None
                    
                    if events:
                        idle_time = 0
                        for event in events:
                            last_sequence = event.sequence
                            event_type = str(event.event_type)
                            
                            if event_type in skip_types:
                                continue
                            
                            # ÊûÑÂª∫Êï∞ÊçÆ
                            data = {
                                "id": event.id,
                                "type": event_type,
                                "phase": str(event.phase) if event.phase else None,
                                "message": event.message,
                                "sequence": event.sequence,
                                "timestamp": event.created_at.isoformat() if event.created_at else None,
                            }
                            
                            # Ê∑ªÂä†ËØ¶ÊÉÖ
                            if include_tool_calls and event.tool_name:
                                data["tool"] = {
                                    "name": event.tool_name,
                                    "input": event.tool_input,
                                    "output": event.tool_output,
                                    "duration_ms": event.tool_duration_ms,
                                }
                                
                            if event.event_metadata:
                                data["metadata"] = event.event_metadata
                                
                            if event.tokens_used:
                                data["tokens_used"] = event.tokens_used
                            
                            yield format_sse_event(data)
                    else:
                        idle_time += poll_interval
                        
                        # Ê£ÄÊü•ÊòØÂê¶Â∫îËØ•ÁªìÊùü
                        if task_status:
                            status_str = str(task_status)
                            # Â¶ÇÊûú‰ªªÂä°Â∑≤ÂÆåÊàê‰∏îÊ≤°ÊúâÊñ∞‰∫ã‰ª∂ÔºåÁªìÊùüÊµÅ
                            if status_str in ["completed", "failed", "cancelled"]:
                                end_data = {
                                    "type": "task_end",
                                    "status": status_str,
                                    "message": f"‰ªªÂä°Â∑≤{status_str}"
                                }
                                yield format_sse_event(end_data)
                                break
                    
                    # ÂøÉË∑≥
                    last_heartbeat += poll_interval
                    if last_heartbeat >= heartbeat_interval:
                        last_heartbeat = 0
                        yield format_sse_event({"type": "heartbeat", "timestamp": datetime.now(timezone.utc).isoformat()})
                    
                    # Ë∂ÖÊó∂
                    if idle_time >= max_idle:
                        break
                    
                    await asyncio.sleep(poll_interval)
                    
                except Exception as e:
                    logger.error(f"DB poll stream error: {e}")
                    yield format_sse_event({"type": "error", "message": str(e)})
                    break
    
    return StreamingResponse(
        enhanced_event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Type": "text/event-stream; charset=utf-8",
        }
    )


@router.get("/{task_id}/events/list", response_model=List[AgentEventResponse])
async def list_agent_events(
    task_id: str,
    after_sequence: int = Query(0, ge=0),
    limit: int = Query(100, ge=1, le=500),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ Agent ‰∫ã‰ª∂ÂàóË°®
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")

    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")

    result = await db.execute(
        select(AgentEvent)
        .where(AgentEvent.task_id == task_id)
        .where(AgentEvent.sequence > after_sequence)
        .order_by(AgentEvent.sequence)
        .limit(limit)
    )
    events = result.scalars().all()

    # üî• Debug logging
    logger.debug(f"[EventsList] Task {task_id}: returning {len(events)} events (after_sequence={after_sequence})")
    if events:
        logger.debug(f"[EventsList] First event: type={events[0].event_type}, seq={events[0].sequence}")
        if len(events) > 1:
            logger.debug(f"[EventsList] Last event: type={events[-1].event_type}, seq={events[-1].sequence}")

    return events


@router.get("/{task_id}/findings", response_model=List[AgentFindingResponse])
async def list_agent_findings(
    task_id: str,
    severity: Optional[str] = None,
    verified_only: bool = False,
    skip: int = Query(0, ge=0),
    limit: int = Query(50, ge=1, le=200),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ Agent ÂèëÁé∞ÂàóË°®
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    query = select(AgentFinding).where(AgentFinding.task_id == task_id)
    
    if severity:
        try:
            sev_enum = VulnerabilitySeverity(severity)
            query = query.where(AgentFinding.severity == sev_enum)
        except ValueError:
            pass
    
    if verified_only:
        query = query.where(AgentFinding.is_verified == True)
    
    # Êåâ‰∏•ÈáçÁ®ãÂ∫¶ÊéíÂ∫è
    severity_order = {
        VulnerabilitySeverity.CRITICAL: 0,
        VulnerabilitySeverity.HIGH: 1,
        VulnerabilitySeverity.MEDIUM: 2,
        VulnerabilitySeverity.LOW: 3,
        VulnerabilitySeverity.INFO: 4,
    }
    
    query = query.order_by(AgentFinding.severity, AgentFinding.created_at.desc())
    query = query.offset(skip).limit(limit)
    
    result = await db.execute(query)
    findings = result.scalars().all()
    
    return findings


@router.get("/{task_id}/summary", response_model=TaskSummaryResponse)
async def get_task_summary(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ‰ªªÂä°ÊëòË¶Å
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    # Ëé∑ÂèñÊâÄÊúâÂèëÁé∞
    result = await db.execute(
        select(AgentFinding).where(AgentFinding.task_id == task_id)
    )
    findings = result.scalars().all()
    
    # ÁªüËÆ°
    severity_distribution = {}
    vulnerability_types = {}
    verified_count = 0
    
    for f in findings:
        # severity Âíå vulnerability_type Â∑≤ÁªèÊòØÂ≠óÁ¨¶‰∏≤
        sev = str(f.severity)
        vtype = str(f.vulnerability_type)
        
        severity_distribution[sev] = severity_distribution.get(sev, 0) + 1
        vulnerability_types[vtype] = vulnerability_types.get(vtype, 0) + 1
        
        if f.is_verified:
            verified_count += 1
    
    # ËÆ°ÁÆóÊåÅÁª≠Êó∂Èó¥
    duration = None
    if task.started_at and task.completed_at:
        duration = int((task.completed_at - task.started_at).total_seconds())
    
    # Ëé∑ÂèñÂ∑≤ÂÆåÊàêÁöÑÈò∂ÊÆµ
    phases_result = await db.execute(
        select(AgentEvent.phase)
        .where(AgentEvent.task_id == task_id)
        .where(AgentEvent.event_type == AgentEventType.PHASE_COMPLETE)
        .distinct()
    )
    phases = [str(p[0]) for p in phases_result.fetchall() if p[0]]
    
    return TaskSummaryResponse(
        task_id=task_id,
        status=str(task.status),  # status Â∑≤ÁªèÊòØÂ≠óÁ¨¶‰∏≤
        security_score=task.security_score,
        total_findings=len(findings),
        verified_findings=verified_count,
        severity_distribution=severity_distribution,
        vulnerability_types=vulnerability_types,
        duration_seconds=duration,
        phases_completed=phases,
    )


@router.patch("/{task_id}/findings/{finding_id}/status")
async def update_finding_status(
    task_id: str,
    finding_id: str,
    status: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Êõ¥Êñ∞ÂèëÁé∞Áä∂ÊÄÅ
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉÊìç‰Ωú")
    
    finding = await db.get(AgentFinding, finding_id)
    if not finding or finding.task_id != task_id:
        raise HTTPException(status_code=404, detail="ÂèëÁé∞‰∏çÂ≠òÂú®")
    
    try:
        finding.status = FindingStatus(status)
    except ValueError:
        raise HTTPException(status_code=400, detail=f"Êó†ÊïàÁöÑÁä∂ÊÄÅ: {status}")
    
    await db.commit()
    
    return {"message": "Áä∂ÊÄÅÂ∑≤Êõ¥Êñ∞", "finding_id": finding_id, "status": status}


# ============ Helper Functions ============

async def _get_project_root(
    project: Project,
    task_id: str,
    branch_name: Optional[str] = None,
    github_token: Optional[str] = None,
    gitlab_token: Optional[str] = None,
    event_emitter: Optional[Any] = None,  # üî• Êñ∞Â¢ûÔºöÁî®‰∫éÂèëÈÄÅÂÆûÊó∂Êó•Âøó
) -> str:
    """
    Ëé∑ÂèñÈ°πÁõÆÊ†πÁõÆÂΩï

    ÊîØÊåÅ‰∏§ÁßçÈ°πÁõÆÁ±ªÂûãÔºö
    - ZIP È°πÁõÆÔºöËß£Âéã ZIP Êñá‰ª∂Âà∞‰∏¥Êó∂ÁõÆÂΩï
    - ‰ªìÂ∫ìÈ°πÁõÆÔºöÂÖãÈöÜ‰ªìÂ∫ìÂà∞‰∏¥Êó∂ÁõÆÂΩï

    Args:
        project: È°πÁõÆÂØπË±°
        task_id: ‰ªªÂä°ID
        branch_name: ÂàÜÊîØÂêçÁß∞Ôºà‰ªìÂ∫ìÈ°πÁõÆ‰ΩøÁî®Ôºå‰ºòÂÖà‰∫é project.default_branchÔºâ
        github_token: GitHub ËÆøÈóÆ‰ª§ÁâåÔºàÁî®‰∫éÁßÅÊúâ‰ªìÂ∫ìÔºâ
        gitlab_token: GitLab ËÆøÈóÆ‰ª§ÁâåÔºàÁî®‰∫éÁßÅÊúâ‰ªìÂ∫ìÔºâ
        event_emitter: ‰∫ã‰ª∂ÂèëÈÄÅÂô®ÔºàÁî®‰∫éÂèëÈÄÅÂÆûÊó∂Êó•ÂøóÔºâ

    Returns:
        È°πÁõÆÊ†πÁõÆÂΩïË∑ØÂæÑ

    Raises:
        RuntimeError: ÂΩìÈ°πÁõÆÊñá‰ª∂Ëé∑ÂèñÂ§±Ë¥•Êó∂
    """
    import zipfile
    import subprocess
    import shutil
    from urllib.parse import urlparse, urlunparse

    # ËæÖÂä©ÂáΩÊï∞ÔºöÂèëÈÄÅ‰∫ã‰ª∂
    async def emit(message: str, level: str = "info"):
        if event_emitter:
            if level == "info":
                await event_emitter.emit_info(message)
            elif level == "warning":
                await event_emitter.emit_warning(message)
            elif level == "error":
                await event_emitter.emit_error(message)

    # üî• ËæÖÂä©ÂáΩÊï∞ÔºöÊ£ÄÊü•ÂèñÊ∂àÁä∂ÊÄÅ
    def check_cancelled():
        if is_task_cancelled(task_id):
            raise asyncio.CancelledError("‰ªªÂä°Â∑≤ÂèñÊ∂à")

    base_path = f"/tmp/deepaudit/{task_id}"

    # Á°Æ‰øùÁõÆÂΩïÂ≠òÂú®‰∏î‰∏∫Á©∫
    if os.path.exists(base_path):
        shutil.rmtree(base_path)
    os.makedirs(base_path, exist_ok=True)

    # üî• Âú®ÂºÄÂßã‰ªª‰ΩïÊìç‰ΩúÂâçÊ£ÄÊü•ÂèñÊ∂à
    check_cancelled()

    # Ê†πÊçÆÈ°πÁõÆÁ±ªÂûãÂ§ÑÁêÜ
    if project.source_type == "zip":
        # üî• ZIP È°πÁõÆÔºöËß£Âéã ZIP Êñá‰ª∂
        check_cancelled()  # üî• Ëß£ÂéãÂâçÊ£ÄÊü•
        await emit(f"üì¶ Ê≠£Âú®Ëß£ÂéãÈ°πÁõÆÊñá‰ª∂...")
        from app.services.zip_storage import load_project_zip

        zip_path = await load_project_zip(project.id)

        if zip_path and os.path.exists(zip_path):
            try:
                check_cancelled()  # üî• Ëß£ÂéãÂâçÂÜçÊ¨°Ê£ÄÊü•
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    # üî• ÈÄê‰∏™Êñá‰ª∂Ëß£ÂéãÔºåÊîØÊåÅÂèñÊ∂àÊ£ÄÊü•
                    file_list = zip_ref.namelist()
                    for i, file_name in enumerate(file_list):
                        if i % 50 == 0:  # ÊØè50‰∏™Êñá‰ª∂Ê£ÄÊü•‰∏ÄÊ¨°
                            check_cancelled()
                        zip_ref.extract(file_name, base_path)
                logger.info(f"‚úÖ Extracted ZIP project {project.id} to {base_path}")
                await emit(f"‚úÖ ZIP Êñá‰ª∂Ëß£ÂéãÂÆåÊàê")
            except Exception as e:
                logger.error(f"Failed to extract ZIP {zip_path}: {e}")
                await emit(f"‚ùå Ëß£ÂéãÂ§±Ë¥•: {e}", "error")
                raise RuntimeError(f"Êó†Ê≥ïËß£ÂéãÈ°πÁõÆÊñá‰ª∂: {e}")
        else:
            logger.warning(f"‚ö†Ô∏è ZIP file not found for project {project.id}")
            await emit(f"‚ùå ZIP Êñá‰ª∂‰∏çÂ≠òÂú®", "error")
            raise RuntimeError(f"È°πÁõÆ ZIP Êñá‰ª∂‰∏çÂ≠òÂú®: {project.id}")

    elif project.source_type == "repository" and project.repository_url:
        # üî• ‰ªìÂ∫ìÈ°πÁõÆÔºö‰ºòÂÖà‰ΩøÁî® ZIP ‰∏ãËΩΩÔºàÊõ¥Âø´Êõ¥Á®≥ÂÆöÔºâÔºågit clone ‰Ωú‰∏∫ÂõûÈÄÄ
        repo_url = project.repository_url
        repo_type = project.repository_type or "other"

        await emit(f"üîÑ Ê≠£Âú®Ëé∑Âèñ‰ªìÂ∫ì: {repo_url}")

        # Ëß£Êûê‰ªìÂ∫ì URL Ëé∑Âèñ owner/repo
        parsed = urlparse(repo_url)
        path_parts = parsed.path.strip('/').replace('.git', '').split('/')
        if len(path_parts) >= 2:
            owner, repo = path_parts[0], path_parts[1]
        else:
            owner, repo = None, None

        # ÊûÑÂª∫ÂàÜÊîØÂ∞ùËØïÈ°∫Â∫è
        branches_to_try = []
        if branch_name:
            branches_to_try.append(branch_name)
        if project.default_branch and project.default_branch not in branches_to_try:
            branches_to_try.append(project.default_branch)
        for common_branch in ["main", "master"]:
            if common_branch not in branches_to_try:
                branches_to_try.append(common_branch)

        download_success = False
        last_error = ""

        # ============ ÊñπÊ°à1: ‰ºòÂÖà‰ΩøÁî® ZIP ‰∏ãËΩΩÔºàÊõ¥Âø´Êõ¥Á®≥ÂÆöÔºâ============
        if owner and repo:
            import httpx

            for branch in branches_to_try:
                check_cancelled()

                # Ê∏ÖÁêÜÁõÆÂΩï
                if os.path.exists(base_path) and os.listdir(base_path):
                    shutil.rmtree(base_path)
                os.makedirs(base_path, exist_ok=True)

                # ÊûÑÂª∫ ZIP ‰∏ãËΩΩ URL
                if repo_type == "github" or "github.com" in repo_url:
                    # GitHub ZIP ‰∏ãËΩΩ URL
                    zip_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/{branch}.zip"
                    headers = {}
                    if github_token:
                        headers["Authorization"] = f"token {github_token}"
                elif repo_type == "gitlab" or "gitlab" in repo_url:
                    # GitLab ZIP ‰∏ãËΩΩ URLÔºàÈúÄË¶ÅÂØπ owner/repo ËøõË°å URL ÁºñÁ†ÅÔºâ
                    import urllib.parse
                    project_path = urllib.parse.quote(f"{owner}/{repo}", safe='')
                    gitlab_host = parsed.netloc
                    zip_url = f"https://{gitlab_host}/api/v4/projects/{project_path}/repository/archive.zip?sha={branch}"
                    headers = {}
                    if gitlab_token:
                        headers["PRIVATE-TOKEN"] = gitlab_token
                else:
                    # ÂÖ∂‰ªñÂπ≥Âè∞ÔºåË∑≥Ëøá ZIP ‰∏ãËΩΩ
                    break

                logger.info(f"üì¶ Â∞ùËØï‰∏ãËΩΩ ZIP ÂΩíÊ°£ (ÂàÜÊîØ: {branch})...")
                await emit(f"üì¶ Â∞ùËØï‰∏ãËΩΩ ZIP ÂΩíÊ°£ (ÂàÜÊîØ: {branch})")

                try:
                    zip_temp_path = f"/tmp/repo_{task_id}_{branch}.zip"

                    async def download_zip():
                        async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:
                            resp = await client.get(zip_url, headers=headers)
                            if resp.status_code == 200:
                                with open(zip_temp_path, 'wb') as f:
                                    f.write(resp.content)
                                return True, None
                            else:
                                return False, f"HTTP {resp.status_code}"

                    # ‰ΩøÁî®ÂèñÊ∂àÊ£ÄÊü•Âæ™ÁéØ
                    download_task = asyncio.create_task(download_zip())
                    while not download_task.done():
                        check_cancelled()
                        try:
                            success, error = await asyncio.wait_for(asyncio.shield(download_task), timeout=1.0)
                            break
                        except asyncio.TimeoutError:
                            continue

                    if download_task.done():
                        success, error = download_task.result()

                    if success and os.path.exists(zip_temp_path):
                        # Ëß£Âéã ZIP
                        check_cancelled()
                        with zipfile.ZipFile(zip_temp_path, 'r') as zip_ref:
                            # ZIP ÂÜÖÈÄöÂ∏∏Êúâ‰∏Ä‰∏™Ê†πÁõÆÂΩïÂ¶Ç repo-branch/
                            file_list = zip_ref.namelist()
                            # ÊâæÂà∞ÂÖ¨ÂÖ±ÂâçÁºÄ
                            if file_list:
                                common_prefix = file_list[0].split('/')[0] + '/'
                                for i, file_name in enumerate(file_list):
                                    if i % 50 == 0:
                                        check_cancelled()
                                    # ÂéªÊéâÂÖ¨ÂÖ±ÂâçÁºÄ
                                    if file_name.startswith(common_prefix):
                                        target_path = file_name[len(common_prefix):]
                                        if target_path:  # Ë∑≥ËøáÁ©∫Ë∑ØÂæÑÔºàÊ†πÁõÆÂΩïÊú¨Ë∫´Ôºâ
                                            full_target = os.path.join(base_path, target_path)
                                            if file_name.endswith('/'):
                                                os.makedirs(full_target, exist_ok=True)
                                            else:
                                                os.makedirs(os.path.dirname(full_target), exist_ok=True)
                                                with zip_ref.open(file_name) as src, open(full_target, 'wb') as dst:
                                                    dst.write(src.read())

                        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
                        os.remove(zip_temp_path)
                        logger.info(f"‚úÖ ZIP ‰∏ãËΩΩÊàêÂäü (ÂàÜÊîØ: {branch})")
                        await emit(f"‚úÖ ‰ªìÂ∫ìËé∑ÂèñÊàêÂäü (ZIP‰∏ãËΩΩ, ÂàÜÊîØ: {branch})")
                        download_success = True
                        break
                    else:
                        last_error = error or "‰∏ãËΩΩÂ§±Ë¥•"
                        logger.warning(f"ZIP ‰∏ãËΩΩÂ§±Ë¥• (ÂàÜÊîØ {branch}): {last_error}")
                        await emit(f"‚ö†Ô∏è ZIP ‰∏ãËΩΩÂ§±Ë¥•ÔºåÂ∞ùËØïÂÖ∂‰ªñÂàÜÊîØ...", "warning")
                        # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂
                        if os.path.exists(zip_temp_path):
                            os.remove(zip_temp_path)

                except asyncio.CancelledError:
                    logger.info(f"[Cancel] ZIP download cancelled for task {task_id}")
                    raise
                except Exception as e:
                    last_error = str(e)
                    logger.warning(f"ZIP ‰∏ãËΩΩÂºÇÂ∏∏ (ÂàÜÊîØ {branch}): {e}")
                    await emit(f"‚ö†Ô∏è ZIP ‰∏ãËΩΩÂºÇÂ∏∏: {str(e)[:50]}...", "warning")

        # ============ ÊñπÊ°à2: ÂõûÈÄÄÂà∞ git clone ============
        if not download_success:
            await emit(f"üîÑ ZIP ‰∏ãËΩΩÂ§±Ë¥•ÔºåÂõûÈÄÄÂà∞ Git ÂÖãÈöÜ...")
            logger.info("ZIP download failed, falling back to git clone")

            # Ê£ÄÊü• git ÊòØÂê¶ÂèØÁî®
            try:
                git_check = subprocess.run(
                    ["git", "--version"],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if git_check.returncode != 0:
                    await emit(f"‚ùå Git Êú™ÂÆâË£Ö", "error")
                    raise RuntimeError("Git Êú™ÂÆâË£ÖÔºåÊó†Ê≥ïÂÖãÈöÜ‰ªìÂ∫ì„ÄÇ")
            except FileNotFoundError:
                await emit(f"‚ùå Git Êú™ÂÆâË£Ö", "error")
                raise RuntimeError("Git Êú™ÂÆâË£ÖÔºåÊó†Ê≥ïÂÖãÈöÜ‰ªìÂ∫ì„ÄÇ")
            except subprocess.TimeoutExpired:
                await emit(f"‚ùå Git Ê£ÄÊµãË∂ÖÊó∂", "error")
                raise RuntimeError("Git Ê£ÄÊµãË∂ÖÊó∂")

            # ÊûÑÂª∫Â∏¶ËÆ§ËØÅÁöÑ URL
            auth_url = repo_url
            if repo_type == "github" and github_token:
                auth_url = urlunparse((
                    parsed.scheme,
                    f"{github_token}@{parsed.netloc}",
                    parsed.path,
                    parsed.params,
                    parsed.query,
                    parsed.fragment
                ))
                await emit(f"üîê ‰ΩøÁî® GitHub Token ËÆ§ËØÅ")
            elif repo_type == "gitlab" and gitlab_token:
                auth_url = urlunparse((
                    parsed.scheme,
                    f"oauth2:{gitlab_token}@{parsed.netloc}",
                    parsed.path,
                    parsed.params,
                    parsed.query,
                    parsed.fragment
                ))
                await emit(f"üîê ‰ΩøÁî® GitLab Token ËÆ§ËØÅ")

            for branch in branches_to_try:
                check_cancelled()

                if os.path.exists(base_path) and os.listdir(base_path):
                    shutil.rmtree(base_path)
                    os.makedirs(base_path, exist_ok=True)

                logger.info(f"üîÑ Â∞ùËØïÂÖãÈöÜÂàÜÊîØ: {branch}")
                await emit(f"üîÑ Â∞ùËØïÂÖãÈöÜÂàÜÊîØ: {branch}")

                try:
                    async def run_clone():
                        return await asyncio.to_thread(
                            subprocess.run,
                            ["git", "clone", "--depth", "1", "--branch", branch, auth_url, base_path],
                            capture_output=True,
                            text=True,
                            timeout=120,
                        )

                    clone_task = asyncio.create_task(run_clone())
                    while not clone_task.done():
                        check_cancelled()
                        try:
                            result = await asyncio.wait_for(asyncio.shield(clone_task), timeout=1.0)
                            break
                        except asyncio.TimeoutError:
                            continue

                    if clone_task.done():
                        result = clone_task.result()

                    if result.returncode == 0:
                        logger.info(f"‚úÖ Git ÂÖãÈöÜÊàêÂäü (ÂàÜÊîØ: {branch})")
                        await emit(f"‚úÖ ‰ªìÂ∫ìËé∑ÂèñÊàêÂäü (GitÂÖãÈöÜ, ÂàÜÊîØ: {branch})")
                        download_success = True
                        break
                    else:
                        last_error = result.stderr
                        logger.warning(f"ÂÖãÈöÜÂ§±Ë¥• (ÂàÜÊîØ {branch}): {last_error[:200]}")
                        await emit(f"‚ö†Ô∏è ÂàÜÊîØ {branch} ÂÖãÈöÜÂ§±Ë¥•...", "warning")
                except subprocess.TimeoutExpired:
                    last_error = f"ÂÖãÈöÜÂàÜÊîØ {branch} Ë∂ÖÊó∂"
                    logger.warning(last_error)
                    await emit(f"‚ö†Ô∏è ÂàÜÊîØ {branch} ÂÖãÈöÜË∂ÖÊó∂...", "warning")
                except asyncio.CancelledError:
                    logger.info(f"[Cancel] Git clone cancelled for task {task_id}")
                    raise

            # Â∞ùËØïÈªòËÆ§ÂàÜÊîØ
            if not download_success:
                check_cancelled()
                await emit(f"üîÑ Â∞ùËØï‰ΩøÁî®‰ªìÂ∫ìÈªòËÆ§ÂàÜÊîØ...")

                if os.path.exists(base_path) and os.listdir(base_path):
                    shutil.rmtree(base_path)
                    os.makedirs(base_path, exist_ok=True)

                try:
                    async def run_default_clone():
                        return await asyncio.to_thread(
                            subprocess.run,
                            ["git", "clone", "--depth", "1", auth_url, base_path],
                            capture_output=True,
                            text=True,
                            timeout=120,
                        )

                    clone_task = asyncio.create_task(run_default_clone())
                    while not clone_task.done():
                        check_cancelled()
                        try:
                            result = await asyncio.wait_for(asyncio.shield(clone_task), timeout=1.0)
                            break
                        except asyncio.TimeoutError:
                            continue

                    if clone_task.done():
                        result = clone_task.result()

                    if result.returncode == 0:
                        logger.info(f"‚úÖ Git ÂÖãÈöÜÊàêÂäü (ÈªòËÆ§ÂàÜÊîØ)")
                        await emit(f"‚úÖ ‰ªìÂ∫ìËé∑ÂèñÊàêÂäü (GitÂÖãÈöÜ, ÈªòËÆ§ÂàÜÊîØ)")
                        download_success = True
                    else:
                        last_error = result.stderr
                except subprocess.TimeoutExpired:
                    last_error = "ÂÖãÈöÜË∂ÖÊó∂"
                except asyncio.CancelledError:
                    logger.info(f"[Cancel] Git clone cancelled for task {task_id}")
                    raise

        if not download_success:
            # ÂàÜÊûêÈîôËØØÂéüÂõ†
            error_msg = "ÂÖãÈöÜ‰ªìÂ∫ìÂ§±Ë¥•"
            if "Authentication failed" in last_error or "401" in last_error:
                error_msg = "ËÆ§ËØÅÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü• GitHub/GitLab Token ÈÖçÁΩÆ"
            elif "not found" in last_error.lower() or "404" in last_error:
                error_msg = "‰ªìÂ∫ì‰∏çÂ≠òÂú®ÊàñÊó†ËÆøÈóÆÊùÉÈôê"
            elif "Could not resolve host" in last_error:
                error_msg = "Êó†Ê≥ïËß£Êûê‰∏ªÊú∫ÂêçÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•"
            elif "Permission denied" in last_error or "403" in last_error:
                error_msg = "Êó†ËÆøÈóÆÊùÉÈôêÔºåËØ∑Ê£ÄÊü•‰ªìÂ∫ìÊùÉÈôêÊàñ Token"
            else:
                error_msg = f"ÂÖãÈöÜ‰ªìÂ∫ìÂ§±Ë¥•: {last_error[:200]}"

            logger.error(f"‚ùå {error_msg}")
            await emit(f"‚ùå {error_msg}", "error")
            raise RuntimeError(error_msg)

    # È™åËØÅÁõÆÂΩï‰∏ç‰∏∫Á©∫
    if not os.listdir(base_path):
        await emit(f"‚ùå È°πÁõÆÁõÆÂΩï‰∏∫Á©∫", "error")
        raise RuntimeError(f"È°πÁõÆÁõÆÂΩï‰∏∫Á©∫ÔºåÂèØËÉΩÊòØÂÖãÈöÜ/Ëß£ÂéãÂ§±Ë¥•: {base_path}")

    await emit(f"üìÅ È°πÁõÆÂáÜÂ§áÂÆåÊàê: {base_path}")
    return base_path


# ============ Agent Tree API ============

class AgentTreeNodeResponse(BaseModel):
    """Agent Ê†ëËäÇÁÇπÂìçÂ∫î"""
    id: str
    agent_id: str
    agent_name: str
    agent_type: str
    parent_agent_id: Optional[str] = None
    depth: int = 0
    task_description: Optional[str] = None
    knowledge_modules: Optional[List[str]] = None
    status: str = "created"
    result_summary: Optional[str] = None
    findings_count: int = 0
    iterations: int = 0
    tokens_used: int = 0
    tool_calls: int = 0
    duration_ms: Optional[int] = None
    children: List["AgentTreeNodeResponse"] = []
    
    class Config:
        from_attributes = True


class AgentTreeResponse(BaseModel):
    """Agent Ê†ëÂìçÂ∫î"""
    task_id: str
    root_agent_id: Optional[str] = None
    total_agents: int = 0
    running_agents: int = 0
    completed_agents: int = 0
    failed_agents: int = 0
    total_findings: int = 0
    nodes: List[AgentTreeNodeResponse] = []


@router.get("/{task_id}/agent-tree", response_model=AgentTreeResponse)
async def get_agent_tree(
    task_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ‰ªªÂä°ÁöÑ Agent Ê†ëÁªìÊûÑ
    
    ËøîÂõûÂä®ÊÄÅ Agent Ê†ëÁöÑÂÆåÊï¥ÁªìÊûÑÔºåÂåÖÊã¨Ôºö
    - ÊâÄÊúâ Agent ËäÇÁÇπ
    - Áà∂Â≠êÂÖ≥Á≥ª
    - ÊâßË°åÁä∂ÊÄÅ
    - ÂèëÁé∞ÁªüËÆ°
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    # Â∞ùËØï‰ªéÂÜÖÂ≠ò‰∏≠Ëé∑Âèñ Agent Ê†ëÔºàËøêË°å‰∏≠ÁöÑ‰ªªÂä°Ôºâ
    runner = _running_tasks.get(task_id)
    logger.debug(f"[AgentTree API] task_id={task_id}, runner exists={runner is not None}")
    
    if runner:
        from app.services.agent.core import agent_registry
        
        tree = agent_registry.get_agent_tree()
        stats = agent_registry.get_statistics()
        logger.debug(f"[AgentTree API] tree nodes={len(tree.get('nodes', {}))}, root={tree.get('root_agent_id')}")
        logger.debug(f"[AgentTree API] ËäÇÁÇπËØ¶ÊÉÖ: {list(tree.get('nodes', {}).keys())}")
        
        # üî• Ëé∑Âèñ root agent IDÔºåÁî®‰∫éÂà§Êñ≠ÊòØÂê¶ÊòØ Orchestrator
        root_agent_id = tree.get("root_agent_id")
        
        # ÊûÑÂª∫ËäÇÁÇπÂàóË°®
        nodes = []
        for agent_id, node_data in tree.get("nodes", {}).items():
            # üî• ‰ªé Agent ÂÆû‰æãËé∑ÂèñÂÆûÊó∂ÁªüËÆ°Êï∞ÊçÆ
            iterations = 0
            tool_calls = 0
            tokens_used = 0
            findings_count = 0
            
            agent_instance = agent_registry.get_agent(agent_id)
            if agent_instance and hasattr(agent_instance, 'get_stats'):
                agent_stats = agent_instance.get_stats()
                iterations = agent_stats.get("iterations", 0)
                tool_calls = agent_stats.get("tool_calls", 0)
                tokens_used = agent_stats.get("tokens_used", 0)
            
            # üî• FIX: ÂØπ‰∫é Orchestrator (root agent)Ôºå‰ΩøÁî® task ÁöÑ findings_count
            # ËøôÁ°Æ‰øù‰∫ÜÊ≠£Á°ÆÊòæÁ§∫ËÅöÂêàÁöÑ findings ÊÄªÊï∞
            if agent_id == root_agent_id:
                findings_count = task.findings_count or 0
            else:
                # ‰ªéÁªìÊûú‰∏≠Ëé∑ÂèñÂèëÁé∞Êï∞ÈáèÔºàÂØπ‰∫éÂ≠ê agentÔºâ
                if node_data.get("result"):
                    result = node_data.get("result", {})
                    findings_count = len(result.get("findings", []))
            
            nodes.append(AgentTreeNodeResponse(
                id=node_data.get("id", agent_id),
                agent_id=agent_id,
                agent_name=node_data.get("name", "Unknown"),
                agent_type=node_data.get("type", "unknown"),
                parent_agent_id=node_data.get("parent_id"),
                task_description=node_data.get("task"),
                knowledge_modules=node_data.get("knowledge_modules", []),
                status=node_data.get("status", "unknown"),
                findings_count=findings_count,
                iterations=iterations,
                tool_calls=tool_calls,
                tokens_used=tokens_used,
                children=[],
            ))
        
        # üî• ‰ΩøÁî® task.findings_count ‰Ωú‰∏∫ total_findingsÔºåÁ°Æ‰øù‰∏ÄËá¥ÊÄß
        return AgentTreeResponse(
            task_id=task_id,
            root_agent_id=root_agent_id,
            total_agents=stats.get("total", 0),
            running_agents=stats.get("running", 0),
            completed_agents=stats.get("completed", 0),
            failed_agents=stats.get("failed", 0),
            total_findings=task.findings_count or 0,
            nodes=nodes,
        )
    
    # ‰ªéÊï∞ÊçÆÂ∫ìËé∑ÂèñÔºàÂ∑≤ÂÆåÊàêÁöÑ‰ªªÂä°Ôºâ
    from app.models.agent_task import AgentTreeNode
    
    result = await db.execute(
        select(AgentTreeNode)
        .where(AgentTreeNode.task_id == task_id)
        .order_by(AgentTreeNode.depth, AgentTreeNode.created_at)
    )
    db_nodes = result.scalars().all()
    
    if not db_nodes:
        return AgentTreeResponse(
            task_id=task_id,
            nodes=[],
        )
    
    # ÊûÑÂª∫ÂìçÂ∫î
    nodes = []
    root_id = None
    running = 0
    completed = 0
    failed = 0
    
    for node in db_nodes:
        if node.parent_agent_id is None:
            root_id = node.agent_id
        
        if node.status == "running":
            running += 1
        elif node.status == "completed":
            completed += 1
        elif node.status == "failed":
            failed += 1
        
        # üî• FIX: ÂØπ‰∫é Orchestrator (root agent)Ôºå‰ΩøÁî® task ÁöÑ findings_count
        # ËøôÁ°Æ‰øù‰∫ÜÊ≠£Á°ÆÊòæÁ§∫ËÅöÂêàÁöÑ findings ÊÄªÊï∞
        if node.parent_agent_id is None:
            # Root agent uses task's total findings
            node_findings_count = task.findings_count or 0
        else:
            node_findings_count = node.findings_count or 0
        
        nodes.append(AgentTreeNodeResponse(
            id=node.id,
            agent_id=node.agent_id,
            agent_name=node.agent_name,
            agent_type=node.agent_type,
            parent_agent_id=node.parent_agent_id,
            depth=node.depth,
            task_description=node.task_description,
            knowledge_modules=node.knowledge_modules,
            status=node.status,
            result_summary=node.result_summary,
            findings_count=node_findings_count,
            iterations=node.iterations or 0,
            tokens_used=node.tokens_used or 0,
            tool_calls=node.tool_calls or 0,
            duration_ms=node.duration_ms,
            children=[],
        ))
    
    # üî• ‰ΩøÁî® task.findings_count ‰Ωú‰∏∫ total_findingsÔºåÁ°Æ‰øù‰∏ÄËá¥ÊÄß
    return AgentTreeResponse(
        task_id=task_id,
        root_agent_id=root_id,
        total_agents=len(nodes),
        running_agents=running,
        completed_agents=completed,
        failed_agents=failed,
        total_findings=task.findings_count or 0,
        nodes=nodes,
    )


# ============ Checkpoint API ============

class CheckpointResponse(BaseModel):
    """Ê£ÄÊü•ÁÇπÂìçÂ∫î"""
    id: str
    agent_id: str
    agent_name: str
    agent_type: str
    iteration: int
    status: str
    total_tokens: int = 0
    tool_calls: int = 0
    findings_count: int = 0
    checkpoint_type: str = "auto"
    checkpoint_name: Optional[str] = None
    created_at: Optional[str] = None
    
    class Config:
        from_attributes = True


@router.get("/{task_id}/checkpoints", response_model=List[CheckpointResponse])
async def list_checkpoints(
    task_id: str,
    agent_id: Optional[str] = None,
    limit: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑Âèñ‰ªªÂä°ÁöÑÊ£ÄÊü•ÁÇπÂàóË°®
    
    Áî®‰∫éÔºö
    - Êü•ÁúãÊâßË°åÂéÜÂè≤
    - Áä∂ÊÄÅÊÅ¢Â§ç
    - Ë∞ÉËØïÂàÜÊûê
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    from app.models.agent_task import AgentCheckpoint
    
    query = select(AgentCheckpoint).where(AgentCheckpoint.task_id == task_id)
    
    if agent_id:
        query = query.where(AgentCheckpoint.agent_id == agent_id)
    
    query = query.order_by(AgentCheckpoint.created_at.desc()).limit(limit)
    
    result = await db.execute(query)
    checkpoints = result.scalars().all()
    
    return [
        CheckpointResponse(
            id=cp.id,
            agent_id=cp.agent_id,
            agent_name=cp.agent_name,
            agent_type=cp.agent_type,
            iteration=cp.iteration,
            status=cp.status,
            total_tokens=cp.total_tokens or 0,
            tool_calls=cp.tool_calls or 0,
            findings_count=cp.findings_count or 0,
            checkpoint_type=cp.checkpoint_type or "auto",
            checkpoint_name=cp.checkpoint_name,
            created_at=cp.created_at.isoformat() if cp.created_at else None,
        )
        for cp in checkpoints
    ]


@router.get("/{task_id}/checkpoints/{checkpoint_id}")
async def get_checkpoint_detail(
    task_id: str,
    checkpoint_id: str,
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
) -> Any:
    """
    Ëé∑ÂèñÊ£ÄÊü•ÁÇπËØ¶ÊÉÖ
    
    ËøîÂõûÂÆåÊï¥ÁöÑ Agent Áä∂ÊÄÅÊï∞ÊçÆ
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    from app.models.agent_task import AgentCheckpoint
    
    checkpoint = await db.get(AgentCheckpoint, checkpoint_id)
    if not checkpoint or checkpoint.task_id != task_id:
        raise HTTPException(status_code=404, detail="Ê£ÄÊü•ÁÇπ‰∏çÂ≠òÂú®")
    
    # Ëß£ÊûêÁä∂ÊÄÅÊï∞ÊçÆ
    state_data = {}
    if checkpoint.state_data:
        try:
            state_data = json.loads(checkpoint.state_data)
        except json.JSONDecodeError:
            pass
    
    return {
        "id": checkpoint.id,
        "task_id": checkpoint.task_id,
        "agent_id": checkpoint.agent_id,
        "agent_name": checkpoint.agent_name,
        "agent_type": checkpoint.agent_type,
        "parent_agent_id": checkpoint.parent_agent_id,
        "iteration": checkpoint.iteration,
        "status": checkpoint.status,
        "total_tokens": checkpoint.total_tokens,
        "tool_calls": checkpoint.tool_calls,
        "findings_count": checkpoint.findings_count,
        "checkpoint_type": checkpoint.checkpoint_type,
        "checkpoint_name": checkpoint.checkpoint_name,
        "state_data": state_data,
        "metadata": checkpoint.checkpoint_metadata,
        "created_at": checkpoint.created_at.isoformat() if checkpoint.created_at else None,
    }


# ============ Report Generation API ============

@router.get("/{task_id}/report")
async def generate_audit_report(
    task_id: str,
    format: str = Query("markdown", regex="^(markdown|json)$"),
    db: AsyncSession = Depends(get_db),
    current_user: User = Depends(deps.get_current_user),
):
    """
    ÁîüÊàêÂÆ°ËÆ°Êä•Âëä
    
    ÊîØÊåÅ Markdown Âíå JSON Ê†ºÂºè
    """
    task = await db.get(AgentTask, task_id)
    if not task:
        raise HTTPException(status_code=404, detail="‰ªªÂä°‰∏çÂ≠òÂú®")
    
    project = await db.get(Project, task.project_id)
    if not project or project.owner_id != current_user.id:
        raise HTTPException(status_code=403, detail="Êó†ÊùÉËÆøÈóÆÊ≠§‰ªªÂä°")
    
    # Ëé∑ÂèñÊ≠§‰ªªÂä°ÁöÑÊâÄÊúâÂèëÁé∞
    findings = await db.execute(
        select(AgentFinding)
        .where(AgentFinding.task_id == task_id)
        .order_by(
            case(
                (AgentFinding.severity == 'critical', 1),
                (AgentFinding.severity == 'high', 2),
                (AgentFinding.severity == 'medium', 3),
                (AgentFinding.severity == 'low', 4),
                else_=5
            ),
            AgentFinding.created_at.desc()
        )
    )
    findings = findings.scalars().all()
    
    # üî• Helper function to normalize severity for comparison (case-insensitive)
    def normalize_severity(sev: str) -> str:
        return str(sev).lower().strip() if sev else ""
    
    # Log findings for debugging
    logger.info(f"[Report] Task {task_id}: Found {len(findings)} findings from database")
    if findings:
        for i, f in enumerate(findings[:3]):  # Log first 3
            logger.debug(f"[Report] Finding {i+1}: severity='{f.severity}', title='{f.title[:50] if f.title else 'N/A'}'")
    
    if format == "json":
        # Enhanced JSON report with full metadata
        return {
            "report_metadata": {
                "task_id": task.id,
                "project_id": task.project_id,
                "project_name": project.name,
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "task_status": task.status,
                "duration_seconds": int((task.completed_at - task.started_at).total_seconds()) if task.completed_at and task.started_at else None,
            },
            "summary": {
                "security_score": task.security_score,
                "total_files_analyzed": task.analyzed_files,
                "total_findings": len(findings),
                "verified_findings": sum(1 for f in findings if f.is_verified),
                "severity_distribution": {
                    "critical": sum(1 for f in findings if normalize_severity(f.severity) == 'critical'),
                    "high": sum(1 for f in findings if normalize_severity(f.severity) == 'high'),
                    "medium": sum(1 for f in findings if normalize_severity(f.severity) == 'medium'),
                    "low": sum(1 for f in findings if normalize_severity(f.severity) == 'low'),
                },
                "agent_metrics": {
                    "total_iterations": task.total_iterations,
                    "tool_calls": task.tool_calls_count,
                    "tokens_used": task.tokens_used,
                }
            },
            "findings": [
                {
                    "id": f.id,
                    "title": f.title,
                    "severity": f.severity,
                    "vulnerability_type": f.vulnerability_type,
                    "description": f.description,
                    "file_path": f.file_path,
                    "line_start": f.line_start,
                    "line_end": f.line_end,
                    "code_snippet": f.code_snippet,
                    "is_verified": f.is_verified,
                    "has_poc": f.has_poc,
                    "poc_code": f.poc_code,
                    "poc_description": f.poc_description,
                    "poc_steps": f.poc_steps,
                    "confidence": f.ai_confidence,
                    "suggestion": f.suggestion,
                    "fix_code": f.fix_code,
                    "created_at": f.created_at.isoformat() if f.created_at else None,
                } for f in findings
            ]
        }

    # Generate Enhanced Markdown Report
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Calculate statistics
    total = len(findings)
    critical = sum(1 for f in findings if normalize_severity(f.severity) == 'critical')
    high = sum(1 for f in findings if normalize_severity(f.severity) == 'high')
    medium = sum(1 for f in findings if normalize_severity(f.severity) == 'medium')
    low = sum(1 for f in findings if normalize_severity(f.severity) == 'low')
    verified = sum(1 for f in findings if f.is_verified)
    with_poc = sum(1 for f in findings if f.has_poc)

    # Calculate duration
    duration_str = "N/A"
    if task.completed_at and task.started_at:
        duration = (task.completed_at - task.started_at).total_seconds()
        if duration >= 3600:
            duration_str = f"{duration / 3600:.1f} Â∞èÊó∂"
        elif duration >= 60:
            duration_str = f"{duration / 60:.1f} ÂàÜÈíü"
        else:
            duration_str = f"{int(duration)} Áßí"

    md_lines = []

    # Header
    md_lines.append("# DeepAudit ÂÆâÂÖ®ÂÆ°ËÆ°Êä•Âëä")
    md_lines.append("")
    md_lines.append("---")
    md_lines.append("")

    # Report Info
    md_lines.append("## Êä•Âëä‰ø°ÊÅØ")
    md_lines.append("")
    md_lines.append(f"| Â±ûÊÄß | ÂÜÖÂÆπ |")
    md_lines.append(f"|----------|-------|")
    md_lines.append(f"| **È°πÁõÆÂêçÁß∞** | {project.name} |")
    md_lines.append(f"| **‰ªªÂä° ID** | `{task.id[:8]}...` |")
    md_lines.append(f"| **ÁîüÊàêÊó∂Èó¥** | {timestamp} |")
    md_lines.append(f"| **‰ªªÂä°Áä∂ÊÄÅ** | {task.status.upper()} |")
    md_lines.append(f"| **ËÄóÊó∂** | {duration_str} |")
    md_lines.append("")

    # Executive Summary
    md_lines.append("## ÊâßË°åÊëòË¶Å")
    md_lines.append("")

    score = task.security_score
    if score is not None:
        if score >= 80:
            score_assessment = "ËâØÂ•Ω - Âª∫ËÆÆËøõË°åÂ∞ëÈáè‰ºòÂåñ"
            score_icon = "ÈÄöËøá"
        elif score >= 60:
            score_assessment = "‰∏≠Á≠â - Â≠òÂú®Ëã•Âπ≤ÈóÆÈ¢òÈúÄË¶ÅÂÖ≥Ê≥®"
            score_icon = "Ë≠¶Âëä"
        else:
            score_assessment = "‰∏•Èáç - ÈúÄË¶ÅÁ´ãÂç≥ËøõË°å‰øÆÂ§ç"
            score_icon = "Êú™ÈÄöËøá"
        md_lines.append(f"**ÂÆâÂÖ®ËØÑÂàÜ: {int(score)}/100** [{score_icon}]")
        md_lines.append(f"*{score_assessment}*")
    else:
        md_lines.append("**ÂÆâÂÖ®ËØÑÂàÜ:** Êú™ËÆ°ÁÆó")
    md_lines.append("")

    # Findings Summary
    md_lines.append("### ÊºèÊ¥ûÂèëÁé∞Ê¶ÇËßà")
    md_lines.append("")
    md_lines.append(f"| ‰∏•ÈáçÁ®ãÂ∫¶ | Êï∞Èáè | Â∑≤È™åËØÅ |")
    md_lines.append(f"|----------|-------|----------|")
    if critical > 0:
        md_lines.append(f"| **‰∏•Èáç (CRITICAL)** | {critical} | {sum(1 for f in findings if normalize_severity(f.severity) == 'critical' and f.is_verified)} |")
    if high > 0:
        md_lines.append(f"| **È´òÂç± (HIGH)** | {high} | {sum(1 for f in findings if normalize_severity(f.severity) == 'high' and f.is_verified)} |")
    if medium > 0:
        md_lines.append(f"| **‰∏≠Âç± (MEDIUM)** | {medium} | {sum(1 for f in findings if normalize_severity(f.severity) == 'medium' and f.is_verified)} |")
    if low > 0:
        md_lines.append(f"| **‰ΩéÂç± (LOW)** | {low} | {sum(1 for f in findings if normalize_severity(f.severity) == 'low' and f.is_verified)} |")
    md_lines.append(f"| **ÊÄªËÆ°** | {total} | {verified} |")
    md_lines.append("")

    # Audit Metrics
    md_lines.append("### ÂÆ°ËÆ°ÊåáÊ†á")
    md_lines.append("")
    md_lines.append(f"- **ÂàÜÊûêÊñá‰ª∂Êï∞:** {task.analyzed_files} / {task.total_files}")
    md_lines.append(f"- **Agent Ëø≠‰ª£Ê¨°Êï∞:** {task.total_iterations}")
    md_lines.append(f"- **Â∑•ÂÖ∑Ë∞ÉÁî®Ê¨°Êï∞:** {task.tool_calls_count}")
    md_lines.append(f"- **Token Ê∂àËÄó:** {task.tokens_used:,}")
    if with_poc > 0:
        md_lines.append(f"- **ÁîüÊàêÁöÑ PoC:** {with_poc}")
    md_lines.append("")

    # Detailed Findings
    if not findings:
        md_lines.append("## ÊºèÊ¥ûËØ¶ÊÉÖ")
        md_lines.append("")
        md_lines.append("*Êú¨Ê¨°ÂÆ°ËÆ°Êú™ÂèëÁé∞ÂÆâÂÖ®ÊºèÊ¥û„ÄÇ*")
        md_lines.append("")
    else:
        # Group findings by severity
        severity_map = {
            'critical': '‰∏•Èáç (Critical)',
            'high': 'È´òÂç± (High)',
            'medium': '‰∏≠Âç± (Medium)',
            'low': '‰ΩéÂç± (Low)'
        }
        
        for severity_level, severity_name in severity_map.items():
            severity_findings = [f for f in findings if normalize_severity(f.severity) == severity_level]
            if not severity_findings:
                continue

            md_lines.append(f"## {severity_name} ÊºèÊ¥û")
            md_lines.append("")

            for i, f in enumerate(severity_findings, 1):
                verified_badge = "[Â∑≤È™åËØÅ]" if f.is_verified else "[Êú™È™åËØÅ]"
                poc_badge = " [Âê´ PoC]" if f.has_poc else ""

                md_lines.append(f"### {severity_level.upper()}-{i}: {f.title}")
                md_lines.append("")
                md_lines.append(f"**{verified_badge}**{poc_badge} | Á±ªÂûã: `{f.vulnerability_type}`")
                md_lines.append("")

                if f.file_path:
                    location = f"`{f.file_path}"
                    if f.line_start:
                        location += f":{f.line_start}"
                        if f.line_end and f.line_end != f.line_start:
                            location += f"-{f.line_end}"
                    location += "`"
                    md_lines.append(f"**‰ΩçÁΩÆ:** {location}")
                    md_lines.append("")

                if f.ai_confidence:
                    md_lines.append(f"**AI ÁΩÆ‰ø°Â∫¶:** {int(f.ai_confidence * 100)}%")
                    md_lines.append("")

                if f.description:
                    md_lines.append("**ÊºèÊ¥ûÊèèËø∞:**")
                    md_lines.append("")
                    md_lines.append(f.description)
                    md_lines.append("")

                if f.code_snippet:
                    # Detect language from file extension
                    lang = "python"
                    if f.file_path:
                        ext = f.file_path.split('.')[-1].lower()
                        lang_map = {
                            'py': 'python', 'js': 'javascript', 'ts': 'typescript',
                            'jsx': 'jsx', 'tsx': 'tsx', 'java': 'java', 'go': 'go',
                            'rs': 'rust', 'rb': 'ruby', 'php': 'php', 'c': 'c',
                            'cpp': 'cpp', 'cs': 'csharp', 'sol': 'solidity'
                        }
                        lang = lang_map.get(ext, 'text')
                    md_lines.append("**ÊºèÊ¥û‰ª£Á†Å:**")
                    md_lines.append("")
                    md_lines.append(f"```{lang}")
                    md_lines.append(f.code_snippet.strip())
                    md_lines.append("```")
                    md_lines.append("")

                if f.suggestion:
                    md_lines.append("**‰øÆÂ§çÂª∫ËÆÆ:**")
                    md_lines.append("")
                    md_lines.append(f.suggestion)
                    md_lines.append("")

                if f.fix_code:
                    md_lines.append("**ÂèÇËÄÉ‰øÆÂ§ç‰ª£Á†Å:**")
                    md_lines.append("")
                    md_lines.append(f"```{lang if f.file_path else 'text'}")
                    md_lines.append(f.fix_code.strip())
                    md_lines.append("```")
                    md_lines.append("")

                # üî• Ê∑ªÂä† PoC ËØ¶ÊÉÖ
                if f.has_poc:
                    md_lines.append("**Ê¶ÇÂøµÈ™åËØÅ (PoC):**")
                    md_lines.append("")

                    if f.poc_description:
                        md_lines.append(f"*{f.poc_description}*")
                        md_lines.append("")

                    if f.poc_steps:
                        md_lines.append("**Â§çÁé∞Ê≠•È™§:**")
                        md_lines.append("")
                        for step_idx, step in enumerate(f.poc_steps, 1):
                            md_lines.append(f"{step_idx}. {step}")
                        md_lines.append("")

                    if f.poc_code:
                        md_lines.append("**PoC ‰ª£Á†Å:**")
                        md_lines.append("")
                        md_lines.append("```")
                        md_lines.append(f.poc_code.strip())
                        md_lines.append("```")
                        md_lines.append("")

                md_lines.append("---")
                md_lines.append("")

    # Remediation Priority
    if critical > 0 or high > 0:
        md_lines.append("## ‰øÆÂ§ç‰ºòÂÖàÁ∫ßÂª∫ËÆÆ")
        md_lines.append("")
        md_lines.append("Âü∫‰∫éÂ∑≤ÂèëÁé∞ÁöÑÊºèÊ¥ûÔºåÊàë‰ª¨Âª∫ËÆÆÊåâ‰ª•‰∏ã‰ºòÂÖàÁ∫ßËøõË°å‰øÆÂ§çÔºö")
        md_lines.append("")
        priority_idx = 1
        if critical > 0:
            md_lines.append(f"{priority_idx}. **Á´ãÂç≥‰øÆÂ§ç:** Â§ÑÁêÜ {critical} ‰∏™‰∏•ÈáçÊºèÊ¥û - ÂèØËÉΩÈÄ†Êàê‰∏•ÈáçÂΩ±Âìç")
            priority_idx += 1
        if high > 0:
            md_lines.append(f"{priority_idx}. **È´ò‰ºòÂÖàÁ∫ß:** Âú® 1 Âë®ÂÜÖ‰øÆÂ§ç {high} ‰∏™È´òÂç±ÊºèÊ¥û")
            priority_idx += 1
        if medium > 0:
            md_lines.append(f"{priority_idx}. **‰∏≠‰ºòÂÖàÁ∫ß:** Âú® 2-4 Âë®ÂÜÖ‰øÆÂ§ç {medium} ‰∏™‰∏≠Âç±ÊºèÊ¥û")
            priority_idx += 1
        if low > 0:
            md_lines.append(f"{priority_idx}. **‰Ωé‰ºòÂÖàÁ∫ß:** Âú®Êó•Â∏∏Áª¥Êä§‰∏≠Â§ÑÁêÜ {low} ‰∏™‰ΩéÂç±ÊºèÊ¥û")
            priority_idx += 1
        md_lines.append("")

    # Footer
    md_lines.append("---")
    md_lines.append("")
    md_lines.append("*Êú¨Êä•ÂëäÁî± DeepAudit - AI È©±Âä®ÁöÑÂÆâÂÖ®ÂàÜÊûêÁ≥ªÁªüÁîüÊàê*")
    md_lines.append("")
    content = "\n".join(md_lines)
    
    filename = f"audit_report_{task.id[:8]}_{datetime.now().strftime('%Y%m%d')}.md"
    
    from fastapi.responses import Response
    return Response(
        content=content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f"attachment; filename={filename}"
        }
    )
